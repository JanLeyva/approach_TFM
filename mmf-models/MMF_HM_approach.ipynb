{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMF_HM_approach.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z_9CxLNu1nZ4",
        "UpFMUJH_GPgp",
        "hRzytiBDLstT",
        "LO8dmLlwMB53",
        "3-DeY938MYoU",
        "rCl2i5rkhiPY",
        "jnr_IWvdkfnZ",
        "c8JSIHCb-pXU",
        "RdOzAZtNflNP"
      ],
      "mount_file_id": "1n5NDQTxJV2rWrK9TZoZyE8qi9jDWvPoP",
      "authorship_tag": "ABX9TyNf81K38TFUlV2OJVnX6pEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanLeyva/approach_TFM/blob/master/MMF_HM_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXBXYcC_Yo6a",
        "outputId": "f501de7c-b78f-4738-c9cc-49085fa7549d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#A8EB15'> <b> Set up enviorament </b>\n",
        "\n",
        "The `mmf` framework is more stable with python 3.7.11. For this reason we going to create conda enviorament with it."
      ],
      "metadata": {
        "id": "z_9CxLNu1nZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title enviorament creation\n",
        "%%bash\n",
        "\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-py37_4.11.0-Linux-x86_64.sh \n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "Ed8j1eCYYplx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21795c24-93c1-4c0b-e3e5-668c47b4096f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - _openmp_mutex==4.5=1_gnu\n",
            "    - brotlipy==0.7.0=py37h27cfd23_1003\n",
            "    - ca-certificates==2021.10.26=h06a4308_2\n",
            "    - certifi==2021.10.8=py37h06a4308_2\n",
            "    - cffi==1.15.0=py37hd667e15_1\n",
            "    - charset-normalizer==2.0.4=pyhd3eb1b0_0\n",
            "    - conda-content-trust==0.1.1=pyhd3eb1b0_0\n",
            "    - conda-package-handling==1.7.3=py37h27cfd23_1\n",
            "    - conda==4.11.0=py37h06a4308_0\n",
            "    - cryptography==36.0.0=py37h9ce1e76_0\n",
            "    - idna==3.3=pyhd3eb1b0_0\n",
            "    - ld_impl_linux-64==2.35.1=h7274673_9\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.3.0=h5101ec6_17\n",
            "    - libgomp==9.3.0=h5101ec6_17\n",
            "    - libstdcxx-ng==9.3.0=hd4cf53a_17\n",
            "    - ncurses==6.3=h7f8727e_2\n",
            "    - openssl==1.1.1m=h7f8727e_0\n",
            "    - pip==21.2.2=py37h06a4308_0\n",
            "    - pycosat==0.6.3=py37h27cfd23_0\n",
            "    - pycparser==2.21=pyhd3eb1b0_0\n",
            "    - pyopenssl==21.0.0=pyhd3eb1b0_1\n",
            "    - pysocks==1.7.1=py37_1\n",
            "    - python==3.7.11=h12debd9_0\n",
            "    - readline==8.1.2=h7f8727e_1\n",
            "    - requests==2.27.1=pyhd3eb1b0_0\n",
            "    - ruamel_yaml==0.15.100=py37h27cfd23_0\n",
            "    - setuptools==58.0.4=py37h06a4308_0\n",
            "    - six==1.16.0=pyhd3eb1b0_0\n",
            "    - sqlite==3.37.0=hc218d9a_0\n",
            "    - tk==8.6.11=h1ccaba5_0\n",
            "    - tqdm==4.62.3=pyhd3eb1b0_1\n",
            "    - urllib3==1.26.7=pyhd3eb1b0_0\n",
            "    - wheel==0.37.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.11=h7f8727e_4\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py37h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2021.10.26-h06a4308_2\n",
            "  certifi            pkgs/main/linux-64::certifi-2021.10.8-py37h06a4308_2\n",
            "  cffi               pkgs/main/linux-64::cffi-1.15.0-py37hd667e15_1\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "  conda              pkgs/main/linux-64::conda-4.11.0-py37h06a4308_0\n",
            "  conda-content-tru~ pkgs/main/noarch::conda-content-trust-0.1.1-pyhd3eb1b0_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.7.3-py37h27cfd23_1\n",
            "  cryptography       pkgs/main/linux-64::cryptography-36.0.0-py37h9ce1e76_0\n",
            "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1m-h7f8727e_0\n",
            "  pip                pkgs/main/linux-64::pip-21.2.2-py37h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h27cfd23_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-21.0.0-pyhd3eb1b0_1\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_1\n",
            "  python             pkgs/main/linux-64::python-3.7.11-h12debd9_0\n",
            "  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1\n",
            "  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.100-py37h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-58.0.4-py37h06a4308_0\n",
            "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.37.0-hc218d9a_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.11-h1ccaba5_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.62.3-pyhd3eb1b0_1\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.7-pyhd3eb1b0_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7f8727e_4\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-05-19 18:08:31--  https://repo.continuum.io/miniconda/Miniconda3-py37_4.11.0-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-py37_4.11.0-Linux-x86_64.sh [following]\n",
            "--2022-05-19 18:08:31--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.11.0-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103730670 (99M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.11.0-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 92.2M 1s\n",
            "    50K .......... .......... .......... .......... ..........  0% 11.1M 5s\n",
            "   100K .......... .......... .......... .......... ..........  0% 14.5M 6s\n",
            "   150K .......... .......... .......... .......... ..........  0% 46.1M 5s\n",
            "   200K .......... .......... .......... .......... ..........  0%  158M 4s\n",
            "   250K .......... .......... .......... .......... ..........  0%  231M 3s\n",
            "   300K .......... .......... .......... .......... ..........  0% 17.7M 4s\n",
            "   350K .......... .......... .......... .......... ..........  0%  199M 3s\n",
            "   400K .......... .......... .......... .......... ..........  0%  155M 3s\n",
            "   450K .......... .......... .......... .......... ..........  0%  154M 3s\n",
            "   500K .......... .......... .......... .......... ..........  0%  114M 3s\n",
            "   550K .......... .......... .......... .......... ..........  0%  128M 2s\n",
            "   600K .......... .......... .......... .......... ..........  0% 96.8M 2s\n",
            "   650K .......... .......... .......... .......... ..........  0%  174M 2s\n",
            "   700K .......... .......... .......... .......... ..........  0% 10.5M 3s\n",
            "   750K .......... .......... .......... .......... ..........  0%  154M 3s\n",
            "   800K .......... .......... .......... .......... ..........  0%  161M 2s\n",
            "   850K .......... .......... .......... .......... ..........  0%  253M 2s\n",
            "   900K .......... .......... .......... .......... ..........  0%  222M 2s\n",
            "   950K .......... .......... .......... .......... ..........  0%  171M 2s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  182M 2s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  208M 2s\n",
            "  1100K .......... .......... .......... .......... ..........  1%  186M 2s\n",
            "  1150K .......... .......... .......... .......... ..........  1%  148M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  1%  196M 2s\n",
            "  1250K .......... .......... .......... .......... ..........  1%  195M 2s\n",
            "  1300K .......... .......... .......... .......... ..........  1%  199M 2s\n",
            "  1350K .......... .......... .......... .......... ..........  1%  163M 2s\n",
            "  1400K .......... .......... .......... .......... ..........  1%  207M 2s\n",
            "  1450K .......... .......... .......... .......... ..........  1%  225M 2s\n",
            "  1500K .......... .......... .......... .......... ..........  1%  250M 2s\n",
            "  1550K .......... .......... .......... .......... ..........  1%  207M 2s\n",
            "  1600K .......... .......... .......... .......... ..........  1% 26.4M 2s\n",
            "  1650K .......... .......... .......... .......... ..........  1%  139M 2s\n",
            "  1700K .......... .......... .......... .......... ..........  1%  182M 2s\n",
            "  1750K .......... .......... .......... .......... ..........  1%  148M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  1%  162M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  1%  170M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  1%  155M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  1%  124M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  2%  159M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  2%  167M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  2%  210M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  2%  199M 1s\n",
            "  2200K .......... .......... .......... .......... ..........  2%  251M 1s\n",
            "  2250K .......... .......... .......... .......... ..........  2%  254M 1s\n",
            "  2300K .......... .......... .......... .......... ..........  2%  251M 1s\n",
            "  2350K .......... .......... .......... .......... ..........  2%  207M 1s\n",
            "  2400K .......... .......... .......... .......... ..........  2%  243M 1s\n",
            "  2450K .......... .......... .......... .......... ..........  2% 49.4M 1s\n",
            "  2500K .......... .......... .......... .......... ..........  2%  169M 1s\n",
            "  2550K .......... .......... .......... .......... ..........  2%  144M 1s\n",
            "  2600K .......... .......... .......... .......... ..........  2%  160M 1s\n",
            "  2650K .......... .......... .......... .......... ..........  2%  164M 1s\n",
            "  2700K .......... .......... .......... .......... ..........  2%  180M 1s\n",
            "  2750K .......... .......... .......... .......... ..........  2%  177M 1s\n",
            "  2800K .......... .......... .......... .......... ..........  2%  233M 1s\n",
            "  2850K .......... .......... .......... .......... ..........  2%  254M 1s\n",
            "  2900K .......... .......... .......... .......... ..........  2%  256M 1s\n",
            "  2950K .......... .......... .......... .......... ..........  2%  222M 1s\n",
            "  3000K .......... .......... .......... .......... ..........  3%  247M 1s\n",
            "  3050K .......... .......... .......... .......... ..........  3%  227M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  3% 52.5M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  3% 62.0M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  3% 22.7M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  3%  150M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  3%  235M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  3%  224M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  3% 22.3M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  3%  170M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  3%  202M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  3%  182M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  3%  186M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  3%  221M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  3% 46.3M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  3% 19.2M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  3%  186M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  3% 18.8M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  3%  164M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  3%  172M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  3%  241M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  4% 34.8M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  4%  129M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  4%  152M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  4%  225M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  4%  244M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  4%  259M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  4% 24.7M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  4%  200M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  4%  209M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  4%  250M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  4%  134M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  4% 70.4M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  4%  125M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  4% 87.8M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  4%  115M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  4% 98.0M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  4% 95.3M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  4%  119M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  4% 61.6M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  4%  129M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  5% 52.8M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  5%  129M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  5% 14.8M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  5%  197M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  5%  242M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  5%  206M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  5%  187M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  5%  209M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  5%  225M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  5% 83.4M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  5% 52.1M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  5%  145M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  5% 17.0M 1s\n",
            "  5700K .......... .......... .......... .......... ..........  5%  213M 1s\n",
            "  5750K .......... .......... .......... .......... ..........  5%  172M 1s\n",
            "  5800K .......... .......... .......... .......... ..........  5%  208M 1s\n",
            "  5850K .......... .......... .......... .......... ..........  5%  246M 1s\n",
            "  5900K .......... .......... .......... .......... ..........  5%  250M 1s\n",
            "  5950K .......... .......... .......... .......... ..........  5%  147M 1s\n",
            "  6000K .......... .......... .......... .......... ..........  5%  240M 1s\n",
            "  6050K .......... .......... .......... .......... ..........  6%  186M 1s\n",
            "  6100K .......... .......... .......... .......... ..........  6% 13.9M 1s\n",
            "  6150K .......... .......... .......... .......... ..........  6%  160M 1s\n",
            "  6200K .......... .......... .......... .......... ..........  6%  208M 1s\n",
            "  6250K .......... .......... .......... .......... ..........  6%  212M 1s\n",
            "  6300K .......... .......... .......... .......... ..........  6%  235M 1s\n",
            "  6350K .......... .......... .......... .......... ..........  6%  230M 1s\n",
            "  6400K .......... .......... .......... .......... ..........  6%  255M 1s\n",
            "  6450K .......... .......... .......... .......... ..........  6%  259M 1s\n",
            "  6500K .......... .......... .......... .......... ..........  6%  258M 1s\n",
            "  6550K .......... .......... .......... .......... ..........  6% 16.3M 1s\n",
            "  6600K .......... .......... .......... .......... ..........  6%  188M 1s\n",
            "  6650K .......... .......... .......... .......... ..........  6%  226M 1s\n",
            "  6700K .......... .......... .......... .......... ..........  6%  256M 1s\n",
            "  6750K .......... .......... .......... .......... ..........  6%  223M 1s\n",
            "  6800K .......... .......... .......... .......... ..........  6% 14.4M 1s\n",
            "  6850K .......... .......... .......... .......... ..........  6%  205M 1s\n",
            "  6900K .......... .......... .......... .......... ..........  6%  212M 1s\n",
            "  6950K .......... .......... .......... .......... ..........  6%  182M 1s\n",
            "  7000K .......... .......... .......... .......... ..........  6%  228M 1s\n",
            "  7050K .......... .......... .......... .......... ..........  7%  249M 1s\n",
            "  7100K .......... .......... .......... .......... ..........  7%  252M 1s\n",
            "  7150K .......... .......... .......... .......... ..........  7%  224M 1s\n",
            "  7200K .......... .......... .......... .......... ..........  7% 7.60M 1s\n",
            "  7250K .......... .......... .......... .......... ..........  7%  116M 1s\n",
            "  7300K .......... .......... .......... .......... ..........  7%  216M 1s\n",
            "  7350K .......... .......... .......... .......... ..........  7%  185M 1s\n",
            "  7400K .......... .......... .......... .......... ..........  7%  221M 1s\n",
            "  7450K .......... .......... .......... .......... ..........  7%  246M 1s\n",
            "  7500K .......... .......... .......... .......... ..........  7%  234M 1s\n",
            "  7550K .......... .......... .......... .......... ..........  7%  204M 1s\n",
            "  7600K .......... .......... .......... .......... ..........  7%  170M 1s\n",
            "  7650K .......... .......... .......... .......... ..........  7%  246M 1s\n",
            "  7700K .......... .......... .......... .......... ..........  7%  243M 1s\n",
            "  7750K .......... .......... .......... .......... ..........  7% 62.4M 1s\n",
            "  7800K .......... .......... .......... .......... ..........  7%  233M 1s\n",
            "  7850K .......... .......... .......... .......... ..........  7%  222M 1s\n",
            "  7900K .......... .......... .......... .......... ..........  7%  240M 1s\n",
            "  7950K .......... .......... .......... .......... ..........  7%  207M 1s\n",
            "  8000K .......... .......... .......... .......... ..........  7%  262M 1s\n",
            "  8050K .......... .......... .......... .......... ..........  7% 34.0M 1s\n",
            "  8100K .......... .......... .......... .......... ..........  8%  189M 1s\n",
            "  8150K .......... .......... .......... .......... ..........  8%  191M 1s\n",
            "  8200K .......... .......... .......... .......... ..........  8%  237M 1s\n",
            "  8250K .......... .......... .......... .......... ..........  8%  244M 1s\n",
            "  8300K .......... .......... .......... .......... ..........  8% 26.5M 1s\n",
            "  8350K .......... .......... .......... .......... ..........  8%  187M 1s\n",
            "  8400K .......... .......... .......... .......... ..........  8%  203M 1s\n",
            "  8450K .......... .......... .......... .......... ..........  8% 22.0M 1s\n",
            "  8500K .......... .......... .......... .......... ..........  8%  188M 1s\n",
            "  8550K .......... .......... .......... .......... ..........  8%  215M 1s\n",
            "  8600K .......... .......... .......... .......... ..........  8%  244M 1s\n",
            "  8650K .......... .......... .......... .......... ..........  8% 15.9M 1s\n",
            "  8700K .......... .......... .......... .......... ..........  8%  183M 1s\n",
            "  8750K .......... .......... .......... .......... ..........  8%  179M 1s\n",
            "  8800K .......... .......... .......... .......... ..........  8%  171M 1s\n",
            "  8850K .......... .......... .......... .......... ..........  8%  171M 1s\n",
            "  8900K .......... .......... .......... .......... ..........  8%  183M 1s\n",
            "  8950K .......... .......... .......... .......... ..........  8%  178M 1s\n",
            "  9000K .......... .......... .......... .......... ..........  8%  200M 1s\n",
            "  9050K .......... .......... .......... .......... ..........  8%  216M 1s\n",
            "  9100K .......... .......... .......... .......... ..........  9%  258M 1s\n",
            "  9150K .......... .......... .......... .......... ..........  9%  228M 1s\n",
            "  9200K .......... .......... .......... .......... ..........  9%  253M 1s\n",
            "  9250K .......... .......... .......... .......... ..........  9%  252M 1s\n",
            "  9300K .......... .......... .......... .......... ..........  9%  225M 1s\n",
            "  9350K .......... .......... .......... .......... ..........  9%  213M 1s\n",
            "  9400K .......... .......... .......... .......... ..........  9%  247M 1s\n",
            "  9450K .......... .......... .......... .......... ..........  9%  249M 1s\n",
            "  9500K .......... .......... .......... .......... ..........  9% 25.1M 1s\n",
            "  9550K .......... .......... .......... .......... ..........  9%  157M 1s\n",
            "  9600K .......... .......... .......... .......... ..........  9%  174M 1s\n",
            "  9650K .......... .......... .......... .......... ..........  9%  236M 1s\n",
            "  9700K .......... .......... .......... .......... ..........  9%  230M 1s\n",
            "  9750K .......... .......... .......... .......... ..........  9% 51.1M 1s\n",
            "  9800K .......... .......... .......... .......... ..........  9%  214M 1s\n",
            "  9850K .......... .......... .......... .......... ..........  9%  192M 1s\n",
            "  9900K .......... .......... .......... .......... ..........  9%  205M 1s\n",
            "  9950K .......... .......... .......... .......... ..........  9%  172M 1s\n",
            " 10000K .......... .......... .......... .......... ..........  9%  227M 1s\n",
            " 10050K .......... .......... .......... .......... ..........  9%  232M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 10%  246M 1s\n",
            " 10150K .......... .......... .......... .......... .......... 10%  214M 1s\n",
            " 10200K .......... .......... .......... .......... .......... 10%  252M 1s\n",
            " 10250K .......... .......... .......... .......... .......... 10%  253M 1s\n",
            " 10300K .......... .......... .......... .......... .......... 10%  236M 1s\n",
            " 10350K .......... .......... .......... .......... .......... 10% 25.7M 1s\n",
            " 10400K .......... .......... .......... .......... .......... 10%  174M 1s\n",
            " 10450K .......... .......... .......... .......... .......... 10%  204M 1s\n",
            " 10500K .......... .......... .......... .......... .......... 10%  232M 1s\n",
            " 10550K .......... .......... .......... .......... .......... 10%  201M 1s\n",
            " 10600K .......... .......... .......... .......... .......... 10%  230M 1s\n",
            " 10650K .......... .......... .......... .......... .......... 10%  227M 1s\n",
            " 10700K .......... .......... .......... .......... .......... 10%  202M 1s\n",
            " 10750K .......... .......... .......... .......... .......... 10% 57.0M 1s\n",
            " 10800K .......... .......... .......... .......... .......... 10% 15.7M 1s\n",
            " 10850K .......... .......... .......... .......... .......... 10%  208M 1s\n",
            " 10900K .......... .......... .......... .......... .......... 10%  225M 1s\n",
            " 10950K .......... .......... .......... .......... .......... 10%  208M 1s\n",
            " 11000K .......... .......... .......... .......... .......... 10% 29.1M 1s\n",
            " 11050K .......... .......... .......... .......... .......... 10%  203M 1s\n",
            " 11100K .......... .......... .......... .......... .......... 11%  204M 1s\n",
            " 11150K .......... .......... .......... .......... .......... 11%  199M 1s\n",
            " 11200K .......... .......... .......... .......... .......... 11%  223M 1s\n",
            " 11250K .......... .......... .......... .......... .......... 11%  227M 1s\n",
            " 11300K .......... .......... .......... .......... .......... 11%  248M 1s\n",
            " 11350K .......... .......... .......... .......... .......... 11%  102M 1s\n",
            " 11400K .......... .......... .......... .......... .......... 11% 60.0M 1s\n",
            " 11450K .......... .......... .......... .......... .......... 11%  174M 1s\n",
            " 11500K .......... .......... .......... .......... .......... 11%  204M 1s\n",
            " 11550K .......... .......... .......... .......... .......... 11%  203M 1s\n",
            " 11600K .......... .......... .......... .......... .......... 11% 27.5M 1s\n",
            " 11650K .......... .......... .......... .......... .......... 11%  248M 1s\n",
            " 11700K .......... .......... .......... .......... .......... 11% 41.6M 1s\n",
            " 11750K .......... .......... .......... .......... .......... 11%  177M 1s\n",
            " 11800K .......... .......... .......... .......... .......... 11%  227M 1s\n",
            " 11850K .......... .......... .......... .......... .......... 11% 14.2M 1s\n",
            " 11900K .......... .......... .......... .......... .......... 11%  228M 1s\n",
            " 11950K .......... .......... .......... .......... .......... 11%  166M 1s\n",
            " 12000K .......... .......... .......... .......... .......... 11%  233M 1s\n",
            " 12050K .......... .......... .......... .......... .......... 11%  249M 1s\n",
            " 12100K .......... .......... .......... .......... .......... 11%  255M 1s\n",
            " 12150K .......... .......... .......... .......... .......... 12%  212M 1s\n",
            " 12200K .......... .......... .......... .......... .......... 12%  234M 1s\n",
            " 12250K .......... .......... .......... .......... .......... 12%  206M 1s\n",
            " 12300K .......... .......... .......... .......... .......... 12%  196M 1s\n",
            " 12350K .......... .......... .......... .......... .......... 12%  118M 1s\n",
            " 12400K .......... .......... .......... .......... .......... 12% 22.5M 1s\n",
            " 12450K .......... .......... .......... .......... .......... 12%  219M 1s\n",
            " 12500K .......... .......... .......... .......... .......... 12%  246M 1s\n",
            " 12550K .......... .......... .......... .......... .......... 12% 60.1M 1s\n",
            " 12600K .......... .......... .......... .......... .......... 12%  123M 1s\n",
            " 12650K .......... .......... .......... .......... .......... 12% 86.3M 1s\n",
            " 12700K .......... .......... .......... .......... .......... 12%  238M 1s\n",
            " 12750K .......... .......... .......... .......... .......... 12% 70.6M 1s\n",
            " 12800K .......... .......... .......... .......... .......... 12% 57.5M 1s\n",
            " 12850K .......... .......... .......... .......... .......... 12% 46.6M 1s\n",
            " 12900K .......... .......... .......... .......... .......... 12% 80.5M 1s\n",
            " 12950K .......... .......... .......... .......... .......... 12% 56.1M 1s\n",
            " 13000K .......... .......... .......... .......... .......... 12%  104M 1s\n",
            " 13050K .......... .......... .......... .......... .......... 12%  106M 1s\n",
            " 13100K .......... .......... .......... .......... .......... 12% 89.4M 1s\n",
            " 13150K .......... .......... .......... .......... .......... 13% 72.1M 1s\n",
            " 13200K .......... .......... .......... .......... .......... 13% 19.8M 1s\n",
            " 13250K .......... .......... .......... .......... .......... 13%  236M 1s\n",
            " 13300K .......... .......... .......... .......... .......... 13%  223M 1s\n",
            " 13350K .......... .......... .......... .......... .......... 13% 15.6M 1s\n",
            " 13400K .......... .......... .......... .......... .......... 13%  206M 1s\n",
            " 13450K .......... .......... .......... .......... .......... 13%  220M 1s\n",
            " 13500K .......... .......... .......... .......... .......... 13%  233M 1s\n",
            " 13550K .......... .......... .......... .......... .......... 13%  209M 1s\n",
            " 13600K .......... .......... .......... .......... .......... 13% 15.6M 1s\n",
            " 13650K .......... .......... .......... .......... .......... 13%  254M 1s\n",
            " 13700K .......... .......... .......... .......... .......... 13% 14.2M 1s\n",
            " 13750K .......... .......... .......... .......... .......... 13%  186M 1s\n",
            " 13800K .......... .......... .......... .......... .......... 13%  233M 1s\n",
            " 13850K .......... .......... .......... .......... .......... 13%  241M 1s\n",
            " 13900K .......... .......... .......... .......... .......... 13%  176M 1s\n",
            " 13950K .......... .......... .......... .......... .......... 13% 42.1M 1s\n",
            " 14000K .......... .......... .......... .......... .......... 13% 66.0M 1s\n",
            " 14050K .......... .......... .......... .......... .......... 13% 59.0M 1s\n",
            " 14100K .......... .......... .......... .......... .......... 13%  235M 1s\n",
            " 14150K .......... .......... .......... .......... .......... 14% 58.7M 1s\n",
            " 14200K .......... .......... .......... .......... .......... 14% 71.6M 1s\n",
            " 14250K .......... .......... .......... .......... .......... 14% 8.24M 1s\n",
            " 14300K .......... .......... .......... .......... .......... 14%  239M 1s\n",
            " 14350K .......... .......... .......... .......... .......... 14%  199M 1s\n",
            " 14400K .......... .......... .......... .......... .......... 14%  261M 1s\n",
            " 14450K .......... .......... .......... .......... .......... 14%  200M 1s\n",
            " 14500K .......... .......... .......... .......... .......... 14%  215M 1s\n",
            " 14550K .......... .......... .......... .......... .......... 14%  173M 1s\n",
            " 14600K .......... .......... .......... .......... .......... 14%  249M 1s\n",
            " 14650K .......... .......... .......... .......... .......... 14%  254M 1s\n",
            " 14700K .......... .......... .......... .......... .......... 14%  262M 1s\n",
            " 14750K .......... .......... .......... .......... .......... 14% 55.9M 1s\n",
            " 14800K .......... .......... .......... .......... .......... 14%  200M 1s\n",
            " 14850K .......... .......... .......... .......... .......... 14%  217M 1s\n",
            " 14900K .......... .......... .......... .......... .......... 14%  169M 1s\n",
            " 14950K .......... .......... .......... .......... .......... 14% 39.7M 1s\n",
            " 15000K .......... .......... .......... .......... .......... 14% 19.4M 1s\n",
            " 15050K .......... .......... .......... .......... .......... 14% 18.9M 1s\n",
            " 15100K .......... .......... .......... .......... .......... 14%  211M 1s\n",
            " 15150K .......... .......... .......... .......... .......... 15%  206M 1s\n",
            " 15200K .......... .......... .......... .......... .......... 15% 16.8M 1s\n",
            " 15250K .......... .......... .......... .......... .......... 15% 21.4M 1s\n",
            " 15300K .......... .......... .......... .......... .......... 15%  198M 1s\n",
            " 15350K .......... .......... .......... .......... .......... 15%  216M 1s\n",
            " 15400K .......... .......... .......... .......... .......... 15% 41.9M 1s\n",
            " 15450K .......... .......... .......... .......... .......... 15%  180M 1s\n",
            " 15500K .......... .......... .......... .......... .......... 15%  213M 1s\n",
            " 15550K .......... .......... .......... .......... .......... 15%  185M 1s\n",
            " 15600K .......... .......... .......... .......... .......... 15% 40.3M 1s\n",
            " 15650K .......... .......... .......... .......... .......... 15%  209M 1s\n",
            " 15700K .......... .......... .......... .......... .......... 15%  229M 1s\n",
            " 15750K .......... .......... .......... .......... .......... 15% 17.2M 1s\n",
            " 15800K .......... .......... .......... .......... .......... 15%  201M 1s\n",
            " 15850K .......... .......... .......... .......... .......... 15%  246M 1s\n",
            " 15900K .......... .......... .......... .......... .......... 15% 58.1M 1s\n",
            " 15950K .......... .......... .......... .......... .......... 15%  176M 1s\n",
            " 16000K .......... .......... .......... .......... .......... 15%  229M 1s\n",
            " 16050K .......... .......... .......... .......... .......... 15% 12.6M 1s\n",
            " 16100K .......... .......... .......... .......... .......... 15%  184M 1s\n",
            " 16150K .......... .......... .......... .......... .......... 15%  171M 1s\n",
            " 16200K .......... .......... .......... .......... .......... 16%  245M 1s\n",
            " 16250K .......... .......... .......... .......... .......... 16%  261M 1s\n",
            " 16300K .......... .......... .......... .......... .......... 16%  238M 1s\n",
            " 16350K .......... .......... .......... .......... .......... 16%  223M 1s\n",
            " 16400K .......... .......... .......... .......... .......... 16%  245M 1s\n",
            " 16450K .......... .......... .......... .......... .......... 16% 28.8M 1s\n",
            " 16500K .......... .......... .......... .......... .......... 16%  246M 1s\n",
            " 16550K .......... .......... .......... .......... .......... 16%  213M 1s\n",
            " 16600K .......... .......... .......... .......... .......... 16% 22.2M 1s\n",
            " 16650K .......... .......... .......... .......... .......... 16%  176M 1s\n",
            " 16700K .......... .......... .......... .......... .......... 16%  206M 1s\n",
            " 16750K .......... .......... .......... .......... .......... 16%  205M 1s\n",
            " 16800K .......... .......... .......... .......... .......... 16%  249M 1s\n",
            " 16850K .......... .......... .......... .......... .......... 16%  252M 1s\n",
            " 16900K .......... .......... .......... .......... .......... 16%  193M 1s\n",
            " 16950K .......... .......... .......... .......... .......... 16%  169M 1s\n",
            " 17000K .......... .......... .......... .......... .......... 16%  208M 1s\n",
            " 17050K .......... .......... .......... .......... .......... 16%  255M 1s\n",
            " 17100K .......... .......... .......... .......... .......... 16%  182M 1s\n",
            " 17150K .......... .......... .......... .......... .......... 16% 82.7M 1s\n",
            " 17200K .......... .......... .......... .......... .......... 17% 12.0M 1s\n",
            " 17250K .......... .......... .......... .......... .......... 17%  201M 1s\n",
            " 17300K .......... .......... .......... .......... .......... 17%  232M 1s\n",
            " 17350K .......... .......... .......... .......... .......... 17%  204M 1s\n",
            " 17400K .......... .......... .......... .......... .......... 17%  252M 1s\n",
            " 17450K .......... .......... .......... .......... .......... 17% 45.4M 1s\n",
            " 17500K .......... .......... .......... .......... .......... 17%  224M 1s\n",
            " 17550K .......... .......... .......... .......... .......... 17% 41.1M 1s\n",
            " 17600K .......... .......... .......... .......... .......... 17%  244M 1s\n",
            " 17650K .......... .......... .......... .......... .......... 17% 48.5M 1s\n",
            " 17700K .......... .......... .......... .......... .......... 17% 48.9M 1s\n",
            " 17750K .......... .......... .......... .......... .......... 17% 29.3M 1s\n",
            " 17800K .......... .......... .......... .......... .......... 17% 40.2M 1s\n",
            " 17850K .......... .......... .......... .......... .......... 17%  212M 1s\n",
            " 17900K .......... .......... .......... .......... .......... 17% 93.3M 1s\n",
            " 17950K .......... .......... .......... .......... .......... 17% 97.5M 1s\n",
            " 18000K .......... .......... .......... .......... .......... 17% 57.5M 1s\n",
            " 18050K .......... .......... .......... .......... .......... 17% 40.6M 1s\n",
            " 18100K .......... .......... .......... .......... .......... 17%  196M 1s\n",
            " 18150K .......... .......... .......... .......... .......... 17% 35.4M 1s\n",
            " 18200K .......... .......... .......... .......... .......... 18%  185M 1s\n",
            " 18250K .......... .......... .......... .......... .......... 18%  152M 1s\n",
            " 18300K .......... .......... .......... .......... .......... 18% 80.4M 1s\n",
            " 18350K .......... .......... .......... .......... .......... 18% 97.7M 1s\n",
            " 18400K .......... .......... .......... .......... .......... 18% 83.4M 1s\n",
            " 18450K .......... .......... .......... .......... .......... 18% 74.4M 1s\n",
            " 18500K .......... .......... .......... .......... .......... 18% 65.2M 1s\n",
            " 18550K .......... .......... .......... .......... .......... 18% 52.3M 1s\n",
            " 18600K .......... .......... .......... .......... .......... 18% 30.6M 1s\n",
            " 18650K .......... .......... .......... .......... .......... 18% 94.1M 1s\n",
            " 18700K .......... .......... .......... .......... .......... 18% 22.6M 1s\n",
            " 18750K .......... .......... .......... .......... .......... 18% 20.6M 1s\n",
            " 18800K .......... .......... .......... .......... .......... 18%  201M 1s\n",
            " 18850K .......... .......... .......... .......... .......... 18%  259M 1s\n",
            " 18900K .......... .......... .......... .......... .......... 18% 49.5M 1s\n",
            " 18950K .......... .......... .......... .......... .......... 18% 59.8M 1s\n",
            " 19000K .......... .......... .......... .......... .......... 18% 83.0M 1s\n",
            " 19050K .......... .......... .......... .......... .......... 18% 43.7M 1s\n",
            " 19100K .......... .......... .......... .......... .......... 18% 40.5M 1s\n",
            " 19150K .......... .......... .......... .......... .......... 18% 29.0M 1s\n",
            " 19200K .......... .......... .......... .......... .......... 19% 62.8M 1s\n",
            " 19250K .......... .......... .......... .......... .......... 19% 40.4M 1s\n",
            " 19300K .......... .......... .......... .......... .......... 19% 41.3M 1s\n",
            " 19350K .......... .......... .......... .......... .......... 19% 44.3M 1s\n",
            " 19400K .......... .......... .......... .......... .......... 19% 41.6M 1s\n",
            " 19450K .......... .......... .......... .......... .......... 19% 42.6M 1s\n",
            " 19500K .......... .......... .......... .......... .......... 19% 37.8M 1s\n",
            " 19550K .......... .......... .......... .......... .......... 19% 34.7M 1s\n",
            " 19600K .......... .......... .......... .......... .......... 19% 40.0M 1s\n",
            " 19650K .......... .......... .......... .......... .......... 19% 10.3M 1s\n",
            " 19700K .......... .......... .......... .......... .......... 19%  186M 1s\n",
            " 19750K .......... .......... .......... .......... .......... 19%  194M 1s\n",
            " 19800K .......... .......... .......... .......... .......... 19%  233M 1s\n",
            " 19850K .......... .......... .......... .......... .......... 19% 37.1M 1s\n",
            " 19900K .......... .......... .......... .......... .......... 19% 91.4M 1s\n",
            " 19950K .......... .......... .......... .......... .......... 19% 38.3M 1s\n",
            " 20000K .......... .......... .......... .......... .......... 19% 42.0M 1s\n",
            " 20050K .......... .......... .......... .......... .......... 19% 35.3M 1s\n",
            " 20100K .......... .......... .......... .......... .......... 19% 39.7M 1s\n",
            " 20150K .......... .......... .......... .......... .......... 19% 17.2M 1s\n",
            " 20200K .......... .......... .......... .......... .......... 19%  234M 1s\n",
            " 20250K .......... .......... .......... .......... .......... 20%  227M 1s\n",
            " 20300K .......... .......... .......... .......... .......... 20% 54.2M 1s\n",
            " 20350K .......... .......... .......... .......... .......... 20% 37.1M 1s\n",
            " 20400K .......... .......... .......... .......... .......... 20% 35.2M 1s\n",
            " 20450K .......... .......... .......... .......... .......... 20% 84.2M 1s\n",
            " 20500K .......... .......... .......... .......... .......... 20% 22.3M 1s\n",
            " 20550K .......... .......... .......... .......... .......... 20%  116M 1s\n",
            " 20600K .......... .......... .......... .......... .......... 20% 49.6M 1s\n",
            " 20650K .......... .......... .......... .......... .......... 20% 47.0M 1s\n",
            " 20700K .......... .......... .......... .......... .......... 20% 40.4M 1s\n",
            " 20750K .......... .......... .......... .......... .......... 20% 23.1M 1s\n",
            " 20800K .......... .......... .......... .......... .......... 20%  144M 1s\n",
            " 20850K .......... .......... .......... .......... .......... 20% 28.8M 1s\n",
            " 20900K .......... .......... .......... .......... .......... 20% 76.2M 1s\n",
            " 20950K .......... .......... .......... .......... .......... 20% 49.3M 1s\n",
            " 21000K .......... .......... .......... .......... .......... 20% 35.0M 1s\n",
            " 21050K .......... .......... .......... .......... .......... 20% 19.5M 1s\n",
            " 21100K .......... .......... .......... .......... .......... 20%  184M 1s\n",
            " 21150K .......... .......... .......... .......... .......... 20%  187M 1s\n",
            " 21200K .......... .......... .......... .......... .......... 20% 54.2M 1s\n",
            " 21250K .......... .......... .......... .......... .......... 21% 16.6M 1s\n",
            " 21300K .......... .......... .......... .......... .......... 21%  208M 1s\n",
            " 21350K .......... .......... .......... .......... .......... 21%  185M 1s\n",
            " 21400K .......... .......... .......... .......... .......... 21% 55.5M 1s\n",
            " 21450K .......... .......... .......... .......... .......... 21% 46.6M 1s\n",
            " 21500K .......... .......... .......... .......... .......... 21% 38.2M 1s\n",
            " 21550K .......... .......... .......... .......... .......... 21% 11.2M 1s\n",
            " 21600K .......... .......... .......... .......... .......... 21%  220M 1s\n",
            " 21650K .......... .......... .......... .......... .......... 21%  209M 1s\n",
            " 21700K .......... .......... .......... .......... .......... 21%  251M 1s\n",
            " 21750K .......... .......... .......... .......... .......... 21% 13.8M 1s\n",
            " 21800K .......... .......... .......... .......... .......... 21%  224M 1s\n",
            " 21850K .......... .......... .......... .......... .......... 21%  238M 1s\n",
            " 21900K .......... .......... .......... .......... .......... 21% 99.0M 1s\n",
            " 21950K .......... .......... .......... .......... .......... 21% 63.1M 1s\n",
            " 22000K .......... .......... .......... .......... .......... 21% 24.0M 1s\n",
            " 22050K .......... .......... .......... .......... .......... 21%  224M 1s\n",
            " 22100K .......... .......... .......... .......... .......... 21%  128M 1s\n",
            " 22150K .......... .......... .......... .......... .......... 21% 16.4M 1s\n",
            " 22200K .......... .......... .......... .......... .......... 21%  212M 1s\n",
            " 22250K .......... .......... .......... .......... .......... 22%  160M 1s\n",
            " 22300K .......... .......... .......... .......... .......... 22%  209M 1s\n",
            " 22350K .......... .......... .......... .......... .......... 22%  211M 1s\n",
            " 22400K .......... .......... .......... .......... .......... 22%  233M 1s\n",
            " 22450K .......... .......... .......... .......... .......... 22%  196M 1s\n",
            " 22500K .......... .......... .......... .......... .......... 22%  254M 1s\n",
            " 22550K .......... .......... .......... .......... .......... 22% 92.1M 1s\n",
            " 22600K .......... .......... .......... .......... .......... 22%  192M 1s\n",
            " 22650K .......... .......... .......... .......... .......... 22%  183M 1s\n",
            " 22700K .......... .......... .......... .......... .......... 22%  171M 1s\n",
            " 22750K .......... .......... .......... .......... .......... 22% 16.9M 1s\n",
            " 22800K .......... .......... .......... .......... .......... 22%  199M 1s\n",
            " 22850K .......... .......... .......... .......... .......... 22%  210M 1s\n",
            " 22900K .......... .......... .......... .......... .......... 22%  211M 1s\n",
            " 22950K .......... .......... .......... .......... .......... 22%  228M 1s\n",
            " 23000K .......... .......... .......... .......... .......... 22%  235M 1s\n",
            " 23050K .......... .......... .......... .......... .......... 22%  238M 1s\n",
            " 23100K .......... .......... .......... .......... .......... 22% 47.3M 1s\n",
            " 23150K .......... .......... .......... .......... .......... 22%  172M 1s\n",
            " 23200K .......... .......... .......... .......... .......... 22%  227M 1s\n",
            " 23250K .......... .......... .......... .......... .......... 23%  188M 1s\n",
            " 23300K .......... .......... .......... .......... .......... 23%  199M 1s\n",
            " 23350K .......... .......... .......... .......... .......... 23%  144M 1s\n",
            " 23400K .......... .......... .......... .......... .......... 23% 45.4M 1s\n",
            " 23450K .......... .......... .......... .......... .......... 23%  120M 1s\n",
            " 23500K .......... .......... .......... .......... .......... 23%  228M 1s\n",
            " 23550K .......... .......... .......... .......... .......... 23% 10.1M 1s\n",
            " 23600K .......... .......... .......... .......... .......... 23%  167M 1s\n",
            " 23650K .......... .......... .......... .......... .......... 23%  190M 1s\n",
            " 23700K .......... .......... .......... .......... .......... 23%  215M 1s\n",
            " 23750K .......... .......... .......... .......... .......... 23%  164M 1s\n",
            " 23800K .......... .......... .......... .......... .......... 23%  225M 1s\n",
            " 23850K .......... .......... .......... .......... .......... 23%  198M 1s\n",
            " 23900K .......... .......... .......... .......... .......... 23%  221M 1s\n",
            " 23950K .......... .......... .......... .......... .......... 23%  169M 1s\n",
            " 24000K .......... .......... .......... .......... .......... 23%  202M 1s\n",
            " 24050K .......... .......... .......... .......... .......... 23%  218M 1s\n",
            " 24100K .......... .......... .......... .......... .......... 23%  248M 1s\n",
            " 24150K .......... .......... .......... .......... .......... 23%  210M 1s\n",
            " 24200K .......... .......... .......... .......... .......... 23%  249M 1s\n",
            " 24250K .......... .......... .......... .......... .......... 23%  238M 1s\n",
            " 24300K .......... .......... .......... .......... .......... 24% 30.2M 1s\n",
            " 24350K .......... .......... .......... .......... .......... 24%  195M 1s\n",
            " 24400K .......... .......... .......... .......... .......... 24%  242M 1s\n",
            " 24450K .......... .......... .......... .......... .......... 24% 84.3M 1s\n",
            " 24500K .......... .......... .......... .......... .......... 24%  208M 1s\n",
            " 24550K .......... .......... .......... .......... .......... 24%  214M 1s\n",
            " 24600K .......... .......... .......... .......... .......... 24%  240M 1s\n",
            " 24650K .......... .......... .......... .......... .......... 24%  237M 1s\n",
            " 24700K .......... .......... .......... .......... .......... 24% 23.2M 1s\n",
            " 24750K .......... .......... .......... .......... .......... 24%  181M 1s\n",
            " 24800K .......... .......... .......... .......... .......... 24%  219M 1s\n",
            " 24850K .......... .......... .......... .......... .......... 24%  247M 1s\n",
            " 24900K .......... .......... .......... .......... .......... 24%  244M 1s\n",
            " 24950K .......... .......... .......... .......... .......... 24%  211M 1s\n",
            " 25000K .......... .......... .......... .......... .......... 24% 11.9M 1s\n",
            " 25050K .......... .......... .......... .......... .......... 24%  199M 1s\n",
            " 25100K .......... .......... .......... .......... .......... 24%  207M 1s\n",
            " 25150K .......... .......... .......... .......... .......... 24%  157M 1s\n",
            " 25200K .......... .......... .......... .......... .......... 24%  209M 1s\n",
            " 25250K .......... .......... .......... .......... .......... 24%  250M 1s\n",
            " 25300K .......... .......... .......... .......... .......... 25%  248M 1s\n",
            " 25350K .......... .......... .......... .......... .......... 25%  220M 1s\n",
            " 25400K .......... .......... .......... .......... .......... 25%  208M 1s\n",
            " 25450K .......... .......... .......... .......... .......... 25%  244M 1s\n",
            " 25500K .......... .......... .......... .......... .......... 25% 5.31M 1s\n",
            " 25550K .......... .......... .......... .......... .......... 25%  176M 1s\n",
            " 25600K .......... .......... .......... .......... .......... 25%  232M 1s\n",
            " 25650K .......... .......... .......... .......... .......... 25%  246M 1s\n",
            " 25700K .......... .......... .......... .......... .......... 25%  227M 1s\n",
            " 25750K .......... .......... .......... .......... .......... 25% 6.56M 1s\n",
            " 25800K .......... .......... .......... .......... .......... 25%  206M 1s\n",
            " 25850K .......... .......... .......... .......... .......... 25%  192M 1s\n",
            " 25900K .......... .......... .......... .......... .......... 25%  218M 1s\n",
            " 25950K .......... .......... .......... .......... .......... 25%  178M 1s\n",
            " 26000K .......... .......... .......... .......... .......... 25%  205M 1s\n",
            " 26050K .......... .......... .......... .......... .......... 25%  210M 1s\n",
            " 26100K .......... .......... .......... .......... .......... 25%  228M 1s\n",
            " 26150K .......... .......... .......... .......... .......... 25%  229M 1s\n",
            " 26200K .......... .......... .......... .......... .......... 25%  237M 1s\n",
            " 26250K .......... .......... .......... .......... .......... 25%  250M 1s\n",
            " 26300K .......... .......... .......... .......... .......... 26%  236M 1s\n",
            " 26350K .......... .......... .......... .......... .......... 26%  218M 1s\n",
            " 26400K .......... .......... .......... .......... .......... 26%  262M 1s\n",
            " 26450K .......... .......... .......... .......... .......... 26%  171M 1s\n",
            " 26500K .......... .......... .......... .......... .......... 26%  270M 1s\n",
            " 26550K .......... .......... .......... .......... .......... 26%  217M 1s\n",
            " 26600K .......... .......... .......... .......... .......... 26% 90.9M 1s\n",
            " 26650K .......... .......... .......... .......... .......... 26%  230M 1s\n",
            " 26700K .......... .......... .......... .......... .......... 26%  239M 1s\n",
            " 26750K .......... .......... .......... .......... .......... 26%  208M 1s\n",
            " 26800K .......... .......... .......... .......... .......... 26% 24.5M 1s\n",
            " 26850K .......... .......... .......... .......... .......... 26%  170M 1s\n",
            " 26900K .......... .......... .......... .......... .......... 26%  245M 1s\n",
            " 26950K .......... .......... .......... .......... .......... 26%  199M 1s\n",
            " 27000K .......... .......... .......... .......... .......... 26%  214M 1s\n",
            " 27050K .......... .......... .......... .......... .......... 26%  234M 1s\n",
            " 27100K .......... .......... .......... .......... .......... 26%  188M 1s\n",
            " 27150K .......... .......... .......... .......... .......... 26%  207M 1s\n",
            " 27200K .......... .......... .......... .......... .......... 26%  248M 1s\n",
            " 27250K .......... .......... .......... .......... .......... 26%  252M 1s\n",
            " 27300K .......... .......... .......... .......... .......... 26%  241M 1s\n",
            " 27350K .......... .......... .......... .......... .......... 27%  225M 1s\n",
            " 27400K .......... .......... .......... .......... .......... 27%  254M 1s\n",
            " 27450K .......... .......... .......... .......... .......... 27%  233M 1s\n",
            " 27500K .......... .......... .......... .......... .......... 27%  209M 1s\n",
            " 27550K .......... .......... .......... .......... .......... 27%  188M 1s\n",
            " 27600K .......... .......... .......... .......... .......... 27%  252M 1s\n",
            " 27650K .......... .......... .......... .......... .......... 27%  263M 1s\n",
            " 27700K .......... .......... .......... .......... .......... 27%  250M 1s\n",
            " 27750K .......... .......... .......... .......... .......... 27% 18.6M 1s\n",
            " 27800K .......... .......... .......... .......... .......... 27%  217M 1s\n",
            " 27850K .......... .......... .......... .......... .......... 27%  206M 1s\n",
            " 27900K .......... .......... .......... .......... .......... 27%  195M 1s\n",
            " 27950K .......... .......... .......... .......... .......... 27%  199M 1s\n",
            " 28000K .......... .......... .......... .......... .......... 27%  256M 1s\n",
            " 28050K .......... .......... .......... .......... .......... 27%  131M 1s\n",
            " 28100K .......... .......... .......... .......... .......... 27%  180M 1s\n",
            " 28150K .......... .......... .......... .......... .......... 27%  176M 1s\n",
            " 28200K .......... .......... .......... .......... .......... 27%  202M 1s\n",
            " 28250K .......... .......... .......... .......... .......... 27%  252M 1s\n",
            " 28300K .......... .......... .......... .......... .......... 27%  242M 1s\n",
            " 28350K .......... .......... .......... .......... .......... 28%  218M 1s\n",
            " 28400K .......... .......... .......... .......... .......... 28%  252M 1s\n",
            " 28450K .......... .......... .......... .......... .......... 28%  155M 1s\n",
            " 28500K .......... .......... .......... .......... .......... 28%  198M 1s\n",
            " 28550K .......... .......... .......... .......... .......... 28%  198M 1s\n",
            " 28600K .......... .......... .......... .......... .......... 28%  243M 1s\n",
            " 28650K .......... .......... .......... .......... .......... 28%  188M 1s\n",
            " 28700K .......... .......... .......... .......... .......... 28%  199M 1s\n",
            " 28750K .......... .......... .......... .......... .......... 28%  175M 1s\n",
            " 28800K .......... .......... .......... .......... .......... 28%  238M 1s\n",
            " 28850K .......... .......... .......... .......... .......... 28%  243M 1s\n",
            " 28900K .......... .......... .......... .......... .......... 28%  246M 1s\n",
            " 28950K .......... .......... .......... .......... .......... 28%  197M 1s\n",
            " 29000K .......... .......... .......... .......... .......... 28%  260M 1s\n",
            " 29050K .......... .......... .......... .......... .......... 28%  252M 1s\n",
            " 29100K .......... .......... .......... .......... .......... 28%  249M 1s\n",
            " 29150K .......... .......... .......... .......... .......... 28%  145M 1s\n",
            " 29200K .......... .......... .......... .......... .......... 28%  241M 1s\n",
            " 29250K .......... .......... .......... .......... .......... 28% 6.50M 1s\n",
            " 29300K .......... .......... .......... .......... .......... 28%  177M 1s\n",
            " 29350K .......... .......... .......... .......... .......... 29%  186M 1s\n",
            " 29400K .......... .......... .......... .......... .......... 29%  225M 1s\n",
            " 29450K .......... .......... .......... .......... .......... 29%  254M 1s\n",
            " 29500K .......... .......... .......... .......... .......... 29%  255M 1s\n",
            " 29550K .......... .......... .......... .......... .......... 29%  205M 1s\n",
            " 29600K .......... .......... .......... .......... .......... 29%  198M 1s\n",
            " 29650K .......... .......... .......... .......... .......... 29%  203M 1s\n",
            " 29700K .......... .......... .......... .......... .......... 29%  247M 1s\n",
            " 29750K .......... .......... .......... .......... .......... 29%  211M 1s\n",
            " 29800K .......... .......... .......... .......... .......... 29%  255M 1s\n",
            " 29850K .......... .......... .......... .......... .......... 29%  258M 1s\n",
            " 29900K .......... .......... .......... .......... .......... 29% 24.4M 1s\n",
            " 29950K .......... .......... .......... .......... .......... 29%  175M 1s\n",
            " 30000K .......... .......... .......... .......... .......... 29%  217M 1s\n",
            " 30050K .......... .......... .......... .......... .......... 29%  239M 1s\n",
            " 30100K .......... .......... .......... .......... .......... 29%  267M 1s\n",
            " 30150K .......... .......... .......... .......... .......... 29% 12.2M 1s\n",
            " 30200K .......... .......... .......... .......... .......... 29%  212M 1s\n",
            " 30250K .......... .......... .......... .......... .......... 29%  204M 1s\n",
            " 30300K .......... .......... .......... .......... .......... 29%  196M 1s\n",
            " 30350K .......... .......... .......... .......... .......... 30%  188M 1s\n",
            " 30400K .......... .......... .......... .......... .......... 30%  231M 1s\n",
            " 30450K .......... .......... .......... .......... .......... 30%  217M 1s\n",
            " 30500K .......... .......... .......... .......... .......... 30%  227M 1s\n",
            " 30550K .......... .......... .......... .......... .......... 30%  175M 1s\n",
            " 30600K .......... .......... .......... .......... .......... 30%  227M 1s\n",
            " 30650K .......... .......... .......... .......... .......... 30%  222M 1s\n",
            " 30700K .......... .......... .......... .......... .......... 30%  224M 1s\n",
            " 30750K .......... .......... .......... .......... .......... 30%  196M 1s\n",
            " 30800K .......... .......... .......... .......... .......... 30%  219M 1s\n",
            " 30850K .......... .......... .......... .......... .......... 30%  231M 1s\n",
            " 30900K .......... .......... .......... .......... .......... 30%  224M 1s\n",
            " 30950K .......... .......... .......... .......... .......... 30%  210M 1s\n",
            " 31000K .......... .......... .......... .......... .......... 30%  239M 1s\n",
            " 31050K .......... .......... .......... .......... .......... 30%  249M 1s\n",
            " 31100K .......... .......... .......... .......... .......... 30%  248M 1s\n",
            " 31150K .......... .......... .......... .......... .......... 30% 31.5M 1s\n",
            " 31200K .......... .......... .......... .......... .......... 30%  221M 1s\n",
            " 31250K .......... .......... .......... .......... .......... 30%  259M 1s\n",
            " 31300K .......... .......... .......... .......... .......... 30% 40.9M 1s\n",
            " 31350K .......... .......... .......... .......... .......... 30% 60.4M 1s\n",
            " 31400K .......... .......... .......... .......... .......... 31% 12.2M 1s\n",
            " 31450K .......... .......... .......... .......... .......... 31%  228M 1s\n",
            " 31500K .......... .......... .......... .......... .......... 31%  126M 1s\n",
            " 31550K .......... .......... .......... .......... .......... 31% 19.0M 1s\n",
            " 31600K .......... .......... .......... .......... .......... 31%  224M 1s\n",
            " 31650K .......... .......... .......... .......... .......... 31%  216M 1s\n",
            " 31700K .......... .......... .......... .......... .......... 31%  233M 1s\n",
            " 31750K .......... .......... .......... .......... .......... 31%  209M 1s\n",
            " 31800K .......... .......... .......... .......... .......... 31%  225M 1s\n",
            " 31850K .......... .......... .......... .......... .......... 31% 22.6M 1s\n",
            " 31900K .......... .......... .......... .......... .......... 31%  207M 1s\n",
            " 31950K .......... .......... .......... .......... .......... 31% 36.9M 1s\n",
            " 32000K .......... .......... .......... .......... .......... 31% 29.6M 1s\n",
            " 32050K .......... .......... .......... .......... .......... 31%  222M 1s\n",
            " 32100K .......... .......... .......... .......... .......... 31% 20.4M 1s\n",
            " 32150K .......... .......... .......... .......... .......... 31%  171M 1s\n",
            " 32200K .......... .......... .......... .......... .......... 31%  221M 1s\n",
            " 32250K .......... .......... .......... .......... .......... 31% 78.1M 1s\n",
            " 32300K .......... .......... .......... .......... .......... 31% 21.6M 1s\n",
            " 32350K .......... .......... .......... .......... .......... 31%  165M 1s\n",
            " 32400K .......... .......... .......... .......... .......... 32%  243M 1s\n",
            " 32450K .......... .......... .......... .......... .......... 32% 17.8M 1s\n",
            " 32500K .......... .......... .......... .......... .......... 32% 42.9M 1s\n",
            " 32550K .......... .......... .......... .......... .......... 32%  185M 1s\n",
            " 32600K .......... .......... .......... .......... .......... 32%  229M 1s\n",
            " 32650K .......... .......... .......... .......... .......... 32%  255M 1s\n",
            " 32700K .......... .......... .......... .......... .......... 32% 27.0M 1s\n",
            " 32750K .......... .......... .......... .......... .......... 32%  199M 1s\n",
            " 32800K .......... .......... .......... .......... .......... 32%  208M 1s\n",
            " 32850K .......... .......... .......... .......... .......... 32%  233M 1s\n",
            " 32900K .......... .......... .......... .......... .......... 32%  245M 1s\n",
            " 32950K .......... .......... .......... .......... .......... 32%  187M 1s\n",
            " 33000K .......... .......... .......... .......... .......... 32%  248M 1s\n",
            " 33050K .......... .......... .......... .......... .......... 32%  269M 1s\n",
            " 33100K .......... .......... .......... .......... .......... 32%  258M 1s\n",
            " 33150K .......... .......... .......... .......... .......... 32%  198M 1s\n",
            " 33200K .......... .......... .......... .......... .......... 32%  229M 1s\n",
            " 33250K .......... .......... .......... .......... .......... 32%  113M 1s\n",
            " 33300K .......... .......... .......... .......... .......... 32% 21.2M 1s\n",
            " 33350K .......... .......... .......... .......... .......... 32%  172M 1s\n",
            " 33400K .......... .......... .......... .......... .......... 33%  221M 1s\n",
            " 33450K .......... .......... .......... .......... .......... 33%  109M 1s\n",
            " 33500K .......... .......... .......... .......... .......... 33%  179M 1s\n",
            " 33550K .......... .......... .......... .......... .......... 33%  187M 1s\n",
            " 33600K .......... .......... .......... .......... .......... 33% 14.6M 1s\n",
            " 33650K .......... .......... .......... .......... .......... 33%  204M 1s\n",
            " 33700K .......... .......... .......... .......... .......... 33%  184M 1s\n",
            " 33750K .......... .......... .......... .......... .......... 33%  171M 1s\n",
            " 33800K .......... .......... .......... .......... .......... 33%  182M 1s\n",
            " 33850K .......... .......... .......... .......... .......... 33%  219M 1s\n",
            " 33900K .......... .......... .......... .......... .......... 33%  257M 1s\n",
            " 33950K .......... .......... .......... .......... .......... 33%  133M 1s\n",
            " 34000K .......... .......... .......... .......... .......... 33%  176M 1s\n",
            " 34050K .......... .......... .......... .......... .......... 33%  260M 1s\n",
            " 34100K .......... .......... .......... .......... .......... 33%  177M 1s\n",
            " 34150K .......... .......... .......... .......... .......... 33%  199M 1s\n",
            " 34200K .......... .......... .......... .......... .......... 33%  121M 1s\n",
            " 34250K .......... .......... .......... .......... .......... 33%  201M 1s\n",
            " 34300K .......... .......... .......... .......... .......... 33% 36.4M 1s\n",
            " 34350K .......... .......... .......... .......... .......... 33%  191M 1s\n",
            " 34400K .......... .......... .......... .......... .......... 34% 29.6M 1s\n",
            " 34450K .......... .......... .......... .......... .......... 34%  207M 1s\n",
            " 34500K .......... .......... .......... .......... .......... 34%  256M 1s\n",
            " 34550K .......... .......... .......... .......... .......... 34%  202M 1s\n",
            " 34600K .......... .......... .......... .......... .......... 34%  262M 1s\n",
            " 34650K .......... .......... .......... .......... .......... 34% 18.4M 1s\n",
            " 34700K .......... .......... .......... .......... .......... 34%  217M 1s\n",
            " 34750K .......... .......... .......... .......... .......... 34%  176M 1s\n",
            " 34800K .......... .......... .......... .......... .......... 34%  239M 1s\n",
            " 34850K .......... .......... .......... .......... .......... 34%  251M 1s\n",
            " 34900K .......... .......... .......... .......... .......... 34%  239M 1s\n",
            " 34950K .......... .......... .......... .......... .......... 34%  217M 1s\n",
            " 35000K .......... .......... .......... .......... .......... 34% 37.9M 1s\n",
            " 35050K .......... .......... .......... .......... .......... 34%  215M 1s\n",
            " 35100K .......... .......... .......... .......... .......... 34%  202M 1s\n",
            " 35150K .......... .......... .......... .......... .......... 34%  186M 1s\n",
            " 35200K .......... .......... .......... .......... .......... 34%  232M 1s\n",
            " 35250K .......... .......... .......... .......... .......... 34%  249M 1s\n",
            " 35300K .......... .......... .......... .......... .......... 34% 20.3M 1s\n",
            " 35350K .......... .......... .......... .......... .......... 34% 60.0M 1s\n",
            " 35400K .......... .......... .......... .......... .......... 34%  141M 1s\n",
            " 35450K .......... .......... .......... .......... .......... 35%  121M 1s\n",
            " 35500K .......... .......... .......... .......... .......... 35%  222M 1s\n",
            " 35550K .......... .......... .......... .......... .......... 35%  201M 1s\n",
            " 35600K .......... .......... .......... .......... .......... 35%  254M 1s\n",
            " 35650K .......... .......... .......... .......... .......... 35% 55.7M 1s\n",
            " 35700K .......... .......... .......... .......... .......... 35%  202M 1s\n",
            " 35750K .......... .......... .......... .......... .......... 35%  176M 1s\n",
            " 35800K .......... .......... .......... .......... .......... 35%  202M 1s\n",
            " 35850K .......... .......... .......... .......... .......... 35%  216M 1s\n",
            " 35900K .......... .......... .......... .......... .......... 35%  230M 1s\n",
            " 35950K .......... .......... .......... .......... .......... 35%  191M 1s\n",
            " 36000K .......... .......... .......... .......... .......... 35%  239M 1s\n",
            " 36050K .......... .......... .......... .......... .......... 35% 58.6M 1s\n",
            " 36100K .......... .......... .......... .......... .......... 35%  216M 1s\n",
            " 36150K .......... .......... .......... .......... .......... 35% 37.8M 1s\n",
            " 36200K .......... .......... .......... .......... .......... 35%  204M 1s\n",
            " 36250K .......... .......... .......... .......... .......... 35%  214M 1s\n",
            " 36300K .......... .......... .......... .......... .......... 35%  199M 1s\n",
            " 36350K .......... .......... .......... .......... .......... 35%  192M 1s\n",
            " 36400K .......... .......... .......... .......... .......... 35%  241M 1s\n",
            " 36450K .......... .......... .......... .......... .......... 36% 11.4M 1s\n",
            " 36500K .......... .......... .......... .......... .......... 36%  193M 1s\n",
            " 36550K .......... .......... .......... .......... .......... 36%  209M 1s\n",
            " 36600K .......... .......... .......... .......... .......... 36%  248M 1s\n",
            " 36650K .......... .......... .......... .......... .......... 36%  234M 1s\n",
            " 36700K .......... .......... .......... .......... .......... 36% 21.7M 1s\n",
            " 36750K .......... .......... .......... .......... .......... 36%  167M 1s\n",
            " 36800K .......... .......... .......... .......... .......... 36%  206M 1s\n",
            " 36850K .......... .......... .......... .......... .......... 36%  212M 1s\n",
            " 36900K .......... .......... .......... .......... .......... 36%  210M 1s\n",
            " 36950K .......... .......... .......... .......... .......... 36%  201M 1s\n",
            " 37000K .......... .......... .......... .......... .......... 36%  253M 1s\n",
            " 37050K .......... .......... .......... .......... .......... 36%  229M 1s\n",
            " 37100K .......... .......... .......... .......... .......... 36%  258M 1s\n",
            " 37150K .......... .......... .......... .......... .......... 36%  212M 1s\n",
            " 37200K .......... .......... .......... .......... .......... 36%  237M 1s\n",
            " 37250K .......... .......... .......... .......... .......... 36%  233M 1s\n",
            " 37300K .......... .......... .......... .......... .......... 36%  210M 1s\n",
            " 37350K .......... .......... .......... .......... .......... 36%  178M 1s\n",
            " 37400K .......... .......... .......... .......... .......... 36%  205M 1s\n",
            " 37450K .......... .......... .......... .......... .......... 37%  223M 1s\n",
            " 37500K .......... .......... .......... .......... .......... 37%  241M 1s\n",
            " 37550K .......... .......... .......... .......... .......... 37%  218M 1s\n",
            " 37600K .......... .......... .......... .......... .......... 37%  250M 1s\n",
            " 37650K .......... .......... .......... .......... .......... 37% 42.3M 1s\n",
            " 37700K .......... .......... .......... .......... .......... 37%  215M 1s\n",
            " 37750K .......... .......... .......... .......... .......... 37%  206M 1s\n",
            " 37800K .......... .......... .......... .......... .......... 37% 21.4M 1s\n",
            " 37850K .......... .......... .......... .......... .......... 37%  179M 1s\n",
            " 37900K .......... .......... .......... .......... .......... 37%  255M 1s\n",
            " 37950K .......... .......... .......... .......... .......... 37%  219M 1s\n",
            " 38000K .......... .......... .......... .......... .......... 37% 40.2M 1s\n",
            " 38050K .......... .......... .......... .......... .......... 37%  137M 1s\n",
            " 38100K .......... .......... .......... .......... .......... 37%  215M 1s\n",
            " 38150K .......... .......... .......... .......... .......... 37%  188M 1s\n",
            " 38200K .......... .......... .......... .......... .......... 37%  246M 1s\n",
            " 38250K .......... .......... .......... .......... .......... 37% 71.2M 1s\n",
            " 38300K .......... .......... .......... .......... .......... 37%  223M 1s\n",
            " 38350K .......... .......... .......... .......... .......... 37%  181M 1s\n",
            " 38400K .......... .......... .......... .......... .......... 37%  232M 1s\n",
            " 38450K .......... .......... .......... .......... .......... 38%  230M 1s\n",
            " 38500K .......... .......... .......... .......... .......... 38% 9.74M 1s\n",
            " 38550K .......... .......... .......... .......... .......... 38%  182M 1s\n",
            " 38600K .......... .......... .......... .......... .......... 38%  231M 1s\n",
            " 38650K .......... .......... .......... .......... .......... 38%  238M 1s\n",
            " 38700K .......... .......... .......... .......... .......... 38%  259M 1s\n",
            " 38750K .......... .......... .......... .......... .......... 38%  118M 1s\n",
            " 38800K .......... .......... .......... .......... .......... 38%  231M 1s\n",
            " 38850K .......... .......... .......... .......... .......... 38%  239M 1s\n",
            " 38900K .......... .......... .......... .......... .......... 38% 75.4M 1s\n",
            " 38950K .......... .......... .......... .......... .......... 38%  177M 1s\n",
            " 39000K .......... .......... .......... .......... .......... 38%  215M 1s\n",
            " 39050K .......... .......... .......... .......... .......... 38%  240M 1s\n",
            " 39100K .......... .......... .......... .......... .......... 38%  239M 1s\n",
            " 39150K .......... .......... .......... .......... .......... 38%  189M 1s\n",
            " 39200K .......... .......... .......... .......... .......... 38%  232M 1s\n",
            " 39250K .......... .......... .......... .......... .......... 38%  251M 1s\n",
            " 39300K .......... .......... .......... .......... .......... 38%  230M 1s\n",
            " 39350K .......... .......... .......... .......... .......... 38% 15.9M 1s\n",
            " 39400K .......... .......... .......... .......... .......... 38%  204M 1s\n",
            " 39450K .......... .......... .......... .......... .......... 38%  193M 1s\n",
            " 39500K .......... .......... .......... .......... .......... 39%  222M 1s\n",
            " 39550K .......... .......... .......... .......... .......... 39%  209M 1s\n",
            " 39600K .......... .......... .......... .......... .......... 39%  264M 1s\n",
            " 39650K .......... .......... .......... .......... .......... 39%  268M 1s\n",
            " 39700K .......... .......... .......... .......... .......... 39% 8.03M 1s\n",
            " 39750K .......... .......... .......... .......... .......... 39%  181M 1s\n",
            " 39800K .......... .......... .......... .......... .......... 39%  199M 1s\n",
            " 39850K .......... .......... .......... .......... .......... 39%  236M 1s\n",
            " 39900K .......... .......... .......... .......... .......... 39%  224M 1s\n",
            " 39950K .......... .......... .......... .......... .......... 39%  178M 1s\n",
            " 40000K .......... .......... .......... .......... .......... 39%  253M 1s\n",
            " 40050K .......... .......... .......... .......... .......... 39%  241M 1s\n",
            " 40100K .......... .......... .......... .......... .......... 39%  234M 1s\n",
            " 40150K .......... .......... .......... .......... .......... 39%  186M 1s\n",
            " 40200K .......... .......... .......... .......... .......... 39%  247M 1s\n",
            " 40250K .......... .......... .......... .......... .......... 39%  254M 1s\n",
            " 40300K .......... .......... .......... .......... .......... 39%  255M 1s\n",
            " 40350K .......... .......... .......... .......... .......... 39%  215M 1s\n",
            " 40400K .......... .......... .......... .......... .......... 39%  245M 1s\n",
            " 40450K .......... .......... .......... .......... .......... 39%  254M 1s\n",
            " 40500K .......... .......... .......... .......... .......... 40%  254M 1s\n",
            " 40550K .......... .......... .......... .......... .......... 40% 41.7M 1s\n",
            " 40600K .......... .......... .......... .......... .......... 40%  234M 1s\n",
            " 40650K .......... .......... .......... .......... .......... 40%  218M 1s\n",
            " 40700K .......... .......... .......... .......... .......... 40%  252M 1s\n",
            " 40750K .......... .......... .......... .......... .......... 40%  226M 1s\n",
            " 40800K .......... .......... .......... .......... .......... 40% 14.2M 1s\n",
            " 40850K .......... .......... .......... .......... .......... 40%  204M 1s\n",
            " 40900K .......... .......... .......... .......... .......... 40%  221M 1s\n",
            " 40950K .......... .......... .......... .......... .......... 40%  191M 1s\n",
            " 41000K .......... .......... .......... .......... .......... 40%  253M 1s\n",
            " 41050K .......... .......... .......... .......... .......... 40%  242M 1s\n",
            " 41100K .......... .......... .......... .......... .......... 40% 30.5M 1s\n",
            " 41150K .......... .......... .......... .......... .......... 40%  175M 1s\n",
            " 41200K .......... .......... .......... .......... .......... 40%  204M 1s\n",
            " 41250K .......... .......... .......... .......... .......... 40%  255M 1s\n",
            " 41300K .......... .......... .......... .......... .......... 40%  243M 1s\n",
            " 41350K .......... .......... .......... .......... .......... 40% 12.6M 1s\n",
            " 41400K .......... .......... .......... .......... .......... 40%  216M 1s\n",
            " 41450K .......... .......... .......... .......... .......... 40%  218M 1s\n",
            " 41500K .......... .......... .......... .......... .......... 41%  229M 1s\n",
            " 41550K .......... .......... .......... .......... .......... 41%  111M 1s\n",
            " 41600K .......... .......... .......... .......... .......... 41%  211M 1s\n",
            " 41650K .......... .......... .......... .......... .......... 41%  264M 1s\n",
            " 41700K .......... .......... .......... .......... .......... 41% 96.6M 1s\n",
            " 41750K .......... .......... .......... .......... .......... 41%  167M 1s\n",
            " 41800K .......... .......... .......... .......... .......... 41%  237M 1s\n",
            " 41850K .......... .......... .......... .......... .......... 41%  208M 1s\n",
            " 41900K .......... .......... .......... .......... .......... 41%  212M 1s\n",
            " 41950K .......... .......... .......... .......... .......... 41%  197M 1s\n",
            " 42000K .......... .......... .......... .......... .......... 41%  216M 1s\n",
            " 42050K .......... .......... .......... .......... .......... 41%  222M 1s\n",
            " 42100K .......... .......... .......... .......... .......... 41%  257M 1s\n",
            " 42150K .......... .......... .......... .......... .......... 41%  211M 1s\n",
            " 42200K .......... .......... .......... .......... .......... 41%  251M 1s\n",
            " 42250K .......... .......... .......... .......... .......... 41%  253M 1s\n",
            " 42300K .......... .......... .......... .......... .......... 41%  233M 1s\n",
            " 42350K .......... .......... .......... .......... .......... 41%  227M 1s\n",
            " 42400K .......... .......... .......... .......... .......... 41%  251M 1s\n",
            " 42450K .......... .......... .......... .......... .......... 41%  254M 1s\n",
            " 42500K .......... .......... .......... .......... .......... 42% 38.1M 1s\n",
            " 42550K .......... .......... .......... .......... .......... 42%  172M 1s\n",
            " 42600K .......... .......... .......... .......... .......... 42%  205M 1s\n",
            " 42650K .......... .......... .......... .......... .......... 42%  230M 1s\n",
            " 42700K .......... .......... .......... .......... .......... 42%  252M 1s\n",
            " 42750K .......... .......... .......... .......... .......... 42%  221M 1s\n",
            " 42800K .......... .......... .......... .......... .......... 42%  257M 1s\n",
            " 42850K .......... .......... .......... .......... .......... 42% 30.6M 1s\n",
            " 42900K .......... .......... .......... .......... .......... 42%  196M 1s\n",
            " 42950K .......... .......... .......... .......... .......... 42%  189M 1s\n",
            " 43000K .......... .......... .......... .......... .......... 42%  191M 1s\n",
            " 43050K .......... .......... .......... .......... .......... 42%  223M 1s\n",
            " 43100K .......... .......... .......... .......... .......... 42%  238M 1s\n",
            " 43150K .......... .......... .......... .......... .......... 42%  227M 1s\n",
            " 43200K .......... .......... .......... .......... .......... 42%  181M 1s\n",
            " 43250K .......... .......... .......... .......... .......... 42% 35.2M 1s\n",
            " 43300K .......... .......... .......... .......... .......... 42%  200M 1s\n",
            " 43350K .......... .......... .......... .......... .......... 42%  174M 1s\n",
            " 43400K .......... .......... .......... .......... .......... 42%  223M 1s\n",
            " 43450K .......... .......... .......... .......... .......... 42%  196M 1s\n",
            " 43500K .......... .......... .......... .......... .......... 42%  225M 1s\n",
            " 43550K .......... .......... .......... .......... .......... 43%  173M 1s\n",
            " 43600K .......... .......... .......... .......... .......... 43%  174M 1s\n",
            " 43650K .......... .......... .......... .......... .......... 43% 98.3M 1s\n",
            " 43700K .......... .......... .......... .......... .......... 43% 12.2M 1s\n",
            " 43750K .......... .......... .......... .......... .......... 43%  194M 1s\n",
            " 43800K .......... .......... .......... .......... .......... 43%  259M 1s\n",
            " 43850K .......... .......... .......... .......... .......... 43% 6.29M 1s\n",
            " 43900K .......... .......... .......... .......... .......... 43%  197M 1s\n",
            " 43950K .......... .......... .......... .......... .......... 43%  200M 1s\n",
            " 44000K .......... .......... .......... .......... .......... 43%  242M 1s\n",
            " 44050K .......... .......... .......... .......... .......... 43%  248M 1s\n",
            " 44100K .......... .......... .......... .......... .......... 43% 10.5M 1s\n",
            " 44150K .......... .......... .......... .......... .......... 43%  140M 1s\n",
            " 44200K .......... .......... .......... .......... .......... 43%  214M 1s\n",
            " 44250K .......... .......... .......... .......... .......... 43%  234M 1s\n",
            " 44300K .......... .......... .......... .......... .......... 43% 22.2M 1s\n",
            " 44350K .......... .......... .......... .......... .......... 43%  168M 1s\n",
            " 44400K .......... .......... .......... .......... .......... 43%  207M 1s\n",
            " 44450K .......... .......... .......... .......... .......... 43%  250M 1s\n",
            " 44500K .......... .......... .......... .......... .......... 43%  256M 1s\n",
            " 44550K .......... .......... .......... .......... .......... 44%  203M 1s\n",
            " 44600K .......... .......... .......... .......... .......... 44%  214M 1s\n",
            " 44650K .......... .......... .......... .......... .......... 44%  214M 1s\n",
            " 44700K .......... .......... .......... .......... .......... 44%  221M 1s\n",
            " 44750K .......... .......... .......... .......... .......... 44%  181M 1s\n",
            " 44800K .......... .......... .......... .......... .......... 44%  213M 1s\n",
            " 44850K .......... .......... .......... .......... .......... 44%  210M 1s\n",
            " 44900K .......... .......... .......... .......... .......... 44%  219M 1s\n",
            " 44950K .......... .......... .......... .......... .......... 44%  187M 1s\n",
            " 45000K .......... .......... .......... .......... .......... 44%  216M 1s\n",
            " 45050K .......... .......... .......... .......... .......... 44%  245M 1s\n",
            " 45100K .......... .......... .......... .......... .......... 44%  251M 1s\n",
            " 45150K .......... .......... .......... .......... .......... 44%  211M 1s\n",
            " 45200K .......... .......... .......... .......... .......... 44%  243M 1s\n",
            " 45250K .......... .......... .......... .......... .......... 44%  254M 1s\n",
            " 45300K .......... .......... .......... .......... .......... 44%  265M 1s\n",
            " 45350K .......... .......... .......... .......... .......... 44%  227M 1s\n",
            " 45400K .......... .......... .......... .......... .......... 44%  281M 1s\n",
            " 45450K .......... .......... .......... .......... .......... 44%  207M 1s\n",
            " 45500K .......... .......... .......... .......... .......... 44%  215M 1s\n",
            " 45550K .......... .......... .......... .......... .......... 45%  149M 1s\n",
            " 45600K .......... .......... .......... .......... .......... 45%  243M 1s\n",
            " 45650K .......... .......... .......... .......... .......... 45%  240M 1s\n",
            " 45700K .......... .......... .......... .......... .......... 45%  256M 1s\n",
            " 45750K .......... .......... .......... .......... .......... 45%  215M 1s\n",
            " 45800K .......... .......... .......... .......... .......... 45%  249M 1s\n",
            " 45850K .......... .......... .......... .......... .......... 45% 76.4M 1s\n",
            " 45900K .......... .......... .......... .......... .......... 45%  210M 1s\n",
            " 45950K .......... .......... .......... .......... .......... 45%  184M 1s\n",
            " 46000K .......... .......... .......... .......... .......... 45%  201M 1s\n",
            " 46050K .......... .......... .......... .......... .......... 45%  234M 1s\n",
            " 46100K .......... .......... .......... .......... .......... 45%  251M 1s\n",
            " 46150K .......... .......... .......... .......... .......... 45%  170M 1s\n",
            " 46200K .......... .......... .......... .......... .......... 45%  225M 1s\n",
            " 46250K .......... .......... .......... .......... .......... 45% 28.8M 1s\n",
            " 46300K .......... .......... .......... .......... .......... 45%  214M 1s\n",
            " 46350K .......... .......... .......... .......... .......... 45%  174M 1s\n",
            " 46400K .......... .......... .......... .......... .......... 45%  214M 1s\n",
            " 46450K .......... .......... .......... .......... .......... 45%  214M 1s\n",
            " 46500K .......... .......... .......... .......... .......... 45%  212M 1s\n",
            " 46550K .......... .......... .......... .......... .......... 46%  159M 1s\n",
            " 46600K .......... .......... .......... .......... .......... 46%  195M 1s\n",
            " 46650K .......... .......... .......... .......... .......... 46%  201M 1s\n",
            " 46700K .......... .......... .......... .......... .......... 46%  198M 1s\n",
            " 46750K .......... .......... .......... .......... .......... 46%  107M 1s\n",
            " 46800K .......... .......... .......... .......... .......... 46%  104M 1s\n",
            " 46850K .......... .......... .......... .......... .......... 46% 40.0M 1s\n",
            " 46900K .......... .......... .......... .......... .......... 46%  131M 1s\n",
            " 46950K .......... .......... .......... .......... .......... 46%  225M 1s\n",
            " 47000K .......... .......... .......... .......... .......... 46%  147M 1s\n",
            " 47050K .......... .......... .......... .......... .......... 46%  149M 1s\n",
            " 47100K .......... .......... .......... .......... .......... 46% 13.7M 1s\n",
            " 47150K .......... .......... .......... .......... .......... 46%  175M 1s\n",
            " 47200K .......... .......... .......... .......... .......... 46%  222M 1s\n",
            " 47250K .......... .......... .......... .......... .......... 46%  210M 1s\n",
            " 47300K .......... .......... .......... .......... .......... 46%  245M 1s\n",
            " 47350K .......... .......... .......... .......... .......... 46%  213M 1s\n",
            " 47400K .......... .......... .......... .......... .......... 46%  250M 1s\n",
            " 47450K .......... .......... .......... .......... .......... 46%  257M 1s\n",
            " 47500K .......... .......... .......... .......... .......... 46%  251M 1s\n",
            " 47550K .......... .......... .......... .......... .......... 46% 25.6M 1s\n",
            " 47600K .......... .......... .......... .......... .......... 47%  197M 1s\n",
            " 47650K .......... .......... .......... .......... .......... 47% 53.8M 1s\n",
            " 47700K .......... .......... .......... .......... .......... 47%  207M 1s\n",
            " 47750K .......... .......... .......... .......... .......... 47%  179M 1s\n",
            " 47800K .......... .......... .......... .......... .......... 47%  234M 1s\n",
            " 47850K .......... .......... .......... .......... .......... 47%  215M 1s\n",
            " 47900K .......... .......... .......... .......... .......... 47%  254M 1s\n",
            " 47950K .......... .......... .......... .......... .......... 47%  191M 1s\n",
            " 48000K .......... .......... .......... .......... .......... 47%  232M 1s\n",
            " 48050K .......... .......... .......... .......... .......... 47%  241M 1s\n",
            " 48100K .......... .......... .......... .......... .......... 47% 14.9M 1s\n",
            " 48150K .......... .......... .......... .......... .......... 47%  190M 1s\n",
            " 48200K .......... .......... .......... .......... .......... 47%  226M 1s\n",
            " 48250K .......... .......... .......... .......... .......... 47%  202M 1s\n",
            " 48300K .......... .......... .......... .......... .......... 47%  239M 1s\n",
            " 48350K .......... .......... .......... .......... .......... 47%  225M 1s\n",
            " 48400K .......... .......... .......... .......... .......... 47%  254M 1s\n",
            " 48450K .......... .......... .......... .......... .......... 47%  266M 1s\n",
            " 48500K .......... .......... .......... .......... .......... 47% 35.1M 1s\n",
            " 48550K .......... .......... .......... .......... .......... 47%  195M 1s\n",
            " 48600K .......... .......... .......... .......... .......... 48%  204M 1s\n",
            " 48650K .......... .......... .......... .......... .......... 48%  257M 1s\n",
            " 48700K .......... .......... .......... .......... .......... 48%  252M 1s\n",
            " 48750K .......... .......... .......... .......... .......... 48%  217M 1s\n",
            " 48800K .......... .......... .......... .......... .......... 48%  239M 1s\n",
            " 48850K .......... .......... .......... .......... .......... 48%  225M 1s\n",
            " 48900K .......... .......... .......... .......... .......... 48%  192M 1s\n",
            " 48950K .......... .......... .......... .......... .......... 48% 49.4M 1s\n",
            " 49000K .......... .......... .......... .......... .......... 48% 53.2M 1s\n",
            " 49050K .......... .......... .......... .......... .......... 48%  220M 1s\n",
            " 49100K .......... .......... .......... .......... .......... 48%  207M 1s\n",
            " 49150K .......... .......... .......... .......... .......... 48% 88.7M 1s\n",
            " 49200K .......... .......... .......... .......... .......... 48% 39.5M 1s\n",
            " 49250K .......... .......... .......... .......... .......... 48%  223M 1s\n",
            " 49300K .......... .......... .......... .......... .......... 48% 86.5M 1s\n",
            " 49350K .......... .......... .......... .......... .......... 48%  152M 1s\n",
            " 49400K .......... .......... .......... .......... .......... 48%  142M 1s\n",
            " 49450K .......... .......... .......... .......... .......... 48% 54.3M 1s\n",
            " 49500K .......... .......... .......... .......... .......... 48%  211M 1s\n",
            " 49550K .......... .......... .......... .......... .......... 48% 46.2M 1s\n",
            " 49600K .......... .......... .......... .......... .......... 49%  225M 1s\n",
            " 49650K .......... .......... .......... .......... .......... 49%  212M 1s\n",
            " 49700K .......... .......... .......... .......... .......... 49% 46.3M 1s\n",
            " 49750K .......... .......... .......... .......... .......... 49%  179M 1s\n",
            " 49800K .......... .......... .......... .......... .......... 49%  228M 1s\n",
            " 49850K .......... .......... .......... .......... .......... 49%  120M 1s\n",
            " 49900K .......... .......... .......... .......... .......... 49%  213M 1s\n",
            " 49950K .......... .......... .......... .......... .......... 49%  115M 1s\n",
            " 50000K .......... .......... .......... .......... .......... 49% 24.1M 1s\n",
            " 50050K .......... .......... .......... .......... .......... 49%  214M 1s\n",
            " 50100K .......... .......... .......... .......... .......... 49%  243M 1s\n",
            " 50150K .......... .......... .......... .......... .......... 49%  211M 1s\n",
            " 50200K .......... .......... .......... .......... .......... 49%  126M 1s\n",
            " 50250K .......... .......... .......... .......... .......... 49%  232M 1s\n",
            " 50300K .......... .......... .......... .......... .......... 49% 28.5M 1s\n",
            " 50350K .......... .......... .......... .......... .......... 49%  179M 1s\n",
            " 50400K .......... .......... .......... .......... .......... 49%  230M 1s\n",
            " 50450K .......... .......... .......... .......... .......... 49%  256M 1s\n",
            " 50500K .......... .......... .......... .......... .......... 49%  232M 1s\n",
            " 50550K .......... .......... .......... .......... .......... 49% 38.6M 1s\n",
            " 50600K .......... .......... .......... .......... .......... 50%  213M 1s\n",
            " 50650K .......... .......... .......... .......... .......... 50%  248M 1s\n",
            " 50700K .......... .......... .......... .......... .......... 50%  249M 1s\n",
            " 50750K .......... .......... .......... .......... .......... 50% 58.0M 1s\n",
            " 50800K .......... .......... .......... .......... .......... 50%  187M 1s\n",
            " 50850K .......... .......... .......... .......... .......... 50%  149M 1s\n",
            " 50900K .......... .......... .......... .......... .......... 50%  233M 1s\n",
            " 50950K .......... .......... .......... .......... .......... 50%  190M 1s\n",
            " 51000K .......... .......... .......... .......... .......... 50%  225M 1s\n",
            " 51050K .......... .......... .......... .......... .......... 50%  142M 1s\n",
            " 51100K .......... .......... .......... .......... .......... 50% 96.0M 1s\n",
            " 51150K .......... .......... .......... .......... .......... 50% 62.7M 1s\n",
            " 51200K .......... .......... .......... .......... .......... 50%  195M 1s\n",
            " 51250K .......... .......... .......... .......... .......... 50%  101M 1s\n",
            " 51300K .......... .......... .......... .......... .......... 50%  127M 1s\n",
            " 51350K .......... .......... .......... .......... .......... 50% 94.1M 1s\n",
            " 51400K .......... .......... .......... .......... .......... 50% 99.9M 1s\n",
            " 51450K .......... .......... .......... .......... .......... 50% 56.8M 1s\n",
            " 51500K .......... .......... .......... .......... .......... 50% 82.3M 1s\n",
            " 51550K .......... .......... .......... .......... .......... 50% 39.6M 1s\n",
            " 51600K .......... .......... .......... .......... .......... 50%  205M 1s\n",
            " 51650K .......... .......... .......... .......... .......... 51%  209M 1s\n",
            " 51700K .......... .......... .......... .......... .......... 51% 64.8M 1s\n",
            " 51750K .......... .......... .......... .......... .......... 51%  183M 1s\n",
            " 51800K .......... .......... .......... .......... .......... 51% 5.87M 1s\n",
            " 51850K .......... .......... .......... .......... .......... 51%  190M 1s\n",
            " 51900K .......... .......... .......... .......... .......... 51%  198M 1s\n",
            " 51950K .......... .......... .......... .......... .......... 51%  173M 1s\n",
            " 52000K .......... .......... .......... .......... .......... 51%  212M 1s\n",
            " 52050K .......... .......... .......... .......... .......... 51%  242M 1s\n",
            " 52100K .......... .......... .......... .......... .......... 51%  150M 1s\n",
            " 52150K .......... .......... .......... .......... .......... 51%  155M 1s\n",
            " 52200K .......... .......... .......... .......... .......... 51%  196M 1s\n",
            " 52250K .......... .......... .......... .......... .......... 51%  208M 1s\n",
            " 52300K .......... .......... .......... .......... .......... 51%  250M 1s\n",
            " 52350K .......... .......... .......... .......... .......... 51%  196M 1s\n",
            " 52400K .......... .......... .......... .......... .......... 51%  252M 1s\n",
            " 52450K .......... .......... .......... .......... .......... 51%  194M 1s\n",
            " 52500K .......... .......... .......... .......... .......... 51%  229M 1s\n",
            " 52550K .......... .......... .......... .......... .......... 51%  204M 1s\n",
            " 52600K .......... .......... .......... .......... .......... 51%  259M 1s\n",
            " 52650K .......... .......... .......... .......... .......... 52%  254M 1s\n",
            " 52700K .......... .......... .......... .......... .......... 52%  249M 1s\n",
            " 52750K .......... .......... .......... .......... .......... 52%  230M 1s\n",
            " 52800K .......... .......... .......... .......... .......... 52%  249M 1s\n",
            " 52850K .......... .......... .......... .......... .......... 52%  259M 1s\n",
            " 52900K .......... .......... .......... .......... .......... 52% 23.6M 1s\n",
            " 52950K .......... .......... .......... .......... .......... 52%  177M 1s\n",
            " 53000K .......... .......... .......... .......... .......... 52%  233M 1s\n",
            " 53050K .......... .......... .......... .......... .......... 52%  252M 1s\n",
            " 53100K .......... .......... .......... .......... .......... 52%  241M 1s\n",
            " 53150K .......... .......... .......... .......... .......... 52% 46.6M 1s\n",
            " 53200K .......... .......... .......... .......... .......... 52%  232M 1s\n",
            " 53250K .......... .......... .......... .......... .......... 52% 6.48M 1s\n",
            " 53300K .......... .......... .......... .......... .......... 52%  227M 1s\n",
            " 53350K .......... .......... .......... .......... .......... 52%  181M 1s\n",
            " 53400K .......... .......... .......... .......... .......... 52%  198M 1s\n",
            " 53450K .......... .......... .......... .......... .......... 52%  231M 1s\n",
            " 53500K .......... .......... .......... .......... .......... 52%  235M 1s\n",
            " 53550K .......... .......... .......... .......... .......... 52%  228M 1s\n",
            " 53600K .......... .......... .......... .......... .......... 52%  212M 1s\n",
            " 53650K .......... .......... .......... .......... .......... 53%  230M 1s\n",
            " 53700K .......... .......... .......... .......... .......... 53% 10.1M 1s\n",
            " 53750K .......... .......... .......... .......... .......... 53%  199M 1s\n",
            " 53800K .......... .......... .......... .......... .......... 53%  244M 1s\n",
            " 53850K .......... .......... .......... .......... .......... 53%  245M 1s\n",
            " 53900K .......... .......... .......... .......... .......... 53% 9.35M 1s\n",
            " 53950K .......... .......... .......... .......... .......... 53%  194M 1s\n",
            " 54000K .......... .......... .......... .......... .......... 53%  237M 1s\n",
            " 54050K .......... .......... .......... .......... .......... 53%  233M 1s\n",
            " 54100K .......... .......... .......... .......... .......... 53%  259M 1s\n",
            " 54150K .......... .......... .......... .......... .......... 53% 17.8M 1s\n",
            " 54200K .......... .......... .......... .......... .......... 53%  214M 1s\n",
            " 54250K .......... .......... .......... .......... .......... 53%  216M 1s\n",
            " 54300K .......... .......... .......... .......... .......... 53%  234M 1s\n",
            " 54350K .......... .......... .......... .......... .......... 53%  209M 1s\n",
            " 54400K .......... .......... .......... .......... .......... 53%  238M 1s\n",
            " 54450K .......... .......... .......... .......... .......... 53%  260M 1s\n",
            " 54500K .......... .......... .......... .......... .......... 53%  234M 1s\n",
            " 54550K .......... .......... .......... .......... .......... 53% 22.2M 1s\n",
            " 54600K .......... .......... .......... .......... .......... 53%  212M 1s\n",
            " 54650K .......... .......... .......... .......... .......... 53%  225M 1s\n",
            " 54700K .......... .......... .......... .......... .......... 54%  253M 1s\n",
            " 54750K .......... .......... .......... .......... .......... 54% 40.6M 1s\n",
            " 54800K .......... .......... .......... .......... .......... 54%  224M 1s\n",
            " 54850K .......... .......... .......... .......... .......... 54%  209M 1s\n",
            " 54900K .......... .......... .......... .......... .......... 54%  219M 1s\n",
            " 54950K .......... .......... .......... .......... .......... 54% 72.8M 1s\n",
            " 55000K .......... .......... .......... .......... .......... 54% 64.4M 1s\n",
            " 55050K .......... .......... .......... .......... .......... 54% 74.3M 1s\n",
            " 55100K .......... .......... .......... .......... .......... 54% 75.1M 1s\n",
            " 55150K .......... .......... .......... .......... .......... 54% 54.6M 1s\n",
            " 55200K .......... .......... .......... .......... .......... 54% 61.5M 1s\n",
            " 55250K .......... .......... .......... .......... .......... 54% 37.7M 1s\n",
            " 55300K .......... .......... .......... .......... .......... 54%  221M 1s\n",
            " 55350K .......... .......... .......... .......... .......... 54% 57.3M 1s\n",
            " 55400K .......... .......... .......... .......... .......... 54%  123M 1s\n",
            " 55450K .......... .......... .......... .......... .......... 54% 11.7M 1s\n",
            " 55500K .......... .......... .......... .......... .......... 54%  224M 1s\n",
            " 55550K .......... .......... .......... .......... .......... 54%  177M 1s\n",
            " 55600K .......... .......... .......... .......... .......... 54%  236M 1s\n",
            " 55650K .......... .......... .......... .......... .......... 54%  250M 1s\n",
            " 55700K .......... .......... .......... .......... .......... 55%  261M 1s\n",
            " 55750K .......... .......... .......... .......... .......... 55% 17.0M 1s\n",
            " 55800K .......... .......... .......... .......... .......... 55%  205M 1s\n",
            " 55850K .......... .......... .......... .......... .......... 55%  231M 1s\n",
            " 55900K .......... .......... .......... .......... .......... 55% 12.8M 1s\n",
            " 55950K .......... .......... .......... .......... .......... 55%  153M 1s\n",
            " 56000K .......... .......... .......... .......... .......... 55%  199M 1s\n",
            " 56050K .......... .......... .......... .......... .......... 55%  232M 1s\n",
            " 56100K .......... .......... .......... .......... .......... 55%  253M 1s\n",
            " 56150K .......... .......... .......... .......... .......... 55%  211M 1s\n",
            " 56200K .......... .......... .......... .......... .......... 55%  258M 1s\n",
            " 56250K .......... .......... .......... .......... .......... 55%  264M 1s\n",
            " 56300K .......... .......... .......... .......... .......... 55%  259M 1s\n",
            " 56350K .......... .......... .......... .......... .......... 55% 25.8M 1s\n",
            " 56400K .......... .......... .......... .......... .......... 55%  213M 1s\n",
            " 56450K .......... .......... .......... .......... .......... 55%  219M 1s\n",
            " 56500K .......... .......... .......... .......... .......... 55%  211M 1s\n",
            " 56550K .......... .......... .......... .......... .......... 55% 33.5M 1s\n",
            " 56600K .......... .......... .......... .......... .......... 55%  205M 1s\n",
            " 56650K .......... .......... .......... .......... .......... 55%  221M 1s\n",
            " 56700K .......... .......... .......... .......... .......... 56%  237M 1s\n",
            " 56750K .......... .......... .......... .......... .......... 56%  104M 1s\n",
            " 56800K .......... .......... .......... .......... .......... 56% 61.4M 1s\n",
            " 56850K .......... .......... .......... .......... .......... 56% 73.6M 1s\n",
            " 56900K .......... .......... .......... .......... .......... 56%  158M 1s\n",
            " 56950K .......... .......... .......... .......... .......... 56% 8.87M 1s\n",
            " 57000K .......... .......... .......... .......... .......... 56%  213M 1s\n",
            " 57050K .......... .......... .......... .......... .......... 56%  204M 1s\n",
            " 57100K .......... .......... .......... .......... .......... 56%  219M 1s\n",
            " 57150K .......... .......... .......... .......... .......... 56%  166M 1s\n",
            " 57200K .......... .......... .......... .......... .......... 56%  240M 1s\n",
            " 57250K .......... .......... .......... .......... .......... 56%  253M 1s\n",
            " 57300K .......... .......... .......... .......... .......... 56%  221M 1s\n",
            " 57350K .......... .......... .......... .......... .......... 56%  207M 1s\n",
            " 57400K .......... .......... .......... .......... .......... 56%  231M 1s\n",
            " 57450K .......... .......... .......... .......... .......... 56%  260M 1s\n",
            " 57500K .......... .......... .......... .......... .......... 56%  232M 1s\n",
            " 57550K .......... .......... .......... .......... .......... 56%  187M 1s\n",
            " 57600K .......... .......... .......... .......... .......... 56%  206M 1s\n",
            " 57650K .......... .......... .......... .......... .......... 56%  196M 1s\n",
            " 57700K .......... .......... .......... .......... .......... 57%  207M 1s\n",
            " 57750K .......... .......... .......... .......... .......... 57%  168M 1s\n",
            " 57800K .......... .......... .......... .......... .......... 57%  194M 1s\n",
            " 57850K .......... .......... .......... .......... .......... 57% 97.9M 1s\n",
            " 57900K .......... .......... .......... .......... .......... 57%  112M 1s\n",
            " 57950K .......... .......... .......... .......... .......... 57%  104M 1s\n",
            " 58000K .......... .......... .......... .......... .......... 57%  111M 1s\n",
            " 58050K .......... .......... .......... .......... .......... 57% 73.2M 1s\n",
            " 58100K .......... .......... .......... .......... .......... 57%  203M 1s\n",
            " 58150K .......... .......... .......... .......... .......... 57% 47.9M 1s\n",
            " 58200K .......... .......... .......... .......... .......... 57%  133M 1s\n",
            " 58250K .......... .......... .......... .......... .......... 57%  136M 1s\n",
            " 58300K .......... .......... .......... .......... .......... 57% 88.8M 1s\n",
            " 58350K .......... .......... .......... .......... .......... 57% 72.6M 1s\n",
            " 58400K .......... .......... .......... .......... .......... 57%  111M 1s\n",
            " 58450K .......... .......... .......... .......... .......... 57% 86.0M 1s\n",
            " 58500K .......... .......... .......... .......... .......... 57% 99.2M 1s\n",
            " 58550K .......... .......... .......... .......... .......... 57% 88.7M 1s\n",
            " 58600K .......... .......... .......... .......... .......... 57% 95.7M 1s\n",
            " 58650K .......... .......... .......... .......... .......... 57% 68.2M 1s\n",
            " 58700K .......... .......... .......... .......... .......... 57% 71.7M 1s\n",
            " 58750K .......... .......... .......... .......... .......... 58% 67.2M 1s\n",
            " 58800K .......... .......... .......... .......... .......... 58% 57.8M 1s\n",
            " 58850K .......... .......... .......... .......... .......... 58% 69.8M 1s\n",
            " 58900K .......... .......... .......... .......... .......... 58% 83.8M 1s\n",
            " 58950K .......... .......... .......... .......... .......... 58% 90.4M 1s\n",
            " 59000K .......... .......... .......... .......... .......... 58%  105M 1s\n",
            " 59050K .......... .......... .......... .......... .......... 58%  104M 1s\n",
            " 59100K .......... .......... .......... .......... .......... 58% 19.8M 1s\n",
            " 59150K .......... .......... .......... .......... .......... 58%  186M 1s\n",
            " 59200K .......... .......... .......... .......... .......... 58%  203M 1s\n",
            " 59250K .......... .......... .......... .......... .......... 58%  257M 1s\n",
            " 59300K .......... .......... .......... .......... .......... 58%  255M 1s\n",
            " 59350K .......... .......... .......... .......... .......... 58% 82.2M 1s\n",
            " 59400K .......... .......... .......... .......... .......... 58% 16.7M 1s\n",
            " 59450K .......... .......... .......... .......... .......... 58%  202M 1s\n",
            " 59500K .......... .......... .......... .......... .......... 58%  241M 1s\n",
            " 59550K .......... .......... .......... .......... .......... 58%  190M 1s\n",
            " 59600K .......... .......... .......... .......... .......... 58%  215M 1s\n",
            " 59650K .......... .......... .......... .......... .......... 58%  238M 1s\n",
            " 59700K .......... .......... .......... .......... .......... 58%  252M 1s\n",
            " 59750K .......... .......... .......... .......... .......... 59%  175M 1s\n",
            " 59800K .......... .......... .......... .......... .......... 59%  253M 1s\n",
            " 59850K .......... .......... .......... .......... .......... 59% 10.9M 1s\n",
            " 59900K .......... .......... .......... .......... .......... 59% 66.6M 1s\n",
            " 59950K .......... .......... .......... .......... .......... 59% 97.8M 1s\n",
            " 60000K .......... .......... .......... .......... .......... 59%  232M 1s\n",
            " 60050K .......... .......... .......... .......... .......... 59%  243M 1s\n",
            " 60100K .......... .......... .......... .......... .......... 59%  221M 1s\n",
            " 60150K .......... .......... .......... .......... .......... 59%  212M 1s\n",
            " 60200K .......... .......... .......... .......... .......... 59%  238M 1s\n",
            " 60250K .......... .......... .......... .......... .......... 59%  257M 1s\n",
            " 60300K .......... .......... .......... .......... .......... 59% 40.6M 1s\n",
            " 60350K .......... .......... .......... .......... .......... 59%  176M 1s\n",
            " 60400K .......... .......... .......... .......... .......... 59%  228M 1s\n",
            " 60450K .......... .......... .......... .......... .......... 59%  249M 1s\n",
            " 60500K .......... .......... .......... .......... .......... 59% 19.9M 1s\n",
            " 60550K .......... .......... .......... .......... .......... 59%  136M 1s\n",
            " 60600K .......... .......... .......... .......... .......... 59%  221M 1s\n",
            " 60650K .......... .......... .......... .......... .......... 59%  247M 1s\n",
            " 60700K .......... .......... .......... .......... .......... 59%  201M 1s\n",
            " 60750K .......... .......... .......... .......... .......... 60%  212M 0s\n",
            " 60800K .......... .......... .......... .......... .......... 60% 49.2M 0s\n",
            " 60850K .......... .......... .......... .......... .......... 60%  236M 0s\n",
            " 60900K .......... .......... .......... .......... .......... 60% 29.3M 0s\n",
            " 60950K .......... .......... .......... .......... .......... 60%  172M 0s\n",
            " 61000K .......... .......... .......... .......... .......... 60%  162M 0s\n",
            " 61050K .......... .......... .......... .......... .......... 60%  247M 0s\n",
            " 61100K .......... .......... .......... .......... .......... 60%  210M 0s\n",
            " 61150K .......... .......... .......... .......... .......... 60%  210M 0s\n",
            " 61200K .......... .......... .......... .......... .......... 60%  252M 0s\n",
            " 61250K .......... .......... .......... .......... .......... 60%  220M 0s\n",
            " 61300K .......... .......... .......... .......... .......... 60%  213M 0s\n",
            " 61350K .......... .......... .......... .......... .......... 60%  212M 0s\n",
            " 61400K .......... .......... .......... .......... .......... 60%  232M 0s\n",
            " 61450K .......... .......... .......... .......... .......... 60%  243M 0s\n",
            " 61500K .......... .......... .......... .......... .......... 60%  203M 0s\n",
            " 61550K .......... .......... .......... .......... .......... 60% 90.5M 0s\n",
            " 61600K .......... .......... .......... .......... .......... 60%  131M 0s\n",
            " 61650K .......... .......... .......... .......... .......... 60% 86.8M 0s\n",
            " 61700K .......... .......... .......... .......... .......... 60%  211M 0s\n",
            " 61750K .......... .......... .......... .......... .......... 61% 58.6M 0s\n",
            " 61800K .......... .......... .......... .......... .......... 61% 29.6M 0s\n",
            " 61850K .......... .......... .......... .......... .......... 61%  250M 0s\n",
            " 61900K .......... .......... .......... .......... .......... 61%  151M 0s\n",
            " 61950K .......... .......... .......... .......... .......... 61% 25.5M 0s\n",
            " 62000K .......... .......... .......... .......... .......... 61%  214M 0s\n",
            " 62050K .......... .......... .......... .......... .......... 61%  212M 0s\n",
            " 62100K .......... .......... .......... .......... .......... 61%  249M 0s\n",
            " 62150K .......... .......... .......... .......... .......... 61%  227M 0s\n",
            " 62200K .......... .......... .......... .......... .......... 61%  243M 0s\n",
            " 62250K .......... .......... .......... .......... .......... 61% 17.1M 0s\n",
            " 62300K .......... .......... .......... .......... .......... 61%  210M 0s\n",
            " 62350K .......... .......... .......... .......... .......... 61%  222M 0s\n",
            " 62400K .......... .......... .......... .......... .......... 61%  231M 0s\n",
            " 62450K .......... .......... .......... .......... .......... 61% 41.7M 0s\n",
            " 62500K .......... .......... .......... .......... .......... 61%  178M 0s\n",
            " 62550K .......... .......... .......... .......... .......... 61%  205M 0s\n",
            " 62600K .......... .......... .......... .......... .......... 61%  234M 0s\n",
            " 62650K .......... .......... .......... .......... .......... 61%  253M 0s\n",
            " 62700K .......... .......... .......... .......... .......... 61% 27.3M 0s\n",
            " 62750K .......... .......... .......... .......... .......... 61%  153M 0s\n",
            " 62800K .......... .......... .......... .......... .......... 62%  199M 0s\n",
            " 62850K .......... .......... .......... .......... .......... 62%  233M 0s\n",
            " 62900K .......... .......... .......... .......... .......... 62%  219M 0s\n",
            " 62950K .......... .......... .......... .......... .......... 62%  190M 0s\n",
            " 63000K .......... .......... .......... .......... .......... 62%  217M 0s\n",
            " 63050K .......... .......... .......... .......... .......... 62%  258M 0s\n",
            " 63100K .......... .......... .......... .......... .......... 62%  261M 0s\n",
            " 63150K .......... .......... .......... .......... .......... 62%  181M 0s\n",
            " 63200K .......... .......... .......... .......... .......... 62%  233M 0s\n",
            " 63250K .......... .......... .......... .......... .......... 62%  217M 0s\n",
            " 63300K .......... .......... .......... .......... .......... 62%  227M 0s\n",
            " 63350K .......... .......... .......... .......... .......... 62%  180M 0s\n",
            " 63400K .......... .......... .......... .......... .......... 62%  154M 0s\n",
            " 63450K .......... .......... .......... .......... .......... 62% 91.0M 0s\n",
            " 63500K .......... .......... .......... .......... .......... 62% 60.2M 0s\n",
            " 63550K .......... .......... .......... .......... .......... 62%  184M 0s\n",
            " 63600K .......... .......... .......... .......... .......... 62%  187M 0s\n",
            " 63650K .......... .......... .......... .......... .......... 62%  153M 0s\n",
            " 63700K .......... .......... .......... .......... .......... 62%  100M 0s\n",
            " 63750K .......... .......... .......... .......... .......... 62% 75.8M 0s\n",
            " 63800K .......... .......... .......... .......... .......... 63% 70.9M 0s\n",
            " 63850K .......... .......... .......... .......... .......... 63%  216M 0s\n",
            " 63900K .......... .......... .......... .......... .......... 63%  108M 0s\n",
            " 63950K .......... .......... .......... .......... .......... 63%  122M 0s\n",
            " 64000K .......... .......... .......... .......... .......... 63% 89.0M 0s\n",
            " 64050K .......... .......... .......... .......... .......... 63% 92.5M 0s\n",
            " 64100K .......... .......... .......... .......... .......... 63%  131M 0s\n",
            " 64150K .......... .......... .......... .......... .......... 63% 65.9M 0s\n",
            " 64200K .......... .......... .......... .......... .......... 63% 41.3M 0s\n",
            " 64250K .......... .......... .......... .......... .......... 63%  209M 0s\n",
            " 64300K .......... .......... .......... .......... .......... 63% 16.8M 0s\n",
            " 64350K .......... .......... .......... .......... .......... 63%  186M 0s\n",
            " 64400K .......... .......... .......... .......... .......... 63%  250M 0s\n",
            " 64450K .......... .......... .......... .......... .......... 63%  234M 0s\n",
            " 64500K .......... .......... .......... .......... .......... 63% 32.9M 0s\n",
            " 64550K .......... .......... .......... .......... .......... 63%  200M 0s\n",
            " 64600K .......... .......... .......... .......... .......... 63% 40.9M 0s\n",
            " 64650K .......... .......... .......... .......... .......... 63% 82.3M 0s\n",
            " 64700K .......... .......... .......... .......... .......... 63% 13.7M 0s\n",
            " 64750K .......... .......... .......... .......... .......... 63%  188M 0s\n",
            " 64800K .......... .......... .......... .......... .......... 64%  227M 0s\n",
            " 64850K .......... .......... .......... .......... .......... 64%  155M 0s\n",
            " 64900K .......... .......... .......... .......... .......... 64% 93.6M 0s\n",
            " 64950K .......... .......... .......... .......... .......... 64% 15.3M 0s\n",
            " 65000K .......... .......... .......... .......... .......... 64%  218M 0s\n",
            " 65050K .......... .......... .......... .......... .......... 64%  257M 0s\n",
            " 65100K .......... .......... .......... .......... .......... 64%  261M 0s\n",
            " 65150K .......... .......... .......... .......... .......... 64% 91.5M 0s\n",
            " 65200K .......... .......... .......... .......... .......... 64% 48.6M 0s\n",
            " 65250K .......... .......... .......... .......... .......... 64% 33.8M 0s\n",
            " 65300K .......... .......... .......... .......... .......... 64%  213M 0s\n",
            " 65350K .......... .......... .......... .......... .......... 64% 25.9M 0s\n",
            " 65400K .......... .......... .......... .......... .......... 64%  187M 0s\n",
            " 65450K .......... .......... .......... .......... .......... 64%  291M 0s\n",
            " 65500K .......... .......... .......... .......... .......... 64%  191M 0s\n",
            " 65550K .......... .......... .......... .......... .......... 64%  222M 0s\n",
            " 65600K .......... .......... .......... .......... .......... 64%  187M 0s\n",
            " 65650K .......... .......... .......... .......... .......... 64% 29.3M 0s\n",
            " 65700K .......... .......... .......... .......... .......... 64% 29.6M 0s\n",
            " 65750K .......... .......... .......... .......... .......... 64%  219M 0s\n",
            " 65800K .......... .......... .......... .......... .......... 65%  261M 0s\n",
            " 65850K .......... .......... .......... .......... .......... 65%  239M 0s\n",
            " 65900K .......... .......... .......... .......... .......... 65% 56.5M 0s\n",
            " 65950K .......... .......... .......... .......... .......... 65%  172M 0s\n",
            " 66000K .......... .......... .......... .......... .......... 65%  205M 0s\n",
            " 66050K .......... .......... .......... .......... .......... 65%  189M 0s\n",
            " 66100K .......... .......... .......... .......... .......... 65%  254M 0s\n",
            " 66150K .......... .......... .......... .......... .......... 65%  165M 0s\n",
            " 66200K .......... .......... .......... .......... .......... 65%  215M 0s\n",
            " 66250K .......... .......... .......... .......... .......... 65%  222M 0s\n",
            " 66300K .......... .......... .......... .......... .......... 65%  234M 0s\n",
            " 66350K .......... .......... .......... .......... .......... 65%  157M 0s\n",
            " 66400K .......... .......... .......... .......... .......... 65%  123M 0s\n",
            " 66450K .......... .......... .......... .......... .......... 65%  195M 0s\n",
            " 66500K .......... .......... .......... .......... .......... 65%  186M 0s\n",
            " 66550K .......... .......... .......... .......... .......... 65% 56.9M 0s\n",
            " 66600K .......... .......... .......... .......... .......... 65% 80.8M 0s\n",
            " 66650K .......... .......... .......... .......... .......... 65%  125M 0s\n",
            " 66700K .......... .......... .......... .......... .......... 65% 84.8M 0s\n",
            " 66750K .......... .......... .......... .......... .......... 65% 18.3M 0s\n",
            " 66800K .......... .......... .......... .......... .......... 65%  204M 0s\n",
            " 66850K .......... .......... .......... .......... .......... 66%  246M 0s\n",
            " 66900K .......... .......... .......... .......... .......... 66%  242M 0s\n",
            " 66950K .......... .......... .......... .......... .......... 66%  194M 0s\n",
            " 67000K .......... .......... .......... .......... .......... 66%  250M 0s\n",
            " 67050K .......... .......... .......... .......... .......... 66%  261M 0s\n",
            " 67100K .......... .......... .......... .......... .......... 66% 48.4M 0s\n",
            " 67150K .......... .......... .......... .......... .......... 66%  202M 0s\n",
            " 67200K .......... .......... .......... .......... .......... 66% 81.3M 0s\n",
            " 67250K .......... .......... .......... .......... .......... 66% 59.8M 0s\n",
            " 67300K .......... .......... .......... .......... .......... 66% 26.1M 0s\n",
            " 67350K .......... .......... .......... .......... .......... 66%  176M 0s\n",
            " 67400K .......... .......... .......... .......... .......... 66%  228M 0s\n",
            " 67450K .......... .......... .......... .......... .......... 66%  235M 0s\n",
            " 67500K .......... .......... .......... .......... .......... 66% 11.3M 0s\n",
            " 67550K .......... .......... .......... .......... .......... 66%  132M 0s\n",
            " 67600K .......... .......... .......... .......... .......... 66%  236M 0s\n",
            " 67650K .......... .......... .......... .......... .......... 66%  249M 0s\n",
            " 67700K .......... .......... .......... .......... .......... 66% 39.8M 0s\n",
            " 67750K .......... .......... .......... .......... .......... 66%  101M 0s\n",
            " 67800K .......... .......... .......... .......... .......... 66%  200M 0s\n",
            " 67850K .......... .......... .......... .......... .......... 67%  224M 0s\n",
            " 67900K .......... .......... .......... .......... .......... 67%  150M 0s\n",
            " 67950K .......... .......... .......... .......... .......... 67% 24.4M 0s\n",
            " 68000K .......... .......... .......... .......... .......... 67%  142M 0s\n",
            " 68050K .......... .......... .......... .......... .......... 67%  215M 0s\n",
            " 68100K .......... .......... .......... .......... .......... 67%  217M 0s\n",
            " 68150K .......... .......... .......... .......... .......... 67%  216M 0s\n",
            " 68200K .......... .......... .......... .......... .......... 67% 44.6M 0s\n",
            " 68250K .......... .......... .......... .......... .......... 67%  161M 0s\n",
            " 68300K .......... .......... .......... .......... .......... 67%  212M 0s\n",
            " 68350K .......... .......... .......... .......... .......... 67%  174M 0s\n",
            " 68400K .......... .......... .......... .......... .......... 67%  237M 0s\n",
            " 68450K .......... .......... .......... .......... .......... 67%  199M 0s\n",
            " 68500K .......... .......... .......... .......... .......... 67%  243M 0s\n",
            " 68550K .......... .......... .......... .......... .......... 67%  182M 0s\n",
            " 68600K .......... .......... .......... .......... .......... 67%  231M 0s\n",
            " 68650K .......... .......... .......... .......... .......... 67%  253M 0s\n",
            " 68700K .......... .......... .......... .......... .......... 67%  258M 0s\n",
            " 68750K .......... .......... .......... .......... .......... 67% 12.3M 0s\n",
            " 68800K .......... .......... .......... .......... .......... 67%  239M 0s\n",
            " 68850K .......... .......... .......... .......... .......... 68%  209M 0s\n",
            " 68900K .......... .......... .......... .......... .......... 68%  226M 0s\n",
            " 68950K .......... .......... .......... .......... .......... 68%  220M 0s\n",
            " 69000K .......... .......... .......... .......... .......... 68%  250M 0s\n",
            " 69050K .......... .......... .......... .......... .......... 68% 5.01M 0s\n",
            " 69100K .......... .......... .......... .......... .......... 68%  195M 0s\n",
            " 69150K .......... .......... .......... .......... .......... 68%  208M 0s\n",
            " 69200K .......... .......... .......... .......... .......... 68%  228M 0s\n",
            " 69250K .......... .......... .......... .......... .......... 68% 8.51M 0s\n",
            " 69300K .......... .......... .......... .......... .......... 68%  172M 0s\n",
            " 69350K .......... .......... .......... .......... .......... 68%  207M 0s\n",
            " 69400K .......... .......... .......... .......... .......... 68%  245M 0s\n",
            " 69450K .......... .......... .......... .......... .......... 68%  251M 0s\n",
            " 69500K .......... .......... .......... .......... .......... 68% 18.4M 0s\n",
            " 69550K .......... .......... .......... .......... .......... 68%  177M 0s\n",
            " 69600K .......... .......... .......... .......... .......... 68%  216M 0s\n",
            " 69650K .......... .......... .......... .......... .......... 68%  234M 0s\n",
            " 69700K .......... .......... .......... .......... .......... 68%  257M 0s\n",
            " 69750K .......... .......... .......... .......... .......... 68%  197M 0s\n",
            " 69800K .......... .......... .......... .......... .......... 68% 57.6M 0s\n",
            " 69850K .......... .......... .......... .......... .......... 69%  229M 0s\n",
            " 69900K .......... .......... .......... .......... .......... 69%  224M 0s\n",
            " 69950K .......... .......... .......... .......... .......... 69%  170M 0s\n",
            " 70000K .......... .......... .......... .......... .......... 69%  212M 0s\n",
            " 70050K .......... .......... .......... .......... .......... 69%  213M 0s\n",
            " 70100K .......... .......... .......... .......... .......... 69%  235M 0s\n",
            " 70150K .......... .......... .......... .......... .......... 69%  149M 0s\n",
            " 70200K .......... .......... .......... .......... .......... 69%  210M 0s\n",
            " 70250K .......... .......... .......... .......... .......... 69%  207M 0s\n",
            " 70300K .......... .......... .......... .......... .......... 69%  206M 0s\n",
            " 70350K .......... .......... .......... .......... .......... 69%  164M 0s\n",
            " 70400K .......... .......... .......... .......... .......... 69%  211M 0s\n",
            " 70450K .......... .......... .......... .......... .......... 69%  187M 0s\n",
            " 70500K .......... .......... .......... .......... .......... 69%  213M 0s\n",
            " 70550K .......... .......... .......... .......... .......... 69%  159M 0s\n",
            " 70600K .......... .......... .......... .......... .......... 69%  234M 0s\n",
            " 70650K .......... .......... .......... .......... .......... 69%  221M 0s\n",
            " 70700K .......... .......... .......... .......... .......... 69%  220M 0s\n",
            " 70750K .......... .......... .......... .......... .......... 69%  164M 0s\n",
            " 70800K .......... .......... .......... .......... .......... 69%  205M 0s\n",
            " 70850K .......... .......... .......... .......... .......... 69%  208M 0s\n",
            " 70900K .......... .......... .......... .......... .......... 70%  241M 0s\n",
            " 70950K .......... .......... .......... .......... .......... 70%  214M 0s\n",
            " 71000K .......... .......... .......... .......... .......... 70%  251M 0s\n",
            " 71050K .......... .......... .......... .......... .......... 70%  261M 0s\n",
            " 71100K .......... .......... .......... .......... .......... 70%  218M 0s\n",
            " 71150K .......... .......... .......... .......... .......... 70%  163M 0s\n",
            " 71200K .......... .......... .......... .......... .......... 70%  217M 0s\n",
            " 71250K .......... .......... .......... .......... .......... 70%  202M 0s\n",
            " 71300K .......... .......... .......... .......... .......... 70%  211M 0s\n",
            " 71350K .......... .......... .......... .......... .......... 70%  224M 0s\n",
            " 71400K .......... .......... .......... .......... .......... 70%  242M 0s\n",
            " 71450K .......... .......... .......... .......... .......... 70%  258M 0s\n",
            " 71500K .......... .......... .......... .......... .......... 70%  259M 0s\n",
            " 71550K .......... .......... .......... .......... .......... 70%  203M 0s\n",
            " 71600K .......... .......... .......... .......... .......... 70%  239M 0s\n",
            " 71650K .......... .......... .......... .......... .......... 70%  246M 0s\n",
            " 71700K .......... .......... .......... .......... .......... 70%  261M 0s\n",
            " 71750K .......... .......... .......... .......... .......... 70%  205M 0s\n",
            " 71800K .......... .......... .......... .......... .......... 70%  259M 0s\n",
            " 71850K .......... .......... .......... .......... .......... 70%  260M 0s\n",
            " 71900K .......... .......... .......... .......... .......... 71%  242M 0s\n",
            " 71950K .......... .......... .......... .......... .......... 71%  228M 0s\n",
            " 72000K .......... .......... .......... .......... .......... 71%  253M 0s\n",
            " 72050K .......... .......... .......... .......... .......... 71%  250M 0s\n",
            " 72100K .......... .......... .......... .......... .......... 71%  259M 0s\n",
            " 72150K .......... .......... .......... .......... .......... 71%  213M 0s\n",
            " 72200K .......... .......... .......... .......... .......... 71%  267M 0s\n",
            " 72250K .......... .......... .......... .......... .......... 71%  256M 0s\n",
            " 72300K .......... .......... .......... .......... .......... 71%  255M 0s\n",
            " 72350K .......... .......... .......... .......... .......... 71%  226M 0s\n",
            " 72400K .......... .......... .......... .......... .......... 71%  189M 0s\n",
            " 72450K .......... .......... .......... .......... .......... 71%  231M 0s\n",
            " 72500K .......... .......... .......... .......... .......... 71%  208M 0s\n",
            " 72550K .......... .......... .......... .......... .......... 71%  191M 0s\n",
            " 72600K .......... .......... .......... .......... .......... 71%  204M 0s\n",
            " 72650K .......... .......... .......... .......... .......... 71%  252M 0s\n",
            " 72700K .......... .......... .......... .......... .......... 71%  255M 0s\n",
            " 72750K .......... .......... .......... .......... .......... 71%  172M 0s\n",
            " 72800K .......... .......... .......... .......... .......... 71%  163M 0s\n",
            " 72850K .......... .......... .......... .......... .......... 71%  249M 0s\n",
            " 72900K .......... .......... .......... .......... .......... 72%  256M 0s\n",
            " 72950K .......... .......... .......... .......... .......... 72%  208M 0s\n",
            " 73000K .......... .......... .......... .......... .......... 72%  217M 0s\n",
            " 73050K .......... .......... .......... .......... .......... 72%  213M 0s\n",
            " 73100K .......... .......... .......... .......... .......... 72%  231M 0s\n",
            " 73150K .......... .......... .......... .......... .......... 72%  213M 0s\n",
            " 73200K .......... .......... .......... .......... .......... 72% 15.5M 0s\n",
            " 73250K .......... .......... .......... .......... .......... 72% 46.6M 0s\n",
            " 73300K .......... .......... .......... .......... .......... 72%  359M 0s\n",
            " 73350K .......... .......... .......... .......... .......... 72%  321M 0s\n",
            " 73400K .......... .......... .......... .......... .......... 72%  432M 0s\n",
            " 73450K .......... .......... .......... .......... .......... 72%  432M 0s\n",
            " 73500K .......... .......... .......... .......... .......... 72% 26.5M 0s\n",
            " 73550K .......... .......... .......... .......... .......... 72%  181M 0s\n",
            " 73600K .......... .......... .......... .......... .......... 72%  164M 0s\n",
            " 73650K .......... .......... .......... .......... .......... 72%  186M 0s\n",
            " 73700K .......... .......... .......... .......... .......... 72%  154M 0s\n",
            " 73750K .......... .......... .......... .......... .......... 72%  221M 0s\n",
            " 73800K .......... .......... .......... .......... .......... 72%  254M 0s\n",
            " 73850K .......... .......... .......... .......... .......... 72%  256M 0s\n",
            " 73900K .......... .......... .......... .......... .......... 73%  234M 0s\n",
            " 73950K .......... .......... .......... .......... .......... 73%  136M 0s\n",
            " 74000K .......... .......... .......... .......... .......... 73%  237M 0s\n",
            " 74050K .......... .......... .......... .......... .......... 73%  240M 0s\n",
            " 74100K .......... .......... .......... .......... .......... 73%  239M 0s\n",
            " 74150K .......... .......... .......... .......... .......... 73% 78.3M 0s\n",
            " 74200K .......... .......... .......... .......... .......... 73%  187M 0s\n",
            " 74250K .......... .......... .......... .......... .......... 73%  229M 0s\n",
            " 74300K .......... .......... .......... .......... .......... 73% 13.3M 0s\n",
            " 74350K .......... .......... .......... .......... .......... 73%  125M 0s\n",
            " 74400K .......... .......... .......... .......... .......... 73%  188M 0s\n",
            " 74450K .......... .......... .......... .......... .......... 73%  248M 0s\n",
            " 74500K .......... .......... .......... .......... .......... 73%  260M 0s\n",
            " 74550K .......... .......... .......... .......... .......... 73%  200M 0s\n",
            " 74600K .......... .......... .......... .......... .......... 73%  260M 0s\n",
            " 74650K .......... .......... .......... .......... .......... 73%  253M 0s\n",
            " 74700K .......... .......... .......... .......... .......... 73%  250M 0s\n",
            " 74750K .......... .......... .......... .......... .......... 73% 6.40M 0s\n",
            " 74800K .......... .......... .......... .......... .......... 73%  236M 0s\n",
            " 74850K .......... .......... .......... .......... .......... 73% 18.3M 0s\n",
            " 74900K .......... .......... .......... .......... .......... 73%  252M 0s\n",
            " 74950K .......... .......... .......... .......... .......... 74%  211M 0s\n",
            " 75000K .......... .......... .......... .......... .......... 74% 19.0M 0s\n",
            " 75050K .......... .......... .......... .......... .......... 74%  175M 0s\n",
            " 75100K .......... .......... .......... .......... .......... 74%  108M 0s\n",
            " 75150K .......... .......... .......... .......... .......... 74%  127M 0s\n",
            " 75200K .......... .......... .......... .......... .......... 74%  199M 0s\n",
            " 75250K .......... .......... .......... .......... .......... 74%  213M 0s\n",
            " 75300K .......... .......... .......... .......... .......... 74%  233M 0s\n",
            " 75350K .......... .......... .......... .......... .......... 74%  219M 0s\n",
            " 75400K .......... .......... .......... .......... .......... 74%  255M 0s\n",
            " 75450K .......... .......... .......... .......... .......... 74%  174M 0s\n",
            " 75500K .......... .......... .......... .......... .......... 74%  143M 0s\n",
            " 75550K .......... .......... .......... .......... .......... 74%  170M 0s\n",
            " 75600K .......... .......... .......... .......... .......... 74%  186M 0s\n",
            " 75650K .......... .......... .......... .......... .......... 74%  204M 0s\n",
            " 75700K .......... .......... .......... .......... .......... 74%  223M 0s\n",
            " 75750K .......... .......... .......... .......... .......... 74%  204M 0s\n",
            " 75800K .......... .......... .......... .......... .......... 74% 54.6M 0s\n",
            " 75850K .......... .......... .......... .......... .......... 74%  197M 0s\n",
            " 75900K .......... .......... .......... .......... .......... 74%  214M 0s\n",
            " 75950K .......... .......... .......... .......... .......... 75%  117M 0s\n",
            " 76000K .......... .......... .......... .......... .......... 75%  199M 0s\n",
            " 76050K .......... .......... .......... .......... .......... 75%  133M 0s\n",
            " 76100K .......... .......... .......... .......... .......... 75% 87.8M 0s\n",
            " 76150K .......... .......... .......... .......... .......... 75%  135M 0s\n",
            " 76200K .......... .......... .......... .......... .......... 75%  128M 0s\n",
            " 76250K .......... .......... .......... .......... .......... 75% 69.7M 0s\n",
            " 76300K .......... .......... .......... .......... .......... 75%  146M 0s\n",
            " 76350K .......... .......... .......... .......... .......... 75% 23.3M 0s\n",
            " 76400K .......... .......... .......... .......... .......... 75%  180M 0s\n",
            " 76450K .......... .......... .......... .......... .......... 75%  259M 0s\n",
            " 76500K .......... .......... .......... .......... .......... 75%  262M 0s\n",
            " 76550K .......... .......... .......... .......... .......... 75%  106M 0s\n",
            " 76600K .......... .......... .......... .......... .......... 75%  212M 0s\n",
            " 76650K .......... .......... .......... .......... .......... 75%  295M 0s\n",
            " 76700K .......... .......... .......... .......... .......... 75%  117M 0s\n",
            " 76750K .......... .......... .......... .......... .......... 75% 78.1M 0s\n",
            " 76800K .......... .......... .......... .......... .......... 75% 98.6M 0s\n",
            " 76850K .......... .......... .......... .......... .......... 75% 26.1M 0s\n",
            " 76900K .......... .......... .......... .......... .......... 75%  201M 0s\n",
            " 76950K .......... .......... .......... .......... .......... 76%  243M 0s\n",
            " 77000K .......... .......... .......... .......... .......... 76%  227M 0s\n",
            " 77050K .......... .......... .......... .......... .......... 76%  283M 0s\n",
            " 77100K .......... .......... .......... .......... .......... 76%  103M 0s\n",
            " 77150K .......... .......... .......... .......... .......... 76% 28.2M 0s\n",
            " 77200K .......... .......... .......... .......... .......... 76%  243M 0s\n",
            " 77250K .......... .......... .......... .......... .......... 76%  210M 0s\n",
            " 77300K .......... .......... .......... .......... .......... 76%  287M 0s\n",
            " 77350K .......... .......... .......... .......... .......... 76%  238M 0s\n",
            " 77400K .......... .......... .......... .......... .......... 76% 30.1M 0s\n",
            " 77450K .......... .......... .......... .......... .......... 76%  129M 0s\n",
            " 77500K .......... .......... .......... .......... .......... 76%  127M 0s\n",
            " 77550K .......... .......... .......... .......... .......... 76%  131M 0s\n",
            " 77600K .......... .......... .......... .......... .......... 76%  146M 0s\n",
            " 77650K .......... .......... .......... .......... .......... 76%  141M 0s\n",
            " 77700K .......... .......... .......... .......... .......... 76%  178M 0s\n",
            " 77750K .......... .......... .......... .......... .......... 76%  205M 0s\n",
            " 77800K .......... .......... .......... .......... .......... 76%  234M 0s\n",
            " 77850K .......... .......... .......... .......... .......... 76%  256M 0s\n",
            " 77900K .......... .......... .......... .......... .......... 76%  163M 0s\n",
            " 77950K .......... .......... .......... .......... .......... 76%  177M 0s\n",
            " 78000K .......... .......... .......... .......... .......... 77%  267M 0s\n",
            " 78050K .......... .......... .......... .......... .......... 77%  239M 0s\n",
            " 78100K .......... .......... .......... .......... .......... 77%  255M 0s\n",
            " 78150K .......... .......... .......... .......... .......... 77%  221M 0s\n",
            " 78200K .......... .......... .......... .......... .......... 77%  265M 0s\n",
            " 78250K .......... .......... .......... .......... .......... 77%  174M 0s\n",
            " 78300K .......... .......... .......... .......... .......... 77%  158M 0s\n",
            " 78350K .......... .......... .......... .......... .......... 77%  163M 0s\n",
            " 78400K .......... .......... .......... .......... .......... 77%  118M 0s\n",
            " 78450K .......... .......... .......... .......... .......... 77%  163M 0s\n",
            " 78500K .......... .......... .......... .......... .......... 77%  152M 0s\n",
            " 78550K .......... .......... .......... .......... .......... 77%  123M 0s\n",
            " 78600K .......... .......... .......... .......... .......... 77%  132M 0s\n",
            " 78650K .......... .......... .......... .......... .......... 77%  146M 0s\n",
            " 78700K .......... .......... .......... .......... .......... 77%  163M 0s\n",
            " 78750K .......... .......... .......... .......... .......... 77%  225M 0s\n",
            " 78800K .......... .......... .......... .......... .......... 77%  238M 0s\n",
            " 78850K .......... .......... .......... .......... .......... 77%  139M 0s\n",
            " 78900K .......... .......... .......... .......... .......... 77%  173M 0s\n",
            " 78950K .......... .......... .......... .......... .......... 77%  160M 0s\n",
            " 79000K .......... .......... .......... .......... .......... 78%  147M 0s\n",
            " 79050K .......... .......... .......... .......... .......... 78%  261M 0s\n",
            " 79100K .......... .......... .......... .......... .......... 78%  140M 0s\n",
            " 79150K .......... .......... .......... .......... .......... 78%  139M 0s\n",
            " 79200K .......... .......... .......... .......... .......... 78%  136M 0s\n",
            " 79250K .......... .......... .......... .......... .......... 78%  152M 0s\n",
            " 79300K .......... .......... .......... .......... .......... 78%  161M 0s\n",
            " 79350K .......... .......... .......... .......... .......... 78%  109M 0s\n",
            " 79400K .......... .......... .......... .......... .......... 78% 34.1M 0s\n",
            " 79450K .......... .......... .......... .......... .......... 78%  145M 0s\n",
            " 79500K .......... .......... .......... .......... .......... 78%  153M 0s\n",
            " 79550K .......... .......... .......... .......... .......... 78%  145M 0s\n",
            " 79600K .......... .......... .......... .......... .......... 78%  229M 0s\n",
            " 79650K .......... .......... .......... .......... .......... 78%  161M 0s\n",
            " 79700K .......... .......... .......... .......... .......... 78%  185M 0s\n",
            " 79750K .......... .......... .......... .......... .......... 78%  192M 0s\n",
            " 79800K .......... .......... .......... .......... .......... 78%  222M 0s\n",
            " 79850K .......... .......... .......... .......... .......... 78%  137M 0s\n",
            " 79900K .......... .......... .......... .......... .......... 78%  237M 0s\n",
            " 79950K .......... .......... .......... .......... .......... 78%  139M 0s\n",
            " 80000K .......... .......... .......... .......... .......... 79%  199M 0s\n",
            " 80050K .......... .......... .......... .......... .......... 79%  191M 0s\n",
            " 80100K .......... .......... .......... .......... .......... 79%  206M 0s\n",
            " 80150K .......... .......... .......... .......... .......... 79%  107M 0s\n",
            " 80200K .......... .......... .......... .......... .......... 79%  183M 0s\n",
            " 80250K .......... .......... .......... .......... .......... 79%  152M 0s\n",
            " 80300K .......... .......... .......... .......... .......... 79%  225M 0s\n",
            " 80350K .......... .......... .......... .......... .......... 79%  203M 0s\n",
            " 80400K .......... .......... .......... .......... .......... 79%  181M 0s\n",
            " 80450K .......... .......... .......... .......... .......... 79%  175M 0s\n",
            " 80500K .......... .......... .......... .......... .......... 79%  164M 0s\n",
            " 80550K .......... .......... .......... .......... .......... 79%  105M 0s\n",
            " 80600K .......... .......... .......... .......... .......... 79%  182M 0s\n",
            " 80650K .......... .......... .......... .......... .......... 79%  207M 0s\n",
            " 80700K .......... .......... .......... .......... .......... 79%  184M 0s\n",
            " 80750K .......... .......... .......... .......... .......... 79%  195M 0s\n",
            " 80800K .......... .......... .......... .......... .......... 79%  232M 0s\n",
            " 80850K .......... .......... .......... .......... .......... 79%  164M 0s\n",
            " 80900K .......... .......... .......... .......... .......... 79%  132M 0s\n",
            " 80950K .......... .......... .......... .......... .......... 79%  152M 0s\n",
            " 81000K .......... .......... .......... .......... .......... 80%  169M 0s\n",
            " 81050K .......... .......... .......... .......... .......... 80%  103M 0s\n",
            " 81100K .......... .......... .......... .......... .......... 80%  219M 0s\n",
            " 81150K .......... .......... .......... .......... .......... 80%  170M 0s\n",
            " 81200K .......... .......... .......... .......... .......... 80%  240M 0s\n",
            " 81250K .......... .......... .......... .......... .......... 80%  152M 0s\n",
            " 81300K .......... .......... .......... .......... .......... 80%  198M 0s\n",
            " 81350K .......... .......... .......... .......... .......... 80%  228M 0s\n",
            " 81400K .......... .......... .......... .......... .......... 80%  256M 0s\n",
            " 81450K .......... .......... .......... .......... .......... 80%  233M 0s\n",
            " 81500K .......... .......... .......... .......... .......... 80%  252M 0s\n",
            " 81550K .......... .......... .......... .......... .......... 80%  213M 0s\n",
            " 81600K .......... .......... .......... .......... .......... 80%  258M 0s\n",
            " 81650K .......... .......... .......... .......... .......... 80%  157M 0s\n",
            " 81700K .......... .......... .......... .......... .......... 80%  164M 0s\n",
            " 81750K .......... .......... .......... .......... .......... 80%  194M 0s\n",
            " 81800K .......... .......... .......... .......... .......... 80%  251M 0s\n",
            " 81850K .......... .......... .......... .......... .......... 80%  173M 0s\n",
            " 81900K .......... .......... .......... .......... .......... 80%  235M 0s\n",
            " 81950K .......... .......... .......... .......... .......... 80%  181M 0s\n",
            " 82000K .......... .......... .......... .......... .......... 80%  248M 0s\n",
            " 82050K .......... .......... .......... .......... .......... 81%  245M 0s\n",
            " 82100K .......... .......... .......... .......... .......... 81%  177M 0s\n",
            " 82150K .......... .......... .......... .......... .......... 81% 96.8M 0s\n",
            " 82200K .......... .......... .......... .......... .......... 81%  260M 0s\n",
            " 82250K .......... .......... .......... .......... .......... 81%  231M 0s\n",
            " 82300K .......... .......... .......... .......... .......... 81%  265M 0s\n",
            " 82350K .......... .......... .......... .......... .......... 81%  213M 0s\n",
            " 82400K .......... .......... .......... .......... .......... 81%  257M 0s\n",
            " 82450K .......... .......... .......... .......... .......... 81%  223M 0s\n",
            " 82500K .......... .......... .......... .......... .......... 81% 60.5M 0s\n",
            " 82550K .......... .......... .......... .......... .......... 81%  187M 0s\n",
            " 82600K .......... .......... .......... .......... .......... 81%  306M 0s\n",
            " 82650K .......... .......... .......... .......... .......... 81%  307M 0s\n",
            " 82700K .......... .......... .......... .......... .......... 81%  310M 0s\n",
            " 82750K .......... .......... .......... .......... .......... 81%  162M 0s\n",
            " 82800K .......... .......... .......... .......... .......... 81%  314M 0s\n",
            " 82850K .......... .......... .......... .......... .......... 81% 88.6M 0s\n",
            " 82900K .......... .......... .......... .......... .......... 81%  240M 0s\n",
            " 82950K .......... .......... .......... .......... .......... 81%  101M 0s\n",
            " 83000K .......... .......... .......... .......... .......... 81%  295M 0s\n",
            " 83050K .......... .......... .......... .......... .......... 82%  281M 0s\n",
            " 83100K .......... .......... .......... .......... .......... 82%  301M 0s\n",
            " 83150K .......... .......... .......... .......... .......... 82%  258M 0s\n",
            " 83200K .......... .......... .......... .......... .......... 82%  200M 0s\n",
            " 83250K .......... .......... .......... .......... .......... 82%  234M 0s\n",
            " 83300K .......... .......... .......... .......... .......... 82%  270M 0s\n",
            " 83350K .......... .......... .......... .......... .......... 82%  170M 0s\n",
            " 83400K .......... .......... .......... .......... .......... 82%  136M 0s\n",
            " 83450K .......... .......... .......... .......... .......... 82%  193M 0s\n",
            " 83500K .......... .......... .......... .......... .......... 82%  256M 0s\n",
            " 83550K .......... .......... .......... .......... .......... 82%  135M 0s\n",
            " 83600K .......... .......... .......... .......... .......... 82%  231M 0s\n",
            " 83650K .......... .......... .......... .......... .......... 82%  227M 0s\n",
            " 83700K .......... .......... .......... .......... .......... 82%  236M 0s\n",
            " 83750K .......... .......... .......... .......... .......... 82% 57.8M 0s\n",
            " 83800K .......... .......... .......... .......... .......... 82%  279M 0s\n",
            " 83850K .......... .......... .......... .......... .......... 82%  292M 0s\n",
            " 83900K .......... .......... .......... .......... .......... 82%  293M 0s\n",
            " 83950K .......... .......... .......... .......... .......... 82%  216M 0s\n",
            " 84000K .......... .......... .......... .......... .......... 82%  107M 0s\n",
            " 84050K .......... .......... .......... .......... .......... 83% 80.7M 0s\n",
            " 84100K .......... .......... .......... .......... .......... 83%  205M 0s\n",
            " 84150K .......... .......... .......... .......... .......... 83% 80.1M 0s\n",
            " 84200K .......... .......... .......... .......... .......... 83%  197M 0s\n",
            " 84250K .......... .......... .......... .......... .......... 83%  290M 0s\n",
            " 84300K .......... .......... .......... .......... .......... 83%  266M 0s\n",
            " 84350K .......... .......... .......... .......... .......... 83%  216M 0s\n",
            " 84400K .......... .......... .......... .......... .......... 83%  212M 0s\n",
            " 84450K .......... .......... .......... .......... .......... 83%  206M 0s\n",
            " 84500K .......... .......... .......... .......... .......... 83%  278M 0s\n",
            " 84550K .......... .......... .......... .......... .......... 83%  264M 0s\n",
            " 84600K .......... .......... .......... .......... .......... 83%  288M 0s\n",
            " 84650K .......... .......... .......... .......... .......... 83%  234M 0s\n",
            " 84700K .......... .......... .......... .......... .......... 83%  293M 0s\n",
            " 84750K .......... .......... .......... .......... .......... 83%  144M 0s\n",
            " 84800K .......... .......... .......... .......... .......... 83%  182M 0s\n",
            " 84850K .......... .......... .......... .......... .......... 83%  239M 0s\n",
            " 84900K .......... .......... .......... .......... .......... 83%  267M 0s\n",
            " 84950K .......... .......... .......... .......... .......... 83%  112M 0s\n",
            " 85000K .......... .......... .......... .......... .......... 83%  254M 0s\n",
            " 85050K .......... .......... .......... .......... .......... 84%  166M 0s\n",
            " 85100K .......... .......... .......... .......... .......... 84%  164M 0s\n",
            " 85150K .......... .......... .......... .......... .......... 84%  212M 0s\n",
            " 85200K .......... .......... .......... .......... .......... 84%  270M 0s\n",
            " 85250K .......... .......... .......... .......... .......... 84%  242M 0s\n",
            " 85300K .......... .......... .......... .......... .......... 84%  264M 0s\n",
            " 85350K .......... .......... .......... .......... .......... 84%  159M 0s\n",
            " 85400K .......... .......... .......... .......... .......... 84%  253M 0s\n",
            " 85450K .......... .......... .......... .......... .......... 84%  286M 0s\n",
            " 85500K .......... .......... .......... .......... .......... 84%  255M 0s\n",
            " 85550K .......... .......... .......... .......... .......... 84%  236M 0s\n",
            " 85600K .......... .......... .......... .......... .......... 84%  220M 0s\n",
            " 85650K .......... .......... .......... .......... .......... 84%  162M 0s\n",
            " 85700K .......... .......... .......... .......... .......... 84%  207M 0s\n",
            " 85750K .......... .......... .......... .......... .......... 84%  205M 0s\n",
            " 85800K .......... .......... .......... .......... .......... 84%  143M 0s\n",
            " 85850K .......... .......... .......... .......... .......... 84%  234M 0s\n",
            " 85900K .......... .......... .......... .......... .......... 84% 6.26M 0s\n",
            " 85950K .......... .......... .......... .......... .......... 84% 48.6M 0s\n",
            " 86000K .......... .......... .......... .......... .......... 84% 53.3M 0s\n",
            " 86050K .......... .......... .......... .......... .......... 84% 53.0M 0s\n",
            " 86100K .......... .......... .......... .......... .......... 85% 54.8M 0s\n",
            " 86150K .......... .......... .......... .......... .......... 85% 44.4M 0s\n",
            " 86200K .......... .......... .......... .......... .......... 85% 55.3M 0s\n",
            " 86250K .......... .......... .......... .......... .......... 85% 53.2M 0s\n",
            " 86300K .......... .......... .......... .......... .......... 85% 54.6M 0s\n",
            " 86350K .......... .......... .......... .......... .......... 85% 44.8M 0s\n",
            " 86400K .......... .......... .......... .......... .......... 85% 53.7M 0s\n",
            " 86450K .......... .......... .......... .......... .......... 85% 58.9M 0s\n",
            " 86500K .......... .......... .......... .......... .......... 85% 56.5M 0s\n",
            " 86550K .......... .......... .......... .......... .......... 85% 59.1M 0s\n",
            " 86600K .......... .......... .......... .......... .......... 85% 63.8M 0s\n",
            " 86650K .......... .......... .......... .......... .......... 85% 54.6M 0s\n",
            " 86700K .......... .......... .......... .......... .......... 85% 55.1M 0s\n",
            " 86750K .......... .......... .......... .......... .......... 85% 46.8M 0s\n",
            " 86800K .......... .......... .......... .......... .......... 85%  111M 0s\n",
            " 86850K .......... .......... .......... .......... .......... 85%  219M 0s\n",
            " 86900K .......... .......... .......... .......... .......... 85%  256M 0s\n",
            " 86950K .......... .......... .......... .......... .......... 85%  261M 0s\n",
            " 87000K .......... .......... .......... .......... .......... 85%  242M 0s\n",
            " 87050K .......... .......... .......... .......... .......... 85%  279M 0s\n",
            " 87100K .......... .......... .......... .......... .......... 86%  283M 0s\n",
            " 87150K .......... .......... .......... .......... .......... 86%  194M 0s\n",
            " 87200K .......... .......... .......... .......... .......... 86%  283M 0s\n",
            " 87250K .......... .......... .......... .......... .......... 86%  261M 0s\n",
            " 87300K .......... .......... .......... .......... .......... 86%  277M 0s\n",
            " 87350K .......... .......... .......... .......... .......... 86%  247M 0s\n",
            " 87400K .......... .......... .......... .......... .......... 86%  235M 0s\n",
            " 87450K .......... .......... .......... .......... .......... 86%  271M 0s\n",
            " 87500K .......... .......... .......... .......... .......... 86%  267M 0s\n",
            " 87550K .......... .......... .......... .......... .......... 86%  234M 0s\n",
            " 87600K .......... .......... .......... .......... .......... 86%  277M 0s\n",
            " 87650K .......... .......... .......... .......... .......... 86%  252M 0s\n",
            " 87700K .......... .......... .......... .......... .......... 86%  283M 0s\n",
            " 87750K .......... .......... .......... .......... .......... 86%  253M 0s\n",
            " 87800K .......... .......... .......... .......... .......... 86%  267M 0s\n",
            " 87850K .......... .......... .......... .......... .......... 86%  242M 0s\n",
            " 87900K .......... .......... .......... .......... .......... 86%  133M 0s\n",
            " 87950K .......... .......... .......... .......... .......... 86% 44.5M 0s\n",
            " 88000K .......... .......... .......... .......... .......... 86% 52.3M 0s\n",
            " 88050K .......... .......... .......... .......... .......... 86% 54.8M 0s\n",
            " 88100K .......... .......... .......... .......... .......... 87% 56.2M 0s\n",
            " 88150K .......... .......... .......... .......... .......... 87% 45.9M 0s\n",
            " 88200K .......... .......... .......... .......... .......... 87% 49.8M 0s\n",
            " 88250K .......... .......... .......... .......... .......... 87% 55.8M 0s\n",
            " 88300K .......... .......... .......... .......... .......... 87% 37.0M 0s\n",
            " 88350K .......... .......... .......... .......... .......... 87% 44.0M 0s\n",
            " 88400K .......... .......... .......... .......... .......... 87% 49.8M 0s\n",
            " 88450K .......... .......... .......... .......... .......... 87% 53.8M 0s\n",
            " 88500K .......... .......... .......... .......... .......... 87% 54.6M 0s\n",
            " 88550K .......... .......... .......... .......... .......... 87% 47.8M 0s\n",
            " 88600K .......... .......... .......... .......... .......... 87% 39.5M 0s\n",
            " 88650K .......... .......... .......... .......... .......... 87% 46.6M 0s\n",
            " 88700K .......... .......... .......... .......... .......... 87% 49.9M 0s\n",
            " 88750K .......... .......... .......... .......... .......... 87% 39.4M 0s\n",
            " 88800K .......... .......... .......... .......... .......... 87%  158M 0s\n",
            " 88850K .......... .......... .......... .......... .......... 87%  245M 0s\n",
            " 88900K .......... .......... .......... .......... .......... 87%  275M 0s\n",
            " 88950K .......... .......... .......... .......... .......... 87%  199M 0s\n",
            " 89000K .......... .......... .......... .......... .......... 87%  280M 0s\n",
            " 89050K .......... .......... .......... .......... .......... 87%  298M 0s\n",
            " 89100K .......... .......... .......... .......... .......... 88%  244M 0s\n",
            " 89150K .......... .......... .......... .......... .......... 88%  222M 0s\n",
            " 89200K .......... .......... .......... .......... .......... 88%  265M 0s\n",
            " 89250K .......... .......... .......... .......... .......... 88%  282M 0s\n",
            " 89300K .......... .......... .......... .......... .......... 88%  268M 0s\n",
            " 89350K .......... .......... .......... .......... .......... 88%  226M 0s\n",
            " 89400K .......... .......... .......... .......... .......... 88%  270M 0s\n",
            " 89450K .......... .......... .......... .......... .......... 88%  239M 0s\n",
            " 89500K .......... .......... .......... .......... .......... 88%  274M 0s\n",
            " 89550K .......... .......... .......... .......... .......... 88%  229M 0s\n",
            " 89600K .......... .......... .......... .......... .......... 88%  253M 0s\n",
            " 89650K .......... .......... .......... .......... .......... 88%  263M 0s\n",
            " 89700K .......... .......... .......... .......... .......... 88%  282M 0s\n",
            " 89750K .......... .......... .......... .......... .......... 88%  228M 0s\n",
            " 89800K .......... .......... .......... .......... .......... 88%  279M 0s\n",
            " 89850K .......... .......... .......... .......... .......... 88%  278M 0s\n",
            " 89900K .......... .......... .......... .......... .......... 88% 47.4M 0s\n",
            " 89950K .......... .......... .......... .......... .......... 88% 44.8M 0s\n",
            " 90000K .......... .......... .......... .......... .......... 88% 52.3M 0s\n",
            " 90050K .......... .......... .......... .......... .......... 88% 54.6M 0s\n",
            " 90100K .......... .......... .......... .......... .......... 88% 52.9M 0s\n",
            " 90150K .......... .......... .......... .......... .......... 89% 44.3M 0s\n",
            " 90200K .......... .......... .......... .......... .......... 89% 53.5M 0s\n",
            " 90250K .......... .......... .......... .......... .......... 89% 48.9M 0s\n",
            " 90300K .......... .......... .......... .......... .......... 89% 43.6M 0s\n",
            " 90350K .......... .......... .......... .......... .......... 89% 45.7M 0s\n",
            " 90400K .......... .......... .......... .......... .......... 89% 52.0M 0s\n",
            " 90450K .......... .......... .......... .......... .......... 89% 51.2M 0s\n",
            " 90500K .......... .......... .......... .......... .......... 89% 53.9M 0s\n",
            " 90550K .......... .......... .......... .......... .......... 89% 47.8M 0s\n",
            " 90600K .......... .......... .......... .......... .......... 89% 58.3M 0s\n",
            " 90650K .......... .......... .......... .......... .......... 89% 65.6M 0s\n",
            " 90700K .......... .......... .......... .......... .......... 89% 62.1M 0s\n",
            " 90750K .......... .......... .......... .......... .......... 89% 55.6M 0s\n",
            " 90800K .......... .......... .......... .......... .......... 89% 66.1M 0s\n",
            " 90850K .......... .......... .......... .......... .......... 89%  110M 0s\n",
            " 90900K .......... .......... .......... .......... .......... 89%  279M 0s\n",
            " 90950K .......... .......... .......... .......... .......... 89%  248M 0s\n",
            " 91000K .......... .......... .......... .......... .......... 89%  272M 0s\n",
            " 91050K .......... .......... .......... .......... .......... 89%  248M 0s\n",
            " 91100K .......... .......... .......... .......... .......... 89%  263M 0s\n",
            " 91150K .......... .......... .......... .......... .......... 90%  227M 0s\n",
            " 91200K .......... .......... .......... .......... .......... 90%  241M 0s\n",
            " 91250K .......... .......... .......... .......... .......... 90%  273M 0s\n",
            " 91300K .......... .......... .......... .......... .......... 90%  225M 0s\n",
            " 91350K .......... .......... .......... .......... .......... 90%  243M 0s\n",
            " 91400K .......... .......... .......... .......... .......... 90%  244M 0s\n",
            " 91450K .......... .......... .......... .......... .......... 90%  262M 0s\n",
            " 91500K .......... .......... .......... .......... .......... 90%  266M 0s\n",
            " 91550K .......... .......... .......... .......... .......... 90%  181M 0s\n",
            " 91600K .......... .......... .......... .......... .......... 90%  272M 0s\n",
            " 91650K .......... .......... .......... .......... .......... 90%  233M 0s\n",
            " 91700K .......... .......... .......... .......... .......... 90%  245M 0s\n",
            " 91750K .......... .......... .......... .......... .......... 90%  249M 0s\n",
            " 91800K .......... .......... .......... .......... .......... 90%  256M 0s\n",
            " 91850K .......... .......... .......... .......... .......... 90%  270M 0s\n",
            " 91900K .......... .......... .......... .......... .......... 90%  275M 0s\n",
            " 91950K .......... .......... .......... .......... .......... 90%  211M 0s\n",
            " 92000K .......... .......... .......... .......... .......... 90%  231M 0s\n",
            " 92050K .......... .......... .......... .......... .......... 90% 91.1M 0s\n",
            " 92100K .......... .......... .......... .......... .......... 90% 54.3M 0s\n",
            " 92150K .......... .......... .......... .......... .......... 91% 45.1M 0s\n",
            " 92200K .......... .......... .......... .......... .......... 91% 55.7M 0s\n",
            " 92250K .......... .......... .......... .......... .......... 91% 46.8M 0s\n",
            " 92300K .......... .......... .......... .......... .......... 91% 53.5M 0s\n",
            " 92350K .......... .......... .......... .......... .......... 91% 43.2M 0s\n",
            " 92400K .......... .......... .......... .......... .......... 91% 47.5M 0s\n",
            " 92450K .......... .......... .......... .......... .......... 91% 52.3M 0s\n",
            " 92500K .......... .......... .......... .......... .......... 91% 54.1M 0s\n",
            " 92550K .......... .......... .......... .......... .......... 91% 50.0M 0s\n",
            " 92600K .......... .......... .......... .......... .......... 91% 54.5M 0s\n",
            " 92650K .......... .......... .......... .......... .......... 91% 55.4M 0s\n",
            " 92700K .......... .......... .......... .......... .......... 91% 44.4M 0s\n",
            " 92750K .......... .......... .......... .......... .......... 91% 47.4M 0s\n",
            " 92800K .......... .......... .......... .......... .......... 91% 56.1M 0s\n",
            " 92850K .......... .......... .......... .......... .......... 91% 53.9M 0s\n",
            " 92900K .......... .......... .......... .......... .......... 91% 56.6M 0s\n",
            " 92950K .......... .......... .......... .......... .......... 91%  217M 0s\n",
            " 93000K .......... .......... .......... .......... .......... 91%  227M 0s\n",
            " 93050K .......... .......... .......... .......... .......... 91%  276M 0s\n",
            " 93100K .......... .......... .......... .......... .......... 91%  237M 0s\n",
            " 93150K .......... .......... .......... .......... .......... 92%  232M 0s\n",
            " 93200K .......... .......... .......... .......... .......... 92%  275M 0s\n",
            " 93250K .......... .......... .......... .......... .......... 92%  267M 0s\n",
            " 93300K .......... .......... .......... .......... .......... 92%  273M 0s\n",
            " 93350K .......... .......... .......... .......... .......... 92%  234M 0s\n",
            " 93400K .......... .......... .......... .......... .......... 92%  267M 0s\n",
            " 93450K .......... .......... .......... .......... .......... 92%  272M 0s\n",
            " 93500K .......... .......... .......... .......... .......... 92%  261M 0s\n",
            " 93550K .......... .......... .......... .......... .......... 92%  208M 0s\n",
            " 93600K .......... .......... .......... .......... .......... 92%  263M 0s\n",
            " 93650K .......... .......... .......... .......... .......... 92%  285M 0s\n",
            " 93700K .......... .......... .......... .......... .......... 92%  240M 0s\n",
            " 93750K .......... .......... .......... .......... .......... 92%  237M 0s\n",
            " 93800K .......... .......... .......... .......... .......... 92%  272M 0s\n",
            " 93850K .......... .......... .......... .......... .......... 92%  285M 0s\n",
            " 93900K .......... .......... .......... .......... .......... 92%  278M 0s\n",
            " 93950K .......... .......... .......... .......... .......... 92%  237M 0s\n",
            " 94000K .......... .......... .......... .......... .......... 92%  278M 0s\n",
            " 94050K .......... .......... .......... .......... .......... 92% 51.0M 0s\n",
            " 94100K .......... .......... .......... .......... .......... 92% 49.1M 0s\n",
            " 94150K .......... .......... .......... .......... .......... 92% 46.7M 0s\n",
            " 94200K .......... .......... .......... .......... .......... 93% 46.2M 0s\n",
            " 94250K .......... .......... .......... .......... .......... 93% 52.1M 0s\n",
            " 94300K .......... .......... .......... .......... .......... 93% 55.4M 0s\n",
            " 94350K .......... .......... .......... .......... .......... 93% 45.7M 0s\n",
            " 94400K .......... .......... .......... .......... .......... 93% 56.2M 0s\n",
            " 94450K .......... .......... .......... .......... .......... 93% 51.2M 0s\n",
            " 94500K .......... .......... .......... .......... .......... 93% 51.6M 0s\n",
            " 94550K .......... .......... .......... .......... .......... 93% 47.3M 0s\n",
            " 94600K .......... .......... .......... .......... .......... 93% 55.7M 0s\n",
            " 94650K .......... .......... .......... .......... .......... 93% 53.4M 0s\n",
            " 94700K .......... .......... .......... .......... .......... 93% 54.5M 0s\n",
            " 94750K .......... .......... .......... .......... .......... 93% 46.7M 0s\n",
            " 94800K .......... .......... .......... .......... .......... 93% 55.4M 0s\n",
            " 94850K .......... .......... .......... .......... .......... 93% 53.4M 0s\n",
            " 94900K .......... .......... .......... .......... .......... 93%  144M 0s\n",
            " 94950K .......... .......... .......... .......... .......... 93%  248M 0s\n",
            " 95000K .......... .......... .......... .......... .......... 93%  260M 0s\n",
            " 95050K .......... .......... .......... .......... .......... 93%  260M 0s\n",
            " 95100K .......... .......... .......... .......... .......... 93%  285M 0s\n",
            " 95150K .......... .......... .......... .......... .......... 93%  219M 0s\n",
            " 95200K .......... .......... .......... .......... .......... 94%  281M 0s\n",
            " 95250K .......... .......... .......... .......... .......... 94%  260M 0s\n",
            " 95300K .......... .......... .......... .......... .......... 94%  262M 0s\n",
            " 95350K .......... .......... .......... .......... .......... 94%  222M 0s\n",
            " 95400K .......... .......... .......... .......... .......... 94%  267M 0s\n",
            " 95450K .......... .......... .......... .......... .......... 94%  283M 0s\n",
            " 95500K .......... .......... .......... .......... .......... 94%  285M 0s\n",
            " 95550K .......... .......... .......... .......... .......... 94%  219M 0s\n",
            " 95600K .......... .......... .......... .......... .......... 94%  275M 0s\n",
            " 95650K .......... .......... .......... .......... .......... 94%  262M 0s\n",
            " 95700K .......... .......... .......... .......... .......... 94%  261M 0s\n",
            " 95750K .......... .......... .......... .......... .......... 94%  245M 0s\n",
            " 95800K .......... .......... .......... .......... .......... 94%  253M 0s\n",
            " 95850K .......... .......... .......... .......... .......... 94%  257M 0s\n",
            " 95900K .......... .......... .......... .......... .......... 94%  281M 0s\n",
            " 95950K .......... .......... .......... .......... .......... 94%  241M 0s\n",
            " 96000K .......... .......... .......... .......... .......... 94%  310M 0s\n",
            " 96050K .......... .......... .......... .......... .......... 94%  304M 0s\n",
            " 96100K .......... .......... .......... .......... .......... 94% 89.2M 0s\n",
            " 96150K .......... .......... .......... .......... .......... 94% 45.5M 0s\n",
            " 96200K .......... .......... .......... .......... .......... 95% 52.1M 0s\n",
            " 96250K .......... .......... .......... .......... .......... 95% 53.2M 0s\n",
            " 96300K .......... .......... .......... .......... .......... 95% 53.0M 0s\n",
            " 96350K .......... .......... .......... .......... .......... 95% 44.9M 0s\n",
            " 96400K .......... .......... .......... .......... .......... 95% 46.1M 0s\n",
            " 96450K .......... .......... .......... .......... .......... 95% 47.2M 0s\n",
            " 96500K .......... .......... .......... .......... .......... 95% 52.7M 0s\n",
            " 96550K .......... .......... .......... .......... .......... 95% 49.7M 0s\n",
            " 96600K .......... .......... .......... .......... .......... 95% 56.4M 0s\n",
            " 96650K .......... .......... .......... .......... .......... 95% 54.8M 0s\n",
            " 96700K .......... .......... .......... .......... .......... 95% 56.5M 0s\n",
            " 96750K .......... .......... .......... .......... .......... 95% 50.0M 0s\n",
            " 96800K .......... .......... .......... .......... .......... 95% 58.1M 0s\n",
            " 96850K .......... .......... .......... .......... .......... 95% 56.7M 0s\n",
            " 96900K .......... .......... .......... .......... .......... 95% 59.2M 0s\n",
            " 96950K .......... .......... .......... .......... .......... 95%  128M 0s\n",
            " 97000K .......... .......... .......... .......... .......... 95%  271M 0s\n",
            " 97050K .......... .......... .......... .......... .......... 95%  291M 0s\n",
            " 97100K .......... .......... .......... .......... .......... 95%  283M 0s\n",
            " 97150K .......... .......... .......... .......... .......... 95%  234M 0s\n",
            " 97200K .......... .......... .......... .......... .......... 96%  312M 0s\n",
            " 97250K .......... .......... .......... .......... .......... 96%  106M 0s\n",
            " 97300K .......... .......... .......... .......... .......... 96% 57.6M 0s\n",
            " 97350K .......... .......... .......... .......... .......... 96% 51.6M 0s\n",
            " 97400K .......... .......... .......... .......... .......... 96% 51.0M 0s\n",
            " 97450K .......... .......... .......... .......... .......... 96% 54.7M 0s\n",
            " 97500K .......... .......... .......... .......... .......... 96% 87.2M 0s\n",
            " 97550K .......... .......... .......... .......... .......... 96%  219M 0s\n",
            " 97600K .......... .......... .......... .......... .......... 96%  265M 0s\n",
            " 97650K .......... .......... .......... .......... .......... 96%  247M 0s\n",
            " 97700K .......... .......... .......... .......... .......... 96%  271M 0s\n",
            " 97750K .......... .......... .......... .......... .......... 96%  242M 0s\n",
            " 97800K .......... .......... .......... .......... .......... 96%  253M 0s\n",
            " 97850K .......... .......... .......... .......... .......... 96%  263M 0s\n",
            " 97900K .......... .......... .......... .......... .......... 96%  271M 0s\n",
            " 97950K .......... .......... .......... .......... .......... 96%  215M 0s\n",
            " 98000K .......... .......... .......... .......... .......... 96%  271M 0s\n",
            " 98050K .......... .......... .......... .......... .......... 96%  256M 0s\n",
            " 98100K .......... .......... .......... .......... .......... 96%  263M 0s\n",
            " 98150K .......... .......... .......... .......... .......... 96%  236M 0s\n",
            " 98200K .......... .......... .......... .......... .......... 96%  264M 0s\n",
            " 98250K .......... .......... .......... .......... .......... 97%  268M 0s\n",
            " 98300K .......... .......... .......... .......... .......... 97%  260M 0s\n",
            " 98350K .......... .......... .......... .......... .......... 97%  226M 0s\n",
            " 98400K .......... .......... .......... .......... .......... 97%  271M 0s\n",
            " 98450K .......... .......... .......... .......... .......... 97%  263M 0s\n",
            " 98500K .......... .......... .......... .......... .......... 97%  282M 0s\n",
            " 98550K .......... .......... .......... .......... .......... 97%  224M 0s\n",
            " 98600K .......... .......... .......... .......... .......... 97%  275M 0s\n",
            " 98650K .......... .......... .......... .......... .......... 97%  277M 0s\n",
            " 98700K .......... .......... .......... .......... .......... 97%  127M 0s\n",
            " 98750K .......... .......... .......... .......... .......... 97% 43.2M 0s\n",
            " 98800K .......... .......... .......... .......... .......... 97% 52.0M 0s\n",
            " 98850K .......... .......... .......... .......... .......... 97% 57.7M 0s\n",
            " 98900K .......... .......... .......... .......... .......... 97% 55.2M 0s\n",
            " 98950K .......... .......... .......... .......... .......... 97% 47.4M 0s\n",
            " 99000K .......... .......... .......... .......... .......... 97% 54.2M 0s\n",
            " 99050K .......... .......... .......... .......... .......... 97% 54.7M 0s\n",
            " 99100K .......... .......... .......... .......... .......... 97% 55.8M 0s\n",
            " 99150K .......... .......... .......... .......... .......... 97% 50.4M 0s\n",
            " 99200K .......... .......... .......... .......... .......... 97% 57.0M 0s\n",
            " 99250K .......... .......... .......... .......... .......... 98% 45.3M 0s\n",
            " 99300K .......... .......... .......... .......... .......... 98% 55.3M 0s\n",
            " 99350K .......... .......... .......... .......... .......... 98% 48.4M 0s\n",
            " 99400K .......... .......... .......... .......... .......... 98% 54.6M 0s\n",
            " 99450K .......... .......... .......... .......... .......... 98% 54.4M 0s\n",
            " 99500K .......... .......... .......... .......... .......... 98% 55.4M 0s\n",
            " 99550K .......... .......... .......... .......... .......... 98%  223M 0s\n",
            " 99600K .......... .......... .......... .......... .......... 98%  261M 0s\n",
            " 99650K .......... .......... .......... .......... .......... 98%  258M 0s\n",
            " 99700K .......... .......... .......... .......... .......... 98%  280M 0s\n",
            " 99750K .......... .......... .......... .......... .......... 98%  230M 0s\n",
            " 99800K .......... .......... .......... .......... .......... 98%  283M 0s\n",
            " 99850K .......... .......... .......... .......... .......... 98%  268M 0s\n",
            " 99900K .......... .......... .......... .......... .......... 98%  278M 0s\n",
            " 99950K .......... .......... .......... .......... .......... 98%  228M 0s\n",
            "100000K .......... .......... .......... .......... .......... 98%  271M 0s\n",
            "100050K .......... .......... .......... .......... .......... 98%  283M 0s\n",
            "100100K .......... .......... .......... .......... .......... 98%  266M 0s\n",
            "100150K .......... .......... .......... .......... .......... 98%  231M 0s\n",
            "100200K .......... .......... .......... .......... .......... 98%  282M 0s\n",
            "100250K .......... .......... .......... .......... .......... 99%  256M 0s\n",
            "100300K .......... .......... .......... .......... .......... 99%  273M 0s\n",
            "100350K .......... .......... .......... .......... .......... 99%  228M 0s\n",
            "100400K .......... .......... .......... .......... .......... 99%  252M 0s\n",
            "100450K .......... .......... .......... .......... .......... 99%  271M 0s\n",
            "100500K .......... .......... .......... .......... .......... 99%  282M 0s\n",
            "100550K .......... .......... .......... .......... .......... 99%  239M 0s\n",
            "100600K .......... .......... .......... .......... .......... 99%  277M 0s\n",
            "100650K .......... .......... .......... .......... .......... 99%  266M 0s\n",
            "100700K .......... .......... .......... .......... .......... 99%  281M 0s\n",
            "100750K .......... .......... .......... .......... .......... 99%  235M 0s\n",
            "100800K .......... .......... .......... .......... .......... 99% 69.7M 0s\n",
            "100850K .......... .......... .......... .......... .......... 99% 51.9M 0s\n",
            "100900K .......... .......... .......... .......... .......... 99% 55.5M 0s\n",
            "100950K .......... .......... .......... .......... .......... 99% 47.2M 0s\n",
            "101000K .......... .......... .......... .......... .......... 99% 53.3M 0s\n",
            "101050K .......... .......... .......... .......... .......... 99% 54.0M 0s\n",
            "101100K .......... .......... .......... .......... .......... 99% 53.8M 0s\n",
            "101150K .......... .......... .......... .......... .......... 99% 47.0M 0s\n",
            "101200K .......... .......... .......... .......... .......... 99% 54.7M 0s\n",
            "101250K .......... .......... .......... .......... ......... 100% 48.0M=1.1s\n",
            "\n",
            "2022-05-19 18:08:33 (86.1 MB/s) - ‘Miniconda3-py37_4.11.0-Linux-x86_64.sh’ saved [103730670/103730670]\n",
            "\n",
            "\r  0%|          | 0/38 [00:00<?, ?it/s]\rExtracting : requests-2.27.1-pyhd3eb1b0_0.conda:   0%|          | 0/38 [00:00<?, ?it/s]\rExtracting : pycosat-0.6.3-py37h27cfd23_0.conda:   3%|▎         | 1/38 [00:00<00:05,  6.66it/s]\rExtracting : pycosat-0.6.3-py37h27cfd23_0.conda:   5%|▌         | 2/38 [00:00<00:02, 13.30it/s]\rExtracting : ld_impl_linux-64-2.35.1-h7274673_9.conda:   5%|▌         | 2/38 [00:00<00:02, 13.30it/s]\rExtracting : ncurses-6.3-h7f8727e_2.conda:   8%|▊         | 3/38 [00:00<00:02, 13.30it/s]            \rExtracting : ncurses-6.3-h7f8727e_2.conda:  11%|█         | 4/38 [00:00<00:05,  6.06it/s]\rExtracting : conda-4.11.0-py37h06a4308_0.conda:  11%|█         | 4/38 [00:01<00:05,  6.06it/s]\rExtracting : conda-4.11.0-py37h06a4308_0.conda:  13%|█▎        | 5/38 [00:01<00:14,  2.28it/s]\rExtracting : certifi-2021.10.8-py37h06a4308_2.conda:  13%|█▎        | 5/38 [00:01<00:14,  2.28it/s]\rExtracting : pip-21.2.2-py37h06a4308_0.conda:  16%|█▌        | 6/38 [00:01<00:14,  2.28it/s]       \rExtracting : yaml-0.2.5-h7b6447c_0.conda:  18%|█▊        | 7/38 [00:01<00:13,  2.28it/s]    \rExtracting : sqlite-3.37.0-hc218d9a_0.conda:  21%|██        | 8/38 [00:01<00:13,  2.28it/s]\rExtracting : charset-normalizer-2.0.4-pyhd3eb1b0_0.conda:  24%|██▎       | 9/38 [00:01<00:12,  2.28it/s]\rExtracting : charset-normalizer-2.0.4-pyhd3eb1b0_0.conda:  26%|██▋       | 10/38 [00:01<00:04,  5.83it/s]\rExtracting : six-1.16.0-pyhd3eb1b0_0.conda:  26%|██▋       | 10/38 [00:01<00:04,  5.83it/s]              \rExtracting : tqdm-4.62.3-pyhd3eb1b0_1.conda:  29%|██▉       | 11/38 [00:01<00:04,  5.83it/s]\rExtracting : libgomp-9.3.0-h5101ec6_17.conda:  32%|███▏      | 12/38 [00:01<00:04,  5.83it/s]\rExtracting : libstdcxx-ng-9.3.0-hd4cf53a_17.conda:  34%|███▍      | 13/38 [00:02<00:04,  5.83it/s]\rExtracting : libstdcxx-ng-9.3.0-hd4cf53a_17.conda:  37%|███▋      | 14/38 [00:02<00:03,  6.60it/s]\rExtracting : python-3.7.11-h12debd9_0.conda:  37%|███▋      | 14/38 [00:03<00:03,  6.60it/s]      \rExtracting : setuptools-58.0.4-py37h06a4308_0.conda:  39%|███▉      | 15/38 [00:03<00:03,  6.60it/s]\rExtracting : setuptools-58.0.4-py37h06a4308_0.conda:  42%|████▏     | 16/38 [00:03<00:05,  4.35it/s]\rExtracting : conda-package-handling-1.7.3-py37h27cfd23_1.conda:  42%|████▏     | 16/38 [00:03<00:05,  4.35it/s]\rExtracting : zlib-1.2.11-h7f8727e_4.conda:  45%|████▍     | 17/38 [00:03<00:04,  4.35it/s]                     \rExtracting : urllib3-1.26.7-pyhd3eb1b0_0.conda:  47%|████▋     | 18/38 [00:03<00:04,  4.35it/s]\rExtracting : idna-3.3-pyhd3eb1b0_0.conda:  50%|█████     | 19/38 [00:03<00:04,  4.35it/s]      \rExtracting : _libgcc_mutex-0.1-main.conda:  53%|█████▎    | 20/38 [00:03<00:04,  4.35it/s]\rExtracting : cryptography-36.0.0-py37h9ce1e76_0.conda:  55%|█████▌    | 21/38 [00:03<00:03,  4.35it/s]\rExtracting : cffi-1.15.0-py37hd667e15_1.conda:  58%|█████▊    | 22/38 [00:03<00:03,  4.35it/s]        \rExtracting : pycparser-2.21-pyhd3eb1b0_0.conda:  61%|██████    | 23/38 [00:03<00:03,  4.35it/s]\rExtracting : wheel-0.37.1-pyhd3eb1b0_0.conda:  63%|██████▎   | 24/38 [00:03<00:03,  4.35it/s]  \rExtracting : conda-content-trust-0.1.1-pyhd3eb1b0_0.conda:  66%|██████▌   | 25/38 [00:03<00:02,  4.35it/s]\rExtracting : conda-content-trust-0.1.1-pyhd3eb1b0_0.conda:  68%|██████▊   | 26/38 [00:03<00:01, 10.73it/s]\rExtracting : libffi-3.3-he6710b0_2.conda:  68%|██████▊   | 26/38 [00:03<00:01, 10.73it/s]                 \rExtracting : xz-5.2.5-h7b6447c_0.conda:  71%|███████   | 27/38 [00:03<00:01, 10.73it/s]  \rExtracting : tk-8.6.11-h1ccaba5_0.conda:  74%|███████▎  | 28/38 [00:03<00:00, 10.73it/s]\rExtracting : brotlipy-0.7.0-py37h27cfd23_1003.conda:  76%|███████▋  | 29/38 [00:03<00:00, 10.73it/s]\rExtracting : brotlipy-0.7.0-py37h27cfd23_1003.conda:  79%|███████▉  | 30/38 [00:03<00:00, 11.03it/s]\rExtracting : ca-certificates-2021.10.26-h06a4308_2.conda:  79%|███████▉  | 30/38 [00:03<00:00, 11.03it/s]\rExtracting : pysocks-1.7.1-py37_1.conda:  82%|████████▏ | 31/38 [00:03<00:00, 11.03it/s]                 \rExtracting : readline-8.1.2-h7f8727e_1.conda:  84%|████████▍ | 32/38 [00:03<00:00, 11.03it/s]\rExtracting : ruamel_yaml-0.15.100-py37h27cfd23_0.conda:  87%|████████▋ | 33/38 [00:04<00:00, 11.03it/s]\rExtracting : ruamel_yaml-0.15.100-py37h27cfd23_0.conda:  89%|████████▉ | 34/38 [00:04<00:00, 11.99it/s]\rExtracting : libgcc-ng-9.3.0-h5101ec6_17.conda:  89%|████████▉ | 34/38 [00:04<00:00, 11.99it/s]        \rExtracting : openssl-1.1.1m-h7f8727e_0.conda:  92%|█████████▏| 35/38 [00:04<00:00, 11.99it/s]  \rExtracting : pyopenssl-21.0.0-pyhd3eb1b0_1.conda:  95%|█████████▍| 36/38 [00:04<00:00, 11.99it/s]\rExtracting : pyopenssl-21.0.0-pyhd3eb1b0_1.conda:  97%|█████████▋| 37/38 [00:04<00:00, 12.75it/s]\rExtracting : _openmp_mutex-4.5-1_gnu.tar.bz2:  97%|█████████▋| 37/38 [00:04<00:00, 12.75it/s]    \r                                                                                             \r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version # now returns Python 3.7.11 :: Anaconda, Inc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6_akZQ7YzLF",
        "outputId": "6ae141ea-425f-4715-984d-c56b86c77968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='#A8EB15'> <b> GPU and Memory Check </b>"
      ],
      "metadata": {
        "id": "UpFMUJH_GPgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "THPB2QFQGZEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867c02cc-38fb-4dba-bda2-455b46298798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 19 18:09:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "944yNUVHGeBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d99cb78-14a1-41e6-8b5f-ce5abc3c6187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Befor install mmf install specified versions of `torch` and `torchvision`"
      ],
      "metadata": {
        "id": "tJTwd3so423a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install torch and torchvision\n",
        "# Install specified versions of `torch` and `torchvision`, before installing mmf (causes an issue)\n",
        "!pip install torch==1.6.0 torchvision==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_doBbgM43GY",
        "outputId": "fd8f335b-346d-44fb-90b4-812190ba9e00",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torch-1.6.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (552.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 552.8 MB 4.4 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.7.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 575 kB/s \n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 14.2 MB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 52.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=b37f8314da6d07dcfdf6396bd876e5d8a0c083b456dec76f2e69f34de9fc60ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: numpy, future, torch, pillow, torchvision\n",
            "Successfully installed future-0.18.2 numpy-1.21.6 pillow-9.1.1 torch-1.6.0+cu92 torchvision-0.7.0+cu92\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(home)\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWdaxYMXdsy_",
        "outputId": "b684f779-bc10-41aa-93b8-573fffc061ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  Miniconda3-py37_4.11.0-Linux-x86_64.sh  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color='#A8EB15'> <b> MMF Configuration </b>\n",
        "\n",
        "\n",
        "* <font color='#FFC300'> Restart RUNTIME at the end. </font>\n",
        "*  <font color='#FFC300'> Remember to Mount Google Drive the first time. </font>\n",
        "* <font color='#FFC300'> Change the github branch if you want the default features  </font>"
      ],
      "metadata": {
        "id": "hRzytiBDLstT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# own repo modify the errors 572 and 573\n",
        "!git clone --branch develope --config core.symlinks=true https://github.com/JanLeyva/mmf.git"
      ],
      "metadata": {
        "id": "RnU46fz-4ups"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the following repo where mmf does not install default image features, \n",
        "# since we will use our own features\n",
        "\n",
        "# !git clone --branch nofeatures --config core.symlinks=true https://github.com/JanLeyva/mmf.git"
      ],
      "metadata": {
        "id": "0uYfSUbUWj9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls mmf/projects/hateful_memes/configs/mmbt\t  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eks3RNYM4z9T",
        "outputId": "0b14e5fd-8a41-4f11-ee05-467bd9e5a22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaults.yaml  with_features.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OGPpCcr6sO2S",
        "outputId": "74bd7d72-ad0a-4e61-846f-6296d3af356c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the dir to mmf\n",
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(os.path.join(home, \"mmf\"))\n",
        "!pip install --editable ."
      ],
      "metadata": {
        "id": "0wJB-v0KL8uK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5a7669f-fb50-4140-a559-550c0c9604d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/mmf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 9b011606f) to /tmp/pip-install-ak5m30ye/pytorch-lightning_0af18d35096c455c81830811809d724c\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-ak5m30ye/pytorch-lightning_0af18d35096c455c81830811809d724c\n",
            "\u001b[33m  WARNING: Did not find branch or tag '9b011606f', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 9b011606f\n",
            "  Resolved https://github.com/PyTorchLightning/pytorch-lightning to commit 9b011606f\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<=1.9.0,>=1.6.0 in /usr/local/lib/python3.7/site-packages (from mmf==1.0.0rc12) (1.6.0+cu92)\n",
            "Collecting torchaudio<=0.9.0,>=0.6.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 8.2 MB/s \n",
            "\u001b[?25hCollecting omegaconf<=2.1,>=2.0.6\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting pillow==9.0.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.4\n",
            "  Downloading matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting GitPython==3.1.0\n",
            "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
            "\u001b[K     |████████████████████████████████| 450 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting pycocotools==2.0.2\n",
            "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
            "Collecting iopath==0.1.8\n",
            "  Downloading iopath-0.1.8-py3-none-any.whl (19 kB)\n",
            "Collecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting tqdm<4.50.0,>=4.43.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting numpy<=1.21.4,>=1.16.6\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting requests==2.23.0\n",
            "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 78.3 MB/s \n",
            "\u001b[?25hCollecting transformers<=4.10.1,>=3.4.0\n",
            "  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting sklearn==0.0\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting fasttext==0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting ftfy==5.8\n",
            "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 38.2 MB/s \n",
            "\u001b[?25hCollecting lmdb==0.98\n",
            "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
            "\u001b[K     |████████████████████████████████| 869 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting editdistance==0.5.3\n",
            "  Downloading editdistance-0.5.3-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<=0.10.0,>=0.7.0 in /usr/local/lib/python3.7/site-packages (from mmf==1.0.0rc12) (0.7.0+cu92)\n",
            "Collecting psutil\n",
            "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
            "Collecting pyarrow>=0.17.1\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 73.6 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 57.4 MB/s \n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/site-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (58.0.4)\n",
            "Collecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
            "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.16.0)\n",
            "Collecting cython>=0.27.3\n",
            "  Using cached Cython-0.29.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests==2.23.0->mmf==1.0.0rc12) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 79.1 MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 31.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.2 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 81.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from torch<=1.9.0,>=1.6.0->mmf==1.0.0rc12) (0.18.2)\n",
            "Collecting torch<=1.9.0,>=1.6.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.8 kB/s \n",
            "\u001b[?25hCollecting torchvision<=0.10.0,>=0.7.0\n",
            "  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.7 MB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard>=2.2.0\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "\u001b[K     |████████████████████████████████| 408 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.9 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.20.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.37.1)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 60.3 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 26.7 MB/s \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 173 kB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 70.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (2.0.4)\n",
            "Collecting click\n",
            "  Downloading click-8.1.2-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=1.1.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 375 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fasttext, ftfy, lmdb, nltk, pycocotools, sklearn, termcolor, antlr4-python3-runtime, pytorch-lightning\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2601016 sha256=c6e689cc82a6f116c21a3b7497bc1cb69cdf5447bc308a8d48ad911320252828\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45633 sha256=685ef1eb30b0d8742c04753578be90de7ab3c6a84e976c3ff053ac012557fdd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/1c/fc/8b19700f939810cd8fd9495ae34934b246279791288eda1c31\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=273199 sha256=f063f63a41f7f7b141a8503ba92c3f0868e6cfed8fd1273e4347f5ce2e225773\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=673b40af1b914b90c35cc607e1984a2b9a99eb03ffa64ea8f9b1a654e447b465\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272440 sha256=7a467e0892fa73720d47c2c5b61fc3e03775bedd4f8cd7e4df0f793007444e34\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=8d529f37ad463628710c633ca5e8baa52e16329d7ea3d123d325b176aadd524a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=e03ddabdea7c55421c081af10e6744fd342a5a33cee4f49213d02bba40e060a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=06703ac76796c7ea74589a3ef49f6dfd58f526bddbc183f3f763196f302db54a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.6.0.dev0-py3-none-any.whl size=561942 sha256=b2d1939435dc46b89d993515843aa2b9a7cf09a403c5ead6d465c7b56a8300c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-su1mbj13/wheels/f6/0c/45/240f54bfe51c18a90f2f1cad555a7c705a9f72b44370c82bd0\n",
            "Successfully built fasttext ftfy lmdb nltk pycocotools sklearn termcolor antlr4-python3-runtime pytorch-lightning\n",
            "Installing collected packages: urllib3, pyasn1, idna, chardet, zipp, typing-extensions, rsa, requests, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, pyparsing, numpy, importlib-metadata, google-auth, attrs, asynctest, async-timeout, aiosignal, werkzeug, tqdm, torch, threadpoolctl, tensorboard-plugin-wit, tensorboard-data-server, smmap, scipy, regex, PyYAML, pytz, python-dateutil, pyDeprecate, protobuf, pillow, packaging, markdown, kiwisolver, joblib, grpcio, google-auth-oauthlib, fsspec, filelock, dill, cycler, click, aiohttp, absl-py, xxhash, wcwidth, torchmetrics, tokenizers, tensorboard, sentencepiece, scikit-learn, sacremoses, pybind11, pyarrow, portalocker, pandas, multiprocess, matplotlib, huggingface-hub, gitdb, cython, antlr4-python3-runtime, transformers, torchvision, torchtext, torchaudio, termcolor, sklearn, pytorch-lightning, pycocotools, psutil, omegaconf, nltk, lmdb, iopath, GitPython, ftfy, fasttext, editdistance, datasets, mmf\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.7\n",
            "    Uninstalling urllib3-1.26.7:\n",
            "      Successfully uninstalled urllib3-1.26.7\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.6.0+cu92\n",
            "    Uninstalling torch-1.6.0+cu92:\n",
            "      Successfully uninstalled torch-1.6.0+cu92\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.1.0\n",
            "    Uninstalling Pillow-9.1.0:\n",
            "      Successfully uninstalled Pillow-9.1.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.7.0+cu92\n",
            "    Uninstalling torchvision-0.7.0+cu92:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu92\n",
            "  Running setup.py develop for mmf\n",
            "Successfully installed GitPython-3.1.0 PyYAML-6.0 absl-py-1.0.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 cachetools-5.0.0 chardet-3.0.4 click-8.1.2 cycler-0.11.0 cython-0.29.28 datasets-1.2.1 dill-0.3.4 editdistance-0.5.3 fasttext-0.9.1 filelock-3.6.0 frozenlist-1.3.0 fsspec-2022.3.0 ftfy-5.8 gitdb-4.0.9 google-auth-2.6.5 google-auth-oauthlib-0.4.6 grpcio-1.44.0 huggingface-hub-0.5.1 idna-2.10 importlib-metadata-4.11.3 iopath-0.1.8 joblib-1.1.0 kiwisolver-1.4.2 lmdb-0.98 markdown-3.3.6 matplotlib-3.3.4 mmf-1.0.0rc12 multidict-6.0.2 multiprocess-0.70.12.2 nltk-3.4.5 numpy-1.21.4 oauthlib-3.2.0 omegaconf-2.1.0 packaging-21.3 pandas-1.3.5 pillow-9.0.1 portalocker-2.4.0 protobuf-3.20.0 psutil-5.9.0 pyDeprecate-0.3.2 pyarrow-7.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pybind11-2.9.2 pycocotools-2.0.2 pyparsing-3.0.8 python-dateutil-2.8.2 pytorch-lightning-1.6.0.dev0 pytz-2022.1 regex-2022.3.15 requests-2.23.0 requests-oauthlib-1.3.1 rsa-4.8 sacremoses-0.0.49 scikit-learn-1.0.2 scipy-1.7.3 sentencepiece-0.1.96 sklearn-0.0 smmap-5.0.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-1.1.0 threadpoolctl-3.1.0 tokenizers-0.10.3 torch-1.9.0 torchaudio-0.9.0 torchmetrics-0.8.0 torchtext-0.5.0 torchvision-0.10.0 tqdm-4.49.0 transformers-4.10.1 typing-extensions-4.2.0 urllib3-1.25.11 wcwidth-0.2.5 werkzeug-2.1.1 xxhash-3.0.0 yarl-1.7.2 zipp-3.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color='#A8EB15'> <b> UnZip dataset"
      ],
      "metadata": {
        "id": "LO8dmLlwMB53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(home)"
      ],
      "metadata": {
        "id": "jVHDJdro6O7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Dbl-CQI61U3kDzzttE37lgl22b0Ay_nD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Dbl-CQI61U3kDzzttE37lgl22b0Ay_nD\" -O hateful_memes.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yarLlYd8gaj9",
        "outputId": "92e73c6b-1bbb-481c-ec6a-ecab510cfa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 18:14:34--  https://docs.google.com/uc?export=download&confirm=t&id=1Dbl-CQI61U3kDzzttE37lgl22b0Ay_nD\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.113, 108.177.126.138, 108.177.126.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8qqdr8gvuj3nr0tqcpvs6se8vl0qi0jb/1652984025000/01761641334275034120/*/1Dbl-CQI61U3kDzzttE37lgl22b0Ay_nD?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-19 18:14:34--  https://doc-00-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8qqdr8gvuj3nr0tqcpvs6se8vl0qi0jb/1652984025000/01761641334275034120/*/1Dbl-CQI61U3kDzzttE37lgl22b0Ay_nD?e=download\n",
            "Resolving doc-00-44-docs.googleusercontent.com (doc-00-44-docs.googleusercontent.com)... 142.250.145.132, 2a00:1450:4013:c14::84\n",
            "Connecting to doc-00-44-docs.googleusercontent.com (doc-00-44-docs.googleusercontent.com)|142.250.145.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 857275041 (818M) [application/rar]\n",
            "Saving to: ‘hateful_memes.zip’\n",
            "\n",
            "hateful_memes.zip   100%[===================>] 817.56M   280MB/s    in 2.9s    \n",
            "\n",
            "2022-05-19 18:14:37 (280 MB/s) - ‘hateful_memes.zip’ saved [857275041/857275041]\n",
            "\n",
            "CPU times: user 55.2 ms, sys: 4.1 ms, total: 59.3 ms\n",
            "Wall time: 4.16 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XiwUjy9BSwKw2x3eDDOG7e8tHrntj1l3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1XiwUjy9BSwKw2x3eDDOG7e8tHrntj1l3\" -O hateful_memes.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdq23qV-STLH",
        "outputId": "952af4c4-a5a3-489f-e92a-70e72a4402c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h6YNIKFhTdc",
        "outputId": "544c647a-61c8-47a7-8572-b6cb4ed3c4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  hateful_memes.zip  Miniconda3-py37_4.11.0-Linux-x86_64.sh  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -P \"pass\" \"/content/hateful_memes.zip\""
      ],
      "metadata": {
        "id": "9j1IgYJVSqZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9348795e-b7f8-4d7f-fa81-e744b169a9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/hateful_memes.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/hateful_memes.zip or\n",
            "        /content/hateful_memes.zip.zip, and cannot find /content/hateful_memes.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# cp drive/MyDrive/datasetZIP/hateful_memesv3.zip \"/content\"\n",
        "mmf_convert_hm --zip_file=\"hateful_memesv3.zip\" --password=\"pass\" --bypass_checksum 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeInKf53CrBt",
        "outputId": "b06f2837-d9eb-46a3-b213-03430d3c16df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is hateful_memesv3.zip\n",
            "Copying hateful_memesv3.zip\n",
            "Unzipping hateful_memesv3.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/site-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mmf_convert_hm\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_convert_hm')())\n",
            "  File \"/content/mmf/mmf_cli/hm_convert.py\", line 206, in main\n",
            "    converter.convert()\n",
            "  File \"/content/mmf/mmf_cli/hm_convert.py\", line 139, in convert\n",
            "    phase_one = self.assert_files(images_path)\n",
            "  File \"/content/mmf/mmf_cli/hm_convert.py\", line 53, in assert_files\n",
            "    ), f\"{file} doesn't exist in {folder}\"\n",
            "AssertionError: train.jsonl doesn't exist in /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QlSKTw6dr3h",
        "outputId": "b8d86341-6fd5-4fe6-e7e8-f34c5d6014f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_seen.jsonl\t  hateful_memesv3.zip  LICENSE.txt\ttest_unseen.jsonl\n",
            "dev_unseen.jsonl  hateful_memes.zip    README.md\ttrain.jsonl\n",
            "hateful_memes\t  img\t\t       test_seen.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctXMWZPdjlrP",
        "outputId": "c659c287-81f2-47c3-cbf4-6f3bac1782b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up the disk by removing .zip, .tar files\n",
        "!rm -rf /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hateful_memesv3.zip\n",
        "!rm -rf $home/hateful_memesv3.zip\n",
        "!rm -rf $home/Miniconda3-py37_4.11.0-Linux-x86_64.sh"
      ],
      "metadata": {
        "id": "XKpAFl3j0gM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color='#A8EB15'> <b> Conf before Train </b>"
      ],
      "metadata": {
        "id": "3-DeY938MYoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCK77Qps8uZ4",
        "outputId": "dbc60e08-4c74-46c7-b06d-907b6aaabd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (0.18.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMUUUHBlz8Xm",
        "outputId": "f2e44b93-0b59-437d-a162-1e696591075d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Creamos el directorio `annotations`\n",
        "* pasamos todo el directorio `images` to `annotations`\n",
        "\n"
      ],
      "metadata": {
        "id": "7K45s9-4DHs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations"
      ],
      "metadata": {
        "id": "LN53ro-VC2yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/*  /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS26wxI10E4W",
        "outputId": "53d76e63-eb71-4243-fc9b-aba71f6af3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory '/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hateful_memes'\n",
            "cp: -r not specified; omitting directory '/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/img"
      ],
      "metadata": {
        "id": "pH-p3Kj-Cxdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/* /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/img"
      ],
      "metadata": {
        "id": "R4_9F_rO0ZRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqNPqPKz1bdp",
        "outputId": "22c5c8cf-9ce3-4629-8c12-a633376f2b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_seen.jsonl\t   img\t\ttest_seen.jsonl\n",
            "dev_unseen.jsonl   LICENSE.txt\ttest_unseen.jsonl\n",
            "hateful_memes.zip  README.md\ttrain.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/img | wc -l "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4v2BJT0lWW",
        "outputId": "71905bee-a6e8-4a48-e1d5-1b0019e91713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -R save/* drive/MyDrive/save_model2"
      ],
      "metadata": {
        "id": "zSyaT2YufFRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we try to change the folder name"
      ],
      "metadata": {
        "id": "9UXT3f9jz5bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title help function [ConfMatrix]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as pltcolors\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def plotHeatMap(X, classes, title=None, fmt='.2g', ax=None, xlabel=None, ylabel=None):\n",
        "    \"\"\" Fix heatmap plot from Seaborn with pyplot 3.1.0, 3.1.1\n",
        "        https://stackoverflow.com/questions/56942670/matplotlib-seaborn-first-and-last-row-cut-in-half-of-heatmap-plot\n",
        "    \"\"\"\n",
        "    ax = sns.heatmap(X, xticklabels=classes, yticklabels=classes, annot=True, \\\n",
        "                     fmt=fmt, cmap=plt.cm.Blues, ax=ax) #notation: \"annot\" not \"annote\"\n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    if xlabel:\n",
        "        ax.set_xlabel(xlabel)\n",
        "    if ylabel:\n",
        "        ax.set_ylabel(ylabel)\n",
        "        \n",
        "def plotConfusionMatrix(yTrue, yEst, classes, title=None, fmt='.2g', ax=None):\n",
        "    plotHeatMap(metrics.confusion_matrix(yTrue, yEst), classes, title, fmt, ax, xlabel='Estimations', \\\n",
        "                ylabel='True values'); "
      ],
      "metadata": {
        "cellView": "form",
        "id": "2falIWVKmTDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='#A8EB15'> <b> VilBERT </b>"
      ],
      "metadata": {
        "id": "rCl2i5rkhiPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=\"projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./sub1 \\\n",
        "        env.tensorboard_logdir=logs/fit/sub1 \\"
      ],
      "metadata": {
        "id": "6zpnjgalnceu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "YID5IuQpqB0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "IYcaRMwwqB9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
        "#   model=vilbert \\\n",
        "#   dataset=hateful_memes \\\n",
        "#   training.log_interval=50 \\\n",
        "#   training.max_updates=3000 \\\n",
        "#   training.batch_size=16 \\\n",
        "#   training.evaluation_interval=500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isxO-YnChlCY",
        "outputId": "3a3bb96b-8709-4c34-fd3e-54fc7be3e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 50\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 16\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 500\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'training.log_interval=50', 'training.max_updates=3000', 'training.batch_size=16', 'training.evaluation_interval=500'])\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf_cli.run: \u001b[0mUsing seed 12365102\n",
            "\u001b[32m2022-03-19T18:23:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:23:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:23:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:23:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T18:23:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T18:23:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T18:23:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-03-19T18:23:25 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-03-19T18:23:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, train/hateful_memes/cross_entropy: 0.7290, train/hateful_memes/cross_entropy/avg: 0.7290, train/total_loss: 0.7290, train/total_loss/avg: 0.7290, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 3000, lr: 0., ups: 1.79, time: 28s 709ms, time_since_start: 28s 727ms, eta: 30m 09s 042ms\n",
            "\u001b[32m2022-03-19T18:24:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.7290, train/hateful_memes/cross_entropy/avg: 0.7929, train/total_loss: 0.7290, train/total_loss/avg: 0.7929, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 019ms, time_since_start: 55s 747ms, eta: 27m 53s 713ms\n",
            "\u001b[32m2022-03-19T18:24:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, train/hateful_memes/cross_entropy: 0.7290, train/hateful_memes/cross_entropy/avg: 0.7579, train/total_loss: 0.7290, train/total_loss/avg: 0.7579, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 412ms, time_since_start: 01m 23s 159ms, eta: 27m 48s 760ms\n",
            "\u001b[32m2022-03-19T18:25:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.7371, train/total_loss: 0.6878, train/total_loss/avg: 0.7371, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 988ms, time_since_start: 01m 50s 148ms, eta: 26m 54s 133ms\n",
            "\u001b[32m2022-03-19T18:25:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, train/hateful_memes/cross_entropy: 0.6941, train/hateful_memes/cross_entropy/avg: 0.7285, train/total_loss: 0.6941, train/total_loss/avg: 0.7285, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 110ms, time_since_start: 02m 17s 259ms, eta: 26m 32s 489ms\n",
            "\u001b[32m2022-03-19T18:26:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.7062, train/total_loss: 0.6878, train/total_loss/avg: 0.7062, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 870ms, time_since_start: 02m 44s 129ms, eta: 25m 49s 690ms\n",
            "\u001b[32m2022-03-19T18:26:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.7024, train/total_loss: 0.6878, train/total_loss/avg: 0.7024, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 350, iterations: 350, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 046ms, time_since_start: 03m 11s 176ms, eta: 25m 30s 940ms\n",
            "\u001b[32m2022-03-19T18:27:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.7084, train/total_loss: 0.6878, train/total_loss/avg: 0.7084, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 847ms, time_since_start: 03m 38s 023ms, eta: 24m 50s 990ms\n",
            "\u001b[32m2022-03-19T18:27:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.6868, train/total_loss: 0.6878, train/total_loss/avg: 0.6868, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 450, iterations: 450, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 069ms, time_since_start: 04m 05s 093ms, eta: 24m 34s 408ms\n",
            "\u001b[32m2022-03-19T18:27:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6792, train/hateful_memes/cross_entropy/avg: 0.6809, train/total_loss: 0.6792, train/total_loss/avg: 0.6809, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 833ms, time_since_start: 04m 31s 926ms, eta: 23m 52s 927ms\n",
            "\u001b[32m2022-03-19T18:27:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:27:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-03-19T18:28:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:28:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:28:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:28:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:28:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:28:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:28:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.7092, val/total_loss: 0.7092, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0196, val/hateful_memes/roc_auc: 0.5186, num_updates: 500, epoch: 1, iterations: 500, max_updates: 3000, val_time: 41s 846ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.518618\n",
            "\u001b[32m2022-03-19T18:29:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.6842, train/total_loss: 0.6878, train/total_loss/avg: 0.6842, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 550, iterations: 550, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 700ms, time_since_start: 05m 41s 476ms, eta: 24m 09s 636ms\n",
            "\u001b[32m2022-03-19T18:29:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.6849, train/total_loss: 0.6878, train/total_loss/avg: 0.6849, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 067ms, time_since_start: 06m 08s 543ms, eta: 23m 07s 577ms\n",
            "\u001b[32m2022-03-19T18:30:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, train/hateful_memes/cross_entropy: 0.6878, train/hateful_memes/cross_entropy/avg: 0.6811, train/total_loss: 0.6878, train/total_loss/avg: 0.6811, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 650, iterations: 650, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 200ms, time_since_start: 06m 35s 744ms, eta: 22m 45s 373ms\n",
            "\u001b[32m2022-03-19T18:30:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.6792, train/hateful_memes/cross_entropy/avg: 0.6783, train/total_loss: 0.6792, train/total_loss/avg: 0.6783, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 908ms, time_since_start: 07m 02s 653ms, eta: 22m 01s 969ms\n",
            "\u001b[32m2022-03-19T18:30:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, train/hateful_memes/cross_entropy: 0.6792, train/hateful_memes/cross_entropy/avg: 0.6694, train/total_loss: 0.6792, train/total_loss/avg: 0.6694, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 750, iterations: 750, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 080ms, time_since_start: 07m 29s 734ms, eta: 21m 41s 510ms\n",
            "\u001b[32m2022-03-19T18:31:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.6750, train/hateful_memes/cross_entropy/avg: 0.6649, train/total_loss: 0.6750, train/total_loss/avg: 0.6649, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 860ms, time_since_start: 07m 56s 594ms, eta: 21m 02s 220ms\n",
            "\u001b[32m2022-03-19T18:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, train/hateful_memes/cross_entropy: 0.6750, train/hateful_memes/cross_entropy/avg: 0.6573, train/total_loss: 0.6750, train/total_loss/avg: 0.6573, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 850, iterations: 850, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 075ms, time_since_start: 08m 23s 670ms, eta: 20m 43s 422ms\n",
            "\u001b[32m2022-03-19T18:32:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.6414, train/hateful_memes/cross_entropy/avg: 0.6488, train/total_loss: 0.6414, train/total_loss/avg: 0.6488, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 900ms, time_since_start: 08m 50s 570ms, eta: 20m 06s 633ms\n",
            "\u001b[32m2022-03-19T18:32:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, train/hateful_memes/cross_entropy: 0.6682, train/hateful_memes/cross_entropy/avg: 0.6499, train/total_loss: 0.6682, train/total_loss/avg: 0.6499, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 950, iterations: 950, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 045ms, time_since_start: 09m 17s 615ms, eta: 19m 44s 250ms\n",
            "\u001b[32m2022-03-19T18:33:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-19T18:33:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:33:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:33:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:33:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.6682, train/hateful_memes/cross_entropy/avg: 0.6661, train/total_loss: 0.6682, train/total_loss/avg: 0.6661, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00001, ups: 1.02, time: 49s 885ms, time_since_start: 10m 07s 501ms, eta: 35m 31s 106ms\n",
            "\u001b[32m2022-03-19T18:33:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:33:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:33:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:33:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:33:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:33:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:33:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:33:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:34:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:34:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:34:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.7877, val/total_loss: 0.7877, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0099, val/hateful_memes/roc_auc: 0.5730, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 3000, val_time: 41s 964ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.572956\n",
            "\u001b[32m2022-03-19T18:34:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, train/hateful_memes/cross_entropy: 0.6431, train/hateful_memes/cross_entropy/avg: 0.6650, train/total_loss: 0.6431, train/total_loss/avg: 0.6650, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 1050, iterations: 1050, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 345ms, time_since_start: 11m 16s 812ms, eta: 18m 59s 015ms\n",
            "\u001b[32m2022-03-19T18:35:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.6414, train/hateful_memes/cross_entropy/avg: 0.6624, train/total_loss: 0.6414, train/total_loss/avg: 0.6624, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 852ms, time_since_start: 11m 43s 664ms, eta: 18m 09s 775ms\n",
            "\u001b[32m2022-03-19T18:35:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, train/hateful_memes/cross_entropy: 0.6414, train/hateful_memes/cross_entropy/avg: 0.6620, train/total_loss: 0.6414, train/total_loss/avg: 0.6620, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1150, iterations: 1150, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 010ms, time_since_start: 12m 10s 675ms, eta: 17m 47s 344ms\n",
            "\u001b[32m2022-03-19T18:36:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.6414, train/hateful_memes/cross_entropy/avg: 0.6615, train/total_loss: 0.6414, train/total_loss/avg: 0.6615, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 888ms, time_since_start: 12m 37s 563ms, eta: 17m 13s 798ms\n",
            "\u001b[32m2022-03-19T18:36:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, train/hateful_memes/cross_entropy: 0.6352, train/hateful_memes/cross_entropy/avg: 0.6483, train/total_loss: 0.6352, train/total_loss/avg: 0.6483, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1250, iterations: 1250, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 097ms, time_since_start: 13m 04s 660ms, eta: 16m 52s 891ms\n",
            "\u001b[32m2022-03-19T18:36:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.6352, train/hateful_memes/cross_entropy/avg: 0.6434, train/total_loss: 0.6352, train/total_loss/avg: 0.6434, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 887ms, time_since_start: 13m 31s 548ms, eta: 16m 16s 350ms\n",
            "\u001b[32m2022-03-19T18:37:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, train/hateful_memes/cross_entropy: 0.6352, train/hateful_memes/cross_entropy/avg: 0.6471, train/total_loss: 0.6352, train/total_loss/avg: 0.6471, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1350, iterations: 1350, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 018ms, time_since_start: 13m 58s 567ms, eta: 15m 52s 233ms\n",
            "\u001b[32m2022-03-19T18:37:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.6320, train/hateful_memes/cross_entropy/avg: 0.6466, train/total_loss: 0.6320, train/total_loss/avg: 0.6466, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 849ms, time_since_start: 14m 25s 416ms, eta: 15m 17s 607ms\n",
            "\u001b[32m2022-03-19T18:38:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, train/hateful_memes/cross_entropy: 0.6320, train/hateful_memes/cross_entropy/avg: 0.6394, train/total_loss: 0.6320, train/total_loss/avg: 0.6394, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1450, iterations: 1450, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 264ms, time_since_start: 14m 52s 680ms, eta: 15m 02s 667ms\n",
            "\u001b[32m2022-03-19T18:38:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.6320, train/hateful_memes/cross_entropy/avg: 0.6352, train/total_loss: 0.6320, train/total_loss/avg: 0.6352, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 907ms, time_since_start: 15m 19s 587ms, eta: 14m 22s 105ms\n",
            "\u001b[32m2022-03-19T18:38:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:38:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:38:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:38:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:38:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:39:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:39:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:39:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:39:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.7045, val/total_loss: 0.7045, val/hateful_memes/accuracy: 0.6074, val/hateful_memes/binary_f1: 0.3653, val/hateful_memes/roc_auc: 0.6092, num_updates: 1500, epoch: 3, iterations: 1500, max_updates: 3000, val_time: 41s 107ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.609176\n",
            "\u001b[32m2022-03-19T18:39:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, train/hateful_memes/cross_entropy: 0.6060, train/hateful_memes/cross_entropy/avg: 0.6274, train/total_loss: 0.6060, train/total_loss/avg: 0.6274, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1550, iterations: 1550, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 427ms, time_since_start: 16m 28s 123ms, eta: 14m 09s 475ms\n",
            "\u001b[32m2022-03-19T18:40:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.5971, train/hateful_memes/cross_entropy/avg: 0.6221, train/total_loss: 0.5971, train/total_loss/avg: 0.6221, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 829ms, time_since_start: 16m 54s 953ms, eta: 13m 22s 322ms\n",
            "\u001b[32m2022-03-19T18:40:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, train/hateful_memes/cross_entropy: 0.5458, train/hateful_memes/cross_entropy/avg: 0.6177, train/total_loss: 0.5458, train/total_loss/avg: 0.6177, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1650, iterations: 1650, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 022ms, time_since_start: 17m 21s 976ms, eta: 12m 59s 234ms\n",
            "\u001b[32m2022-03-19T18:41:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.5358, train/hateful_memes/cross_entropy/avg: 0.6129, train/total_loss: 0.5358, train/total_loss/avg: 0.6129, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 831ms, time_since_start: 17m 48s 808ms, eta: 12m 25s 058ms\n",
            "\u001b[32m2022-03-19T18:41:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/3000, train/hateful_memes/cross_entropy: 0.5220, train/hateful_memes/cross_entropy/avg: 0.6027, train/total_loss: 0.5220, train/total_loss/avg: 0.6027, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1750, iterations: 1750, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 020ms, time_since_start: 18m 15s 828ms, eta: 12m 01s 442ms\n",
            "\u001b[32m2022-03-19T18:42:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.5129, train/hateful_memes/cross_entropy/avg: 0.5986, train/total_loss: 0.5129, train/total_loss/avg: 0.5986, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 885ms, time_since_start: 18m 42s 714ms, eta: 11m 29s 124ms\n",
            "\u001b[32m2022-03-19T18:42:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/3000, train/hateful_memes/cross_entropy: 0.5049, train/hateful_memes/cross_entropy/avg: 0.5939, train/total_loss: 0.5049, train/total_loss/avg: 0.5939, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1850, iterations: 1850, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 011ms, time_since_start: 19m 09s 725ms, eta: 11m 03s 513ms\n",
            "\u001b[32m2022-03-19T18:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.4772, train/hateful_memes/cross_entropy/avg: 0.5856, train/total_loss: 0.4772, train/total_loss/avg: 0.5856, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 845ms, time_since_start: 19m 36s 570ms, eta: 10m 30s 754ms\n",
            "\u001b[32m2022-03-19T18:43:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/3000, train/hateful_memes/cross_entropy: 0.4772, train/hateful_memes/cross_entropy/avg: 0.5836, train/total_loss: 0.4772, train/total_loss/avg: 0.5836, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1950, iterations: 1950, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 070ms, time_since_start: 20m 03s 641ms, eta: 10m 07s 129ms\n",
            "\u001b[32m2022-03-19T18:43:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-19T18:43:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:44:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:44:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:44:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.4562, train/hateful_memes/cross_entropy/avg: 0.5798, train/total_loss: 0.4562, train/total_loss/avg: 0.5798, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00001, ups: 1.02, time: 49s 664ms, time_since_start: 20m 53s 305ms, eta: 17m 40s 826ms\n",
            "\u001b[32m2022-03-19T18:44:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:44:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:44:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:44:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:44:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:44:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:44:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:44:38 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:44:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:45:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 0.7174, val/total_loss: 0.7174, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.2815, val/hateful_memes/roc_auc: 0.6634, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 3000, val_time: 41s 742ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.663382\n",
            "\u001b[32m2022-03-19T18:45:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/3000, train/hateful_memes/cross_entropy: 0.4548, train/hateful_memes/cross_entropy/avg: 0.5747, train/total_loss: 0.4548, train/total_loss/avg: 0.5747, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 2050, iterations: 2050, max_updates: 3000, lr: 0.00001, ups: 1.85, time: 27s 421ms, time_since_start: 22m 02s 474ms, eta: 09m 16s 427ms\n",
            "\u001b[32m2022-03-19T18:45:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.4544, train/hateful_memes/cross_entropy/avg: 0.5715, train/total_loss: 0.4544, train/total_loss/avg: 0.5715, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 820ms, time_since_start: 22m 29s 295ms, eta: 08m 35s 597ms\n",
            "\u001b[32m2022-03-19T18:46:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/3000, train/hateful_memes/cross_entropy: 0.4544, train/hateful_memes/cross_entropy/avg: 0.5702, train/total_loss: 0.4544, train/total_loss/avg: 0.5702, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2150, iterations: 2150, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 968ms, time_since_start: 22m 56s 263ms, eta: 08m 09s 636ms\n",
            "\u001b[32m2022-03-19T18:46:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.4544, train/hateful_memes/cross_entropy/avg: 0.5697, train/total_loss: 0.4544, train/total_loss/avg: 0.5697, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 946ms, time_since_start: 23m 23s 210ms, eta: 07m 40s 468ms\n",
            "\u001b[32m2022-03-19T18:47:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/3000, train/hateful_memes/cross_entropy: 0.4548, train/hateful_memes/cross_entropy/avg: 0.5704, train/total_loss: 0.4548, train/total_loss/avg: 0.5704, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2250, iterations: 2250, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 910ms, time_since_start: 23m 50s 121ms, eta: 07m 11s 112ms\n",
            "\u001b[32m2022-03-19T18:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.4544, train/hateful_memes/cross_entropy/avg: 0.5617, train/total_loss: 0.4544, train/total_loss/avg: 0.5617, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 899ms, time_since_start: 24m 17s 021ms, eta: 06m 42s 206ms\n",
            "\u001b[32m2022-03-19T18:48:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/3000, train/hateful_memes/cross_entropy: 0.4544, train/hateful_memes/cross_entropy/avg: 0.5621, train/total_loss: 0.4544, train/total_loss/avg: 0.5621, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2350, iterations: 2350, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 947ms, time_since_start: 24m 43s 968ms, eta: 06m 14s 133ms\n",
            "\u001b[32m2022-03-19T18:48:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.4411, train/hateful_memes/cross_entropy/avg: 0.5573, train/total_loss: 0.4411, train/total_loss/avg: 0.5573, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 938ms, time_since_start: 25m 10s 907ms, eta: 05m 45s 248ms\n",
            "\u001b[32m2022-03-19T18:49:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/3000, train/hateful_memes/cross_entropy: 0.4411, train/hateful_memes/cross_entropy/avg: 0.5544, train/total_loss: 0.4411, train/total_loss/avg: 0.5544, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2450, iterations: 2450, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 973ms, time_since_start: 25m 37s 880ms, eta: 05m 16s 883ms\n",
            "\u001b[32m2022-03-19T18:49:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.4316, train/hateful_memes/cross_entropy/avg: 0.5496, train/total_loss: 0.4316, train/total_loss/avg: 0.5496, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00001, ups: 1.92, time: 26s 934ms, time_since_start: 26m 04s 815ms, eta: 04m 47s 662ms\n",
            "\u001b[32m2022-03-19T18:49:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:49:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:49:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:49:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:49:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:49:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:49:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:49:49 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:50:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:50:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:50:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 0.7957, val/total_loss: 0.7957, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4472, val/hateful_memes/roc_auc: 0.6679, num_updates: 2500, epoch: 5, iterations: 2500, max_updates: 3000, val_time: 41s 534ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.667941\n",
            "\u001b[32m2022-03-19T18:50:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/3000, train/hateful_memes/cross_entropy: 0.4316, train/hateful_memes/cross_entropy/avg: 0.5434, train/total_loss: 0.4316, train/total_loss/avg: 0.5434, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2550, iterations: 2550, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 250ms, time_since_start: 27m 13s 600ms, eta: 04m 21s 930ms\n",
            "\u001b[32m2022-03-19T18:51:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.4242, train/hateful_memes/cross_entropy/avg: 0.5406, train/total_loss: 0.4242, train/total_loss/avg: 0.5406, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 938ms, time_since_start: 27m 40s 539ms, eta: 03m 50s 161ms\n",
            "\u001b[32m2022-03-19T18:51:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, train/hateful_memes/cross_entropy: 0.4145, train/hateful_memes/cross_entropy/avg: 0.5367, train/total_loss: 0.4145, train/total_loss/avg: 0.5367, max mem: 7250.0, experiment: run, epoch: 5, num_updates: 2650, iterations: 2650, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 949ms, time_since_start: 28m 07s 489ms, eta: 03m 21s 476ms\n",
            "\u001b[32m2022-03-19T18:52:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.3976, train/hateful_memes/cross_entropy/avg: 0.5303, train/total_loss: 0.3976, train/total_loss/avg: 0.5303, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 057ms, time_since_start: 28m 34s 546ms, eta: 02m 53s 381ms\n",
            "\u001b[32m2022-03-19T18:52:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, train/hateful_memes/cross_entropy: 0.4143, train/hateful_memes/cross_entropy/avg: 0.5282, train/total_loss: 0.4143, train/total_loss/avg: 0.5282, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2750, iterations: 2750, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 097ms, time_since_start: 29m 01s 643ms, eta: 02m 24s 698ms\n",
            "\u001b[32m2022-03-19T18:52:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.3976, train/hateful_memes/cross_entropy/avg: 0.5220, train/total_loss: 0.3976, train/total_loss/avg: 0.5220, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 852ms, time_since_start: 29m 28s 495ms, eta: 01m 54s 712ms\n",
            "\u001b[32m2022-03-19T18:53:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, train/hateful_memes/cross_entropy: 0.3824, train/hateful_memes/cross_entropy/avg: 0.5195, train/total_loss: 0.3824, train/total_loss/avg: 0.5195, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2850, iterations: 2850, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 003ms, time_since_start: 29m 55s 498ms, eta: 01m 26s 517ms\n",
            "\u001b[32m2022-03-19T18:53:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.3976, train/hateful_memes/cross_entropy/avg: 0.5197, train/total_loss: 0.3976, train/total_loss/avg: 0.5197, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0., ups: 1.92, time: 26s 878ms, time_since_start: 30m 22s 376ms, eta: 57s 412ms\n",
            "\u001b[32m2022-03-19T18:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, train/hateful_memes/cross_entropy: 0.3824, train/hateful_memes/cross_entropy/avg: 0.5130, train/total_loss: 0.3824, train/total_loss/avg: 0.5130, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 2950, iterations: 2950, max_updates: 3000, lr: 0., ups: 1.85, time: 27s 132ms, time_since_start: 30m 49s 508ms, eta: 28s 977ms\n",
            "\u001b[32m2022-03-19T18:54:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-19T18:54:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:54:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:55:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:55:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.3824, train/hateful_memes/cross_entropy/avg: 0.5111, train/total_loss: 0.3824, train/total_loss/avg: 0.5111, max mem: 7250.0, experiment: run, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.02, time: 49s 708ms, time_since_start: 31m 39s 217ms, eta: 0ms\n",
            "\u001b[32m2022-03-19T18:55:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-19T18:55:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T18:55:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-19T18:55:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:55:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-19T18:55:23 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-19T18:55:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-19T18:55:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-19T18:55:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 0.9147, val/total_loss: 0.9147, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4551, val/hateful_memes/roc_auc: 0.6879, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 3000, val_time: 41s 986ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.687926\n",
            "\u001b[32m2022-03-19T18:55:47 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-03-19T18:55:47 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-03-19T18:55:47 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-03-19T18:55:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-19T18:55:58 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n",
            "\u001b[32m2022-03-19T18:55:58 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n",
            "\u001b[32m2022-03-19T18:55:58 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
            "\u001b[32m2022-03-19T18:56:02 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2022-03-19T18:56:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:56:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T18:56:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 125/125 [00:23<00:00,  5.39it/s]\n",
            "\u001b[32m2022-03-19T18:56:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 125\n",
            "\u001b[32m2022-03-19T18:56:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-19T18:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, test/hateful_memes/cross_entropy: 0.8703, test/total_loss: 0.8703, test/hateful_memes/accuracy: 0.6955, test/hateful_memes/binary_f1: 0.4773, test/hateful_memes/roc_auc: 0.7272\n",
            "\u001b[32m2022-03-19T18:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 32m 59s 843ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <font color='#A8EB15'> Predictions in test set"
      ],
      "metadata": {
        "id": "ka3HClLdkKM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_predict config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
        "    model=vilbert \\\n",
        "    dataset=hateful_memes \\\n",
        "    run_type=test \\\n",
        "    checkpoint.resume_file=save/best.ckpt"
      ],
      "metadata": {
        "id": "IP1029t_kKSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7701ddbb-85a3-4d15-9cce-33e3d00a4cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf_cli.run: \u001b[0mUsing seed 27267814\n",
            "\u001b[32m2022-03-19T19:14:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T19:14:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T19:14:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-19T19:14:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T19:14:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T19:14:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-19T19:14:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-19T19:14:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-19T19:14:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-19T19:14:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T19:14:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-19T19:14:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-03-19T19:14:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 63/63 [00:27<00:00,  2.27it/s]\n",
            "\u001b[32m2022-03-19T19:15:14 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/mmf/save/hateful_memes_vilbert_27267814/reports/hateful_memes_run_test_2022-03-19T19:15:14.csv\n",
            "\u001b[32m2022-03-19T19:15:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n",
            "\u001b[32m2022-03-19T19:15:14 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='#A8EB15'> Confusion Matrix"
      ],
      "metadata": {
        "id": "jnr_IWvdkfnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pred = pd.read_csv(\"/content/mmf/save/hateful_memes_vilbert_27267814/reports/hateful_memes_run_test_2022-03-19T19:15:14.csv\")\n",
        "test = pd.read_json(\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/test_unseen.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "60QBF5MTkftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Martix\n",
        "plotConfusionMatrix(test['label'], pred['label'], 2)\n",
        "ConfMatrix=metrics.confusion_matrix(test['label'], pred['label'])\n",
        "print(\"Accuracy:\", (ConfMatrix[0][0]+ConfMatrix[1][1])/2000)"
      ],
      "metadata": {
        "id": "QKS4c3mckhiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b1a12829-7865-4234-d7c7-ec1dd17da644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6955\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(test['label'], pred['proba'],  pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "9je8xeYV830c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d068075-d906-453e-add9-e0b91e3dfe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272117333333333"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "figure(figsize=(6, 6), dpi=80)\n",
        "plt.plot(fpr, tpr)"
      ],
      "metadata": {
        "id": "_ju7cxzu935y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "48912e28-cefb-4203-c8d7-18515dcda0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5ae9422090>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* save the model"
      ],
      "metadata": {
        "id": "QhHt2r5r7j6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OoPKe56V7kAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/ViLbert"
      ],
      "metadata": {
        "id": "4bv6Wdv_7ms7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r save/* drive/MyDrive/ViLbert"
      ],
      "metadata": {
        "id": "-aF9eNiv7q0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='#A8EB15'> <b> VisualBERT </b>"
      ],
      "metadata": {
        "id": "c8JSIHCb-pXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=\"projects/hateful_memes/configs/visual_bert/direct.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./sub1 \\\n",
        "        env.tensorboard_logdir=logs/fit/sub1 \\"
      ],
      "metadata": {
        "id": "K46AKVGMnNzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "xq1ROKnfqs6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "iViP9v1Dq4Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml \\\n",
        "#   model=visual_bert \\\n",
        "#   dataset=hateful_memes \\\n",
        "#   training.log_interval=50 \\\n",
        "#   training.max_updates=3000 \\\n",
        "#   training.batch_size=16 \\\n",
        "#   training.evaluation_interval=500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy6OgkMx-pn_",
        "outputId": "38271ca5-02d9-45b3-fa9e-5d7f064dc570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-10T14:37:11 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/direct.yaml\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 50\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 16\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 500\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'training.log_interval=50', 'training.max_updates=3000', 'training.batch_size=16', 'training.evaluation_interval=500'])\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf_cli.run: \u001b[0mUsing seed 12032315\n",
            "\u001b[32m2022-03-10T14:37:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
            "Downloading features.tar.gz: 100% 10.3G/10.3G [07:48<00:00, 22.0MB/s]\n",
            "[ Starting checksum for features.tar.gz]\n",
            "[ Checksum successful for features.tar.gz]\n",
            "Unpacking features.tar.gz\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
            "Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 142kB/s] \n",
            "[ Starting checksum for extras.tar.gz]\n",
            "[ Checksum successful for extras.tar.gz]\n",
            "Unpacking extras.tar.gz\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6cw9gtvl\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 7.50kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsoxyubv3\n",
            "Downloading: 100% 570/570 [00:00<00:00, 513kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzx0efmk2\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 686kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfivab0ah\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T14:48:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T14:48:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T14:48:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T14:48:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T14:48:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T14:48:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmp8pifapt2\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 64.7MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-03-10T14:48:43 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-03-10T14:49:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.5943, train/total_loss: 0.5943, train/total_loss/avg: 0.5943, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 3000, lr: 0., ups: 1.52, time: 33s 879ms, time_since_start: 33s 956ms, eta: 35m 34s 839ms\n",
            "\u001b[32m2022-03-10T14:49:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.5976, train/total_loss: 0.5943, train/total_loss/avg: 0.5976, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 1.52, time: 33s 027ms, time_since_start: 01m 06s 984ms, eta: 34m 05s 854ms\n",
            "\u001b[32m2022-03-10T14:50:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.5881, train/total_loss: 0.5943, train/total_loss/avg: 0.5881, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 3000, lr: 0., ups: 1.47, time: 34s 225ms, time_since_start: 01m 41s 209ms, eta: 34m 43s 491ms\n",
            "\u001b[32m2022-03-10T14:51:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6163, train/total_loss: 0.5943, train/total_loss/avg: 0.6163, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 551ms, time_since_start: 02m 16s 761ms, eta: 35m 26s 280ms\n",
            "\u001b[32m2022-03-10T14:51:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6083, train/total_loss: 0.5943, train/total_loss/avg: 0.6083, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 353ms, time_since_start: 02m 52s 114ms, eta: 34m 36s 663ms\n",
            "\u001b[32m2022-03-10T14:52:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6190, train/total_loss: 0.5943, train/total_loss/avg: 0.6190, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 229ms, time_since_start: 03m 27s 344ms, eta: 33m 51s 781ms\n",
            "\u001b[32m2022-03-10T14:52:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6148, train/total_loss: 0.5943, train/total_loss/avg: 0.6148, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 350, iterations: 350, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 363ms, time_since_start: 04m 02s 707ms, eta: 33m 21s 707ms\n",
            "\u001b[32m2022-03-10T14:53:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6140, train/total_loss: 0.5943, train/total_loss/avg: 0.6140, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 275ms, time_since_start: 04m 37s 983ms, eta: 32m 39s 073ms\n",
            "\u001b[32m2022-03-10T14:53:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6452, train/total_loss: 0.6008, train/total_loss/avg: 0.6452, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 450, iterations: 450, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 367ms, time_since_start: 05m 13s 351ms, eta: 32m 06s 392ms\n",
            "\u001b[32m2022-03-10T14:54:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6488, train/total_loss: 0.6008, train/total_loss/avg: 0.6488, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 299ms, time_since_start: 05m 48s 650ms, eta: 31m 25s 003ms\n",
            "\u001b[32m2022-03-10T14:54:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T14:54:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-03-10T14:54:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T14:54:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T14:54:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T14:54:46 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-10T14:54:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T14:54:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T14:54:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.6653, val/total_loss: 0.6653, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5367, num_updates: 500, epoch: 1, iterations: 500, max_updates: 3000, val_time: 24s 530ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.536662\n",
            "\u001b[32m2022-03-10T14:55:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6374, train/total_loss: 0.6008, train/total_loss/avg: 0.6374, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 550, iterations: 550, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 993ms, time_since_start: 06m 49s 182ms, eta: 31m 23s 636ms\n",
            "\u001b[32m2022-03-10T14:56:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6161, train/total_loss: 0.5943, train/total_loss/avg: 0.6161, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 298ms, time_since_start: 07m 24s 481ms, eta: 30m 09s 549ms\n",
            "\u001b[32m2022-03-10T14:56:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6223, train/total_loss: 0.6008, train/total_loss/avg: 0.6223, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 650, iterations: 650, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 339ms, time_since_start: 07m 59s 821ms, eta: 29m 33s 921ms\n",
            "\u001b[32m2022-03-10T14:57:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6277, train/total_loss: 0.6008, train/total_loss/avg: 0.6277, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 417ms, time_since_start: 08m 35s 238ms, eta: 28m 59s 972ms\n",
            "\u001b[32m2022-03-10T14:57:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6244, train/total_loss: 0.6008, train/total_loss/avg: 0.6244, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 750, iterations: 750, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 464ms, time_since_start: 09m 10s 703ms, eta: 28m 24s 436ms\n",
            "\u001b[32m2022-03-10T14:58:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6233, train/total_loss: 0.6008, train/total_loss/avg: 0.6233, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 311ms, time_since_start: 09m 46s 014ms, eta: 27m 39s 341ms\n",
            "\u001b[32m2022-03-10T14:59:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6221, train/total_loss: 0.6031, train/total_loss/avg: 0.6221, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 850, iterations: 850, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 511ms, time_since_start: 10m 21s 526ms, eta: 27m 10s 850ms\n",
            "\u001b[32m2022-03-10T14:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.6008, train/hateful_memes/cross_entropy/avg: 0.6207, train/total_loss: 0.6008, train/total_loss/avg: 0.6207, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 353ms, time_since_start: 10m 56s 880ms, eta: 26m 25s 816ms\n",
            "\u001b[32m2022-03-10T15:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6254, train/total_loss: 0.6031, train/total_loss/avg: 0.6254, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 950, iterations: 950, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 482ms, time_since_start: 11m 32s 362ms, eta: 25m 53s 703ms\n",
            "\u001b[32m2022-03-10T15:00:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-10T15:00:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:00:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:01:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:01:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6463, train/total_loss: 0.6031, train/total_loss/avg: 0.6463, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00003, ups: 1.09, time: 46s 225ms, time_since_start: 12m 18s 587ms, eta: 32m 54s 737ms\n",
            "\u001b[32m2022-03-10T15:01:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T15:01:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:01:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:01:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:01:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T15:01:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:01:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:01:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-10T15:01:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:01:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:01:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.7333, val/total_loss: 0.7333, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.3165, val/hateful_memes/roc_auc: 0.6106, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 3000, val_time: 24s 646ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.610603\n",
            "\u001b[32m2022-03-10T15:02:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, train/hateful_memes/cross_entropy: 0.6063, train/hateful_memes/cross_entropy/avg: 0.6504, train/total_loss: 0.6063, train/total_loss/avg: 0.6504, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 1050, iterations: 1050, max_updates: 3000, lr: 0.00003, ups: 1.39, time: 36s 184ms, time_since_start: 13m 19s 419ms, eta: 25m 07s 142ms\n",
            "\u001b[32m2022-03-10T15:02:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.6063, train/hateful_memes/cross_entropy/avg: 0.6410, train/total_loss: 0.6063, train/total_loss/avg: 0.6410, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 094ms, time_since_start: 13m 54s 513ms, eta: 23m 44s 257ms\n",
            "\u001b[32m2022-03-10T15:03:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, train/hateful_memes/cross_entropy: 0.6063, train/hateful_memes/cross_entropy/avg: 0.6332, train/total_loss: 0.6063, train/total_loss/avg: 0.6332, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1150, iterations: 1150, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 497ms, time_since_start: 14m 30s 011ms, eta: 23m 22s 735ms\n",
            "\u001b[32m2022-03-10T15:03:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6271, train/total_loss: 0.6031, train/total_loss/avg: 0.6271, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 411ms, time_since_start: 15m 05s 423ms, eta: 22m 41s 513ms\n",
            "\u001b[32m2022-03-10T15:04:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6174, train/total_loss: 0.6031, train/total_loss/avg: 0.6174, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1250, iterations: 1250, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 453ms, time_since_start: 15m 40s 877ms, eta: 22m 05s 264ms\n",
            "\u001b[32m2022-03-10T15:04:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.5962, train/hateful_memes/cross_entropy/avg: 0.6063, train/total_loss: 0.5962, train/total_loss/avg: 0.6063, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 422ms, time_since_start: 16m 16s 299ms, eta: 21m 26s 254ms\n",
            "\u001b[32m2022-03-10T15:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, train/hateful_memes/cross_entropy: 0.5962, train/hateful_memes/cross_entropy/avg: 0.5993, train/total_loss: 0.5962, train/total_loss/avg: 0.5993, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1350, iterations: 1350, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 478ms, time_since_start: 16m 51s 777ms, eta: 20m 50s 387ms\n",
            "\u001b[32m2022-03-10T15:06:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.5786, train/hateful_memes/cross_entropy/avg: 0.5964, train/total_loss: 0.5786, train/total_loss/avg: 0.5964, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 300ms, time_since_start: 17m 27s 077ms, eta: 20m 06s 421ms\n",
            "\u001b[32m2022-03-10T15:06:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, train/hateful_memes/cross_entropy: 0.5786, train/hateful_memes/cross_entropy/avg: 0.5966, train/total_loss: 0.5786, train/total_loss/avg: 0.5966, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1450, iterations: 1450, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 493ms, time_since_start: 18m 02s 571ms, eta: 19m 35s 106ms\n",
            "\u001b[32m2022-03-10T15:07:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.5229, train/hateful_memes/cross_entropy/avg: 0.5910, train/total_loss: 0.5229, train/total_loss/avg: 0.5910, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 376ms, time_since_start: 18m 37s 947ms, eta: 18m 53s 470ms\n",
            "\u001b[32m2022-03-10T15:07:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T15:07:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:07:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:07:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:07:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T15:07:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:07:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:07:35 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-10T15:07:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:07:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:07:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.7154, val/total_loss: 0.7154, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.2979, val/hateful_memes/roc_auc: 0.6110, num_updates: 1500, epoch: 3, iterations: 1500, max_updates: 3000, val_time: 24s 786ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.611015\n",
            "\u001b[32m2022-03-10T15:08:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, train/hateful_memes/cross_entropy: 0.5187, train/hateful_memes/cross_entropy/avg: 0.5864, train/total_loss: 0.5187, train/total_loss/avg: 0.5864, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1550, iterations: 1550, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 981ms, time_since_start: 19m 38s 723ms, eta: 18m 34s 412ms\n",
            "\u001b[32m2022-03-10T15:08:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.5786, train/hateful_memes/cross_entropy/avg: 0.5881, train/total_loss: 0.5786, train/total_loss/avg: 0.5881, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 254ms, time_since_start: 20m 13s 977ms, eta: 17m 34s 237ms\n",
            "\u001b[32m2022-03-10T15:09:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, train/hateful_memes/cross_entropy: 0.5187, train/hateful_memes/cross_entropy/avg: 0.5836, train/total_loss: 0.5187, train/total_loss/avg: 0.5836, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1650, iterations: 1650, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 434ms, time_since_start: 20m 49s 412ms, eta: 17m 01s 797ms\n",
            "\u001b[32m2022-03-10T15:10:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.4868, train/hateful_memes/cross_entropy/avg: 0.5751, train/total_loss: 0.4868, train/total_loss/avg: 0.5751, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 485ms, time_since_start: 21m 24s 897ms, eta: 16m 25s 353ms\n",
            "\u001b[32m2022-03-10T15:10:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/3000, train/hateful_memes/cross_entropy: 0.4613, train/hateful_memes/cross_entropy/avg: 0.5680, train/total_loss: 0.4613, train/total_loss/avg: 0.5680, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1750, iterations: 1750, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 403ms, time_since_start: 22m 300ms, eta: 15m 45s 266ms\n",
            "\u001b[32m2022-03-10T15:11:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.4484, train/hateful_memes/cross_entropy/avg: 0.5599, train/total_loss: 0.4484, train/total_loss/avg: 0.5599, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 35s 385ms, time_since_start: 22m 35s 686ms, eta: 15m 07s 001ms\n",
            "\u001b[32m2022-03-10T15:11:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/3000, train/hateful_memes/cross_entropy: 0.4484, train/hateful_memes/cross_entropy/avg: 0.5587, train/total_loss: 0.4484, train/total_loss/avg: 0.5587, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1850, iterations: 1850, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 35s 581ms, time_since_start: 23m 11s 267ms, eta: 14m 34s 032ms\n",
            "\u001b[32m2022-03-10T15:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.4431, train/hateful_memes/cross_entropy/avg: 0.5547, train/total_loss: 0.4431, train/total_loss/avg: 0.5547, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 35s 331ms, time_since_start: 23m 46s 599ms, eta: 13m 50s 160ms\n",
            "\u001b[32m2022-03-10T15:13:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/3000, train/hateful_memes/cross_entropy: 0.4381, train/hateful_memes/cross_entropy/avg: 0.5459, train/total_loss: 0.4381, train/total_loss/avg: 0.5459, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1950, iterations: 1950, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 35s 504ms, time_since_start: 24m 22s 104ms, eta: 13m 16s 291ms\n",
            "\u001b[32m2022-03-10T15:13:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-10T15:13:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:13:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:13:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:13:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.4280, train/hateful_memes/cross_entropy/avg: 0.5383, train/total_loss: 0.4280, train/total_loss/avg: 0.5383, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00005, ups: 1.11, time: 45s 821ms, time_since_start: 25m 07s 926ms, eta: 16m 18s 755ms\n",
            "\u001b[32m2022-03-10T15:13:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T15:13:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:13:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:13:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:13:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T15:13:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:14:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:14:05 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-10T15:14:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:14:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:14:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 0.7251, val/total_loss: 0.7251, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.2921, val/hateful_memes/roc_auc: 0.6516, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 3000, val_time: 24s 495ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.651618\n",
            "\u001b[32m2022-03-10T15:14:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/3000, train/hateful_memes/cross_entropy: 0.4280, train/hateful_memes/cross_entropy/avg: 0.5366, train/total_loss: 0.4280, train/total_loss/avg: 0.5366, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2050, iterations: 2050, max_updates: 3000, lr: 0.00005, ups: 1.39, time: 36s 270ms, time_since_start: 26m 08s 692ms, eta: 12m 15s 998ms\n",
            "\u001b[32m2022-03-10T15:15:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.4280, train/hateful_memes/cross_entropy/avg: 0.5388, train/total_loss: 0.4280, train/total_loss/avg: 0.5388, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 35s 158ms, time_since_start: 26m 43s 851ms, eta: 11m 15s 891ms\n",
            "\u001b[32m2022-03-10T15:16:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/3000, train/hateful_memes/cross_entropy: 0.4164, train/hateful_memes/cross_entropy/avg: 0.5329, train/total_loss: 0.4164, train/total_loss/avg: 0.5329, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2150, iterations: 2150, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 361ms, time_since_start: 27m 19s 213ms, eta: 10m 42s 023ms\n",
            "\u001b[32m2022-03-10T15:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.4164, train/hateful_memes/cross_entropy/avg: 0.5307, train/total_loss: 0.4164, train/total_loss/avg: 0.5307, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 679ms, time_since_start: 27m 54s 893ms, eta: 10m 09s 696ms\n",
            "\u001b[32m2022-03-10T15:17:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/3000, train/hateful_memes/cross_entropy: 0.4225, train/hateful_memes/cross_entropy/avg: 0.5283, train/total_loss: 0.4225, train/total_loss/avg: 0.5283, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2250, iterations: 2250, max_updates: 3000, lr: 0.00004, ups: 1.43, time: 35s 642ms, time_since_start: 28m 30s 535ms, eta: 09m 30s 997ms\n",
            "\u001b[32m2022-03-10T15:17:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.4280, train/hateful_memes/cross_entropy/avg: 0.5277, train/total_loss: 0.4280, train/total_loss/avg: 0.5277, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 612ms, time_since_start: 29m 06s 148ms, eta: 08m 52s 474ms\n",
            "\u001b[32m2022-03-10T15:18:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/3000, train/hateful_memes/cross_entropy: 0.4280, train/hateful_memes/cross_entropy/avg: 0.5250, train/total_loss: 0.4280, train/total_loss/avg: 0.5250, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2350, iterations: 2350, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 626ms, time_since_start: 29m 41s 775ms, eta: 08m 14s 645ms\n",
            "\u001b[32m2022-03-10T15:19:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.4225, train/hateful_memes/cross_entropy/avg: 0.5225, train/total_loss: 0.4225, train/total_loss/avg: 0.5225, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 647ms, time_since_start: 30m 17s 423ms, eta: 07m 36s 863ms\n",
            "\u001b[32m2022-03-10T15:19:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/3000, train/hateful_memes/cross_entropy: 0.4225, train/hateful_memes/cross_entropy/avg: 0.5239, train/total_loss: 0.4225, train/total_loss/avg: 0.5239, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2450, iterations: 2450, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 563ms, time_since_start: 30m 52s 986ms, eta: 06m 57s 797ms\n",
            "\u001b[32m2022-03-10T15:20:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.4081, train/hateful_memes/cross_entropy/avg: 0.5176, train/total_loss: 0.4081, train/total_loss/avg: 0.5176, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00003, ups: 1.43, time: 35s 595ms, time_since_start: 31m 28s 582ms, eta: 06m 20s 163ms\n",
            "\u001b[32m2022-03-10T15:20:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T15:20:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:20:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T15:20:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:20:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:20:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:20:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 1.0223, val/total_loss: 1.0223, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.3789, val/hateful_memes/roc_auc: 0.6385, num_updates: 2500, epoch: 5, iterations: 2500, max_updates: 3000, val_time: 19s 257ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.651618\n",
            "\u001b[32m2022-03-10T15:21:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/3000, train/hateful_memes/cross_entropy: 0.4060, train/hateful_memes/cross_entropy/avg: 0.5118, train/total_loss: 0.4060, train/total_loss/avg: 0.5118, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2550, iterations: 2550, max_updates: 3000, lr: 0.00002, ups: 1.39, time: 36s 366ms, time_since_start: 32m 24s 213ms, eta: 05m 49s 556ms\n",
            "\u001b[32m2022-03-10T15:21:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.4060, train/hateful_memes/cross_entropy/avg: 0.5121, train/total_loss: 0.4060, train/total_loss/avg: 0.5121, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 644ms, time_since_start: 32m 59s 858ms, eta: 05m 04s 544ms\n",
            "\u001b[32m2022-03-10T15:22:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, train/hateful_memes/cross_entropy: 0.3989, train/hateful_memes/cross_entropy/avg: 0.5077, train/total_loss: 0.3989, train/total_loss/avg: 0.5077, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2650, iterations: 2650, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 565ms, time_since_start: 33m 35s 423ms, eta: 04m 25s 887ms\n",
            "\u001b[32m2022-03-10T15:22:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.3989, train/hateful_memes/cross_entropy/avg: 0.4991, train/total_loss: 0.3989, train/total_loss/avg: 0.4991, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00002, ups: 1.43, time: 35s 306ms, time_since_start: 34m 10s 730ms, eta: 03m 46s 246ms\n",
            "\u001b[32m2022-03-10T15:23:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, train/hateful_memes/cross_entropy: 0.3989, train/hateful_memes/cross_entropy/avg: 0.4939, train/total_loss: 0.3989, train/total_loss/avg: 0.4939, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2750, iterations: 2750, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 277ms, time_since_start: 34m 46s 007ms, eta: 03m 08s 380ms\n",
            "\u001b[32m2022-03-10T15:24:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.3989, train/hateful_memes/cross_entropy/avg: 0.4862, train/total_loss: 0.3989, train/total_loss/avg: 0.4862, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 419ms, time_since_start: 35m 21s 427ms, eta: 02m 31s 313ms\n",
            "\u001b[32m2022-03-10T15:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, train/hateful_memes/cross_entropy: 0.2830, train/hateful_memes/cross_entropy/avg: 0.4804, train/total_loss: 0.2830, train/total_loss/avg: 0.4804, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2850, iterations: 2850, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 475ms, time_since_start: 35m 56s 903ms, eta: 01m 53s 664ms\n",
            "\u001b[32m2022-03-10T15:25:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.2795, train/hateful_memes/cross_entropy/avg: 0.4766, train/total_loss: 0.2795, train/total_loss/avg: 0.4766, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00001, ups: 1.43, time: 35s 477ms, time_since_start: 36m 32s 381ms, eta: 01m 15s 780ms\n",
            "\u001b[32m2022-03-10T15:25:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, train/hateful_memes/cross_entropy: 0.2795, train/hateful_memes/cross_entropy/avg: 0.4704, train/total_loss: 0.2795, train/total_loss/avg: 0.4704, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2950, iterations: 2950, max_updates: 3000, lr: 0., ups: 1.43, time: 35s 389ms, time_since_start: 37m 07s 770ms, eta: 37s 795ms\n",
            "\u001b[32m2022-03-10T15:26:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-10T15:26:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:26:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:26:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:26:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.2795, train/hateful_memes/cross_entropy/avg: 0.4633, train/total_loss: 0.2795, train/total_loss/avg: 0.4633, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.09, time: 46s 045ms, time_since_start: 37m 53s 816ms, eta: 0ms\n",
            "\u001b[32m2022-03-10T15:26:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-10T15:26:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:26:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:26:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:26:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-10T15:26:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:26:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-10T15:26:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-10T15:26:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-10T15:27:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-10T15:27:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 1.1597, val/total_loss: 1.1597, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4267, val/hateful_memes/roc_auc: 0.6737, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 3000, val_time: 25s 322ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.673676\n",
            "\u001b[32m2022-03-10T15:27:03 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-03-10T15:27:03 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-03-10T15:27:03 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-03-10T15:27:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-10T15:27:04 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n",
            "\u001b[32m2022-03-10T15:27:04 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n",
            "\u001b[32m2022-03-10T15:27:04 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
            "\u001b[32m2022-03-10T15:27:05 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2022-03-10T15:27:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:27:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:27:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 125/125 [00:30<00:00,  4.04it/s]\n",
            "\u001b[32m2022-03-10T15:27:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 125\n",
            "\u001b[32m2022-03-10T15:27:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-10T15:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, test/hateful_memes/cross_entropy: 1.0517, test/total_loss: 1.0517, test/hateful_memes/accuracy: 0.6880, test/hateful_memes/binary_f1: 0.4765, test/hateful_memes/roc_auc: 0.7257\n",
            "\u001b[32m2022-03-10T15:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 38m 53s 985ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=projects/visual_bert/configs/hateful_memes/from_coco.yaml \\\n",
        "    model=visual_bert \\\n",
        "    dataset=hateful_memes \\\n",
        "    run_type=train_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnKj-9_Vj_-Q",
        "outputId": "176034f8-d837-413f-871e-4a70158e9d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-10T16:01:21 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/from_coco.yaml\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val'])\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf_cli.run: \u001b[0mUsing seed 21300645\n",
            "\u001b[32m2022-03-10T16:01:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T16:01:24 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T16:01:24 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T16:01:24 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T16:01:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-10T16:01:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-10T16:01:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-10T16:01:31 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n",
            "Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:21<00:00, 19.5MB/s]\n",
            "[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:59 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:59 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:59 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T16:01:59 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-03-10T16:01:59 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mmf_run\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n",
            "  File \"/content/mmf/mmf_cli/run.py\", line 133, in run\n",
            "    main(configuration, predict=predict)\n",
            "  File \"/content/mmf/mmf_cli/run.py\", line 56, in main\n",
            "    trainer.train()\n",
            "  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 145, in train\n",
            "    self.training_loop()\n",
            "  File \"/content/mmf/mmf/trainers/core/training_loop.py\", line 33, in training_loop\n",
            "    self.run_training_epoch()\n",
            "  File \"/content/mmf/mmf/trainers/core/training_loop.py\", line 91, in run_training_epoch\n",
            "    report = self.run_training_batch(batch, num_batches_for_this_update)\n",
            "  File \"/content/mmf/mmf/trainers/core/training_loop.py\", line 166, in run_training_batch\n",
            "    report = self._forward(batch)\n",
            "  File \"/content/mmf/mmf/trainers/core/training_loop.py\", line 200, in _forward\n",
            "    model_output = self.model(prepared_batch)\n",
            "  File \"/content/mmf/mmf/models/base_model.py\", line 309, in __call__\n",
            "    model_output = super().__call__(sample_list, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mmf/mmf/models/visual_bert.py\", line 577, in forward\n",
            "    getattr_torchscriptable(sample_list, \"masked_lm_labels\", None),\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mmf/mmf/models/visual_bert.py\", line 356, in forward\n",
            "    image_text_alignment,\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mmf/mmf/models/visual_bert.py\", line 134, in forward\n",
            "    encoded_layers = self.encoder(embedding_output, extended_attention_mask)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mmf/mmf/modules/hf_layers.py\", line 339, in forward\n",
            "    encoder_attention_mask,\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mmf/mmf/modules/hf_layers.py\", line 291, in forward\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\", line 439, in forward\n",
            "    hidden_states = self.dropout(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\", line 1168, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 14.76 GiB total capacity; 13.43 GiB already allocated; 15.75 MiB free; 13.69 GiB reserved in total by PyTorch)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_predict config=projects/hateful_memes/configs/visual_bert//direct.yaml \\\n",
        "    model=visual_bert \\\n",
        "    dataset=hateful_memes \\\n",
        "    run_type=test \\\n",
        "    checkpoint.resume_file=save/best.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUbOyyqlhzew",
        "outputId": "44267866-71e5-459c-d9a2-b169e3ebb635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert//direct.yaml\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert//direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf_cli.run: \u001b[0mUsing seed 8294730\n",
            "\u001b[32m2022-03-10T15:53:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:53:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:53:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-10T15:53:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T15:53:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T15:53:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-10T15:53:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-10T15:53:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-10T15:53:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-10T15:53:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:53:26 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-10T15:53:26 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-03-10T15:53:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 16/16 [00:38<00:00,  2.43s/it]\n",
            "\u001b[32m2022-03-10T15:54:05 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/save/hateful_memes_visual_bert_8294730/reports/hateful_memes_run_test_2022-03-10T15:54:05.csv\n",
            "\u001b[32m2022-03-10T15:54:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 16\n",
            "\u001b[32m2022-03-10T15:54:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Copy the model to Drive"
      ],
      "metadata": {
        "id": "56Mx56NwkngG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRVGlCCOkmTf",
        "outputId": "60cbe912-0fc2-4765-c19b-dc5c96817d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\t    mmfv3.zip\t    'saveVisualBERT (1)'\n",
            " CV_Jan_Leyva.pdf\t    mmfv5.zip\t     SL_Test\n",
            " datasetZIP\t\t    Pythonfiles      STATICAL_LEARNING\n",
            "'EIO - UPC'\t\t    save_model\t     TFMdataset\n",
            " Final_Assignment_MVA.zip   save_model2\n",
            "'MESIO - UPC'\t\t    saveVisualBERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r save drive/MyDrive/saveVisualBERT"
      ],
      "metadata": {
        "id": "fp31ELv9ksu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pred = pd.read_csv(\"/content/save/hateful_memes_visual_bert_8294730/reports/hateful_memes_run_test_2022-03-10T15:54:05.csv\")\n",
        "test = pd.read_json(\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/test_unseen.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "kN9MLFjwlWPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Martix\n",
        "plotConfusionMatrix(test['label'], pred['label'], 2)\n",
        "ConfMatrix=metrics.confusion_matrix(test['label'], pred['label'])\n",
        "print(\"Accuracy:\", (ConfMatrix[0][0]+ConfMatrix[1][1])/2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "FjZXPa92lfVI",
        "outputId": "177a92b3-fcaa-4e3f-be61-030b03a2ba89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.688\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/test_unseen.jsonl drive/MyDrive"
      ],
      "metadata": {
        "id": "RWFHeE13vf_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mPvaDzMDvgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='#A8EB15'> <b> VisualBERT COCO </b>"
      ],
      "metadata": {
        "id": "RdOzAZtNflNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "# # Define where image features are\n",
        "# feats_dir = os.path.join(home, \"features\")\n",
        "# # Define where train.jsonl is\n",
        "# train_dir = os.path.join(home, \"train_v9.jsonl\")\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/hateful_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./sub1 \\\n",
        "        env.tensorboard_logdir=logs/fit/sub1 \\"
      ],
      "metadata": {
        "id": "hJ8LUGuQmWk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "9wRm8Erdmx84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "ikHKG0NJmz-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
        "#   model=visual_bert \\\n",
        "#   dataset=hateful_memes \\\n",
        "#   training.log_interval=50 \\\n",
        "#   training.max_updates=3000 \\\n",
        "#   training.batch_size=16 \\\n",
        "#   training.evaluation_interval=500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-k5Gel5fsQW",
        "outputId": "bb8623ec-d52d-4229-aa62-0b0bc4ba2aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 50\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 16\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 500\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'training.log_interval=50', 'training.max_updates=3000', 'training.batch_size=16', 'training.evaluation_interval=500'])\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf_cli.run: \u001b[0mUsing seed 16964781\n",
            "\u001b[32m2022-03-13T14:04:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:04:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:04:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:04:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:04:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmp13eij4mb\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 68.1MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-13T14:04:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-13T14:04:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-13T14:04:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n",
            "Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:05<00:00, 81.3MB/s]\n",
            "[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:45 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:45 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:45 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:04:45 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-03-13T14:04:46 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-03-13T14:05:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, train/hateful_memes/cross_entropy: 0.6846, train/hateful_memes/cross_entropy/avg: 0.6846, train/total_loss: 0.6846, train/total_loss/avg: 0.6846, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 3000, lr: 0., ups: 2.17, time: 23s 264ms, time_since_start: 34s 193ms, eta: 24m 25s 957ms\n",
            "\u001b[32m2022-03-13T14:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6571, train/total_loss: 0.6297, train/total_loss/avg: 0.6571, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 2.38, time: 21s 777ms, time_since_start: 55s 970ms, eta: 22m 28s 974ms\n",
            "\u001b[32m2022-03-13T14:05:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, train/hateful_memes/cross_entropy: 0.6468, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6468, train/total_loss/avg: 0.6537, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 3000, lr: 0., ups: 2.27, time: 22s 002ms, time_since_start: 01m 17s 973ms, eta: 22m 19s 423ms\n",
            "\u001b[32m2022-03-13T14:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6114, train/total_loss: 0.6297, train/total_loss/avg: 0.6114, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 922ms, time_since_start: 01m 39s 895ms, eta: 21m 51s 136ms\n",
            "\u001b[32m2022-03-13T14:06:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6040, train/total_loss: 0.6297, train/total_loss/avg: 0.6040, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 993ms, time_since_start: 02m 01s 888ms, eta: 21m 31s 875ms\n",
            "\u001b[32m2022-03-13T14:06:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.5838, train/hateful_memes/cross_entropy/avg: 0.6006, train/total_loss: 0.5838, train/total_loss/avg: 0.6006, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 949ms, time_since_start: 02m 23s 838ms, eta: 21m 05s 868ms\n",
            "\u001b[32m2022-03-13T14:07:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6111, train/total_loss: 0.6297, train/total_loss/avg: 0.6111, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 350, iterations: 350, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 953ms, time_since_start: 02m 45s 791ms, eta: 20m 42s 644ms\n",
            "\u001b[32m2022-03-13T14:07:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.5838, train/hateful_memes/cross_entropy/avg: 0.6011, train/total_loss: 0.5838, train/total_loss/avg: 0.6011, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 815ms, time_since_start: 03m 07s 606ms, eta: 20m 11s 528ms\n",
            "\u001b[32m2022-03-13T14:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6083, train/total_loss: 0.6297, train/total_loss/avg: 0.6083, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 450, iterations: 450, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 939ms, time_since_start: 03m 29s 546ms, eta: 19m 55s 003ms\n",
            "\u001b[32m2022-03-13T14:08:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.6122, train/total_loss: 0.6297, train/total_loss/avg: 0.6122, max mem: 5481.0, experiment: run, epoch: 1, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 774ms, time_since_start: 03m 51s 320ms, eta: 19m 22s 740ms\n",
            "\u001b[32m2022-03-13T14:08:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:08:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-03-13T14:08:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:08:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:08:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:08:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-13T14:08:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:08:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.6931, val/total_loss: 0.6931, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.3818, val/hateful_memes/roc_auc: 0.6155, num_updates: 500, epoch: 1, iterations: 500, max_updates: 3000, val_time: 21s 931ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.615515\n",
            "\u001b[32m2022-03-13T14:09:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, train/hateful_memes/cross_entropy: 0.6297, train/hateful_memes/cross_entropy/avg: 0.5939, train/total_loss: 0.6297, train/total_loss/avg: 0.5939, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 550, iterations: 550, max_updates: 3000, lr: 0.00001, ups: 2.27, time: 22s 580ms, time_since_start: 04m 35s 838ms, eta: 19m 41s 708ms\n",
            "\u001b[32m2022-03-13T14:09:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.5838, train/hateful_memes/cross_entropy/avg: 0.5670, train/total_loss: 0.5838, train/total_loss/avg: 0.5670, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 810ms, time_since_start: 04m 57s 648ms, eta: 18m 38s 076ms\n",
            "\u001b[32m2022-03-13T14:09:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, train/hateful_memes/cross_entropy: 0.5838, train/hateful_memes/cross_entropy/avg: 0.5643, train/total_loss: 0.5838, train/total_loss/avg: 0.5643, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 650, iterations: 650, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 956ms, time_since_start: 05m 19s 604ms, eta: 18m 22s 140ms\n",
            "\u001b[32m2022-03-13T14:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.5745, train/hateful_memes/cross_entropy/avg: 0.5631, train/total_loss: 0.5745, train/total_loss/avg: 0.5631, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 775ms, time_since_start: 05m 41s 380ms, eta: 17m 49s 799ms\n",
            "\u001b[32m2022-03-13T14:10:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, train/hateful_memes/cross_entropy: 0.5745, train/hateful_memes/cross_entropy/avg: 0.5454, train/total_loss: 0.5745, train/total_loss/avg: 0.5454, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 750, iterations: 750, max_updates: 3000, lr: 0.00002, ups: 2.27, time: 22s 008ms, time_since_start: 06m 03s 388ms, eta: 17m 37s 710ms\n",
            "\u001b[32m2022-03-13T14:11:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.5516, train/hateful_memes/cross_entropy/avg: 0.5458, train/total_loss: 0.5516, train/total_loss/avg: 0.5458, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 852ms, time_since_start: 06m 25s 241ms, eta: 17m 06s 888ms\n",
            "\u001b[32m2022-03-13T14:11:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, train/hateful_memes/cross_entropy: 0.5745, train/hateful_memes/cross_entropy/avg: 0.5478, train/total_loss: 0.5745, train/total_loss/avg: 0.5478, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 850, iterations: 850, max_updates: 3000, lr: 0.00002, ups: 2.27, time: 22s 019ms, time_since_start: 06m 47s 260ms, eta: 16m 51s 221ms\n",
            "\u001b[32m2022-03-13T14:11:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.5745, train/hateful_memes/cross_entropy/avg: 0.5513, train/total_loss: 0.5745, train/total_loss/avg: 0.5513, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 811ms, time_since_start: 07m 09s 072ms, eta: 16m 18s 368ms\n",
            "\u001b[32m2022-03-13T14:12:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, train/hateful_memes/cross_entropy: 0.5745, train/hateful_memes/cross_entropy/avg: 0.5477, train/total_loss: 0.5745, train/total_loss/avg: 0.5477, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 950, iterations: 950, max_updates: 3000, lr: 0.00002, ups: 2.27, time: 22s 001ms, time_since_start: 07m 31s 073ms, eta: 16m 03s 390ms\n",
            "\u001b[32m2022-03-13T14:12:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-13T14:12:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:12:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:12:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.5516, train/hateful_memes/cross_entropy/avg: 0.5391, train/total_loss: 0.5516, train/total_loss/avg: 0.5391, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00003, ups: 1.56, time: 32s 492ms, time_since_start: 08m 03s 565ms, eta: 23m 08s 081ms\n",
            "\u001b[32m2022-03-13T14:12:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:12:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:12:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:12:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:12:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:12:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:12:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:12:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-13T14:12:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:13:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:13:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.7719, val/total_loss: 0.7719, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.3554, val/hateful_memes/roc_auc: 0.6682, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 3000, val_time: 21s 919ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.668176\n",
            "\u001b[32m2022-03-13T14:13:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, train/hateful_memes/cross_entropy: 0.5479, train/hateful_memes/cross_entropy/avg: 0.5194, train/total_loss: 0.5479, train/total_loss/avg: 0.5194, max mem: 5495.0, experiment: run, epoch: 2, num_updates: 1050, iterations: 1050, max_updates: 3000, lr: 0.00003, ups: 2.27, time: 22s 334ms, time_since_start: 08m 47s 821ms, eta: 15m 30s 291ms\n",
            "\u001b[32m2022-03-13T14:13:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.5309, train/hateful_memes/cross_entropy/avg: 0.5148, train/total_loss: 0.5309, train/total_loss/avg: 0.5148, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 988ms, time_since_start: 09m 09s 810ms, eta: 14m 52s 393ms\n",
            "\u001b[32m2022-03-13T14:14:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, train/hateful_memes/cross_entropy: 0.5309, train/hateful_memes/cross_entropy/avg: 0.5083, train/total_loss: 0.5309, train/total_loss/avg: 0.5083, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1150, iterations: 1150, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 999ms, time_since_start: 09m 31s 809ms, eta: 14m 29s 319ms\n",
            "\u001b[32m2022-03-13T14:14:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.5309, train/hateful_memes/cross_entropy/avg: 0.4965, train/total_loss: 0.5309, train/total_loss/avg: 0.4965, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 820ms, time_since_start: 09m 53s 630ms, eta: 13m 58s 944ms\n",
            "\u001b[32m2022-03-13T14:14:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, train/hateful_memes/cross_entropy: 0.4839, train/hateful_memes/cross_entropy/avg: 0.4848, train/total_loss: 0.4839, train/total_loss/avg: 0.4848, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1250, iterations: 1250, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 952ms, time_since_start: 10m 15s 583ms, eta: 13m 40s 600ms\n",
            "\u001b[32m2022-03-13T14:15:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.4196, train/hateful_memes/cross_entropy/avg: 0.4813, train/total_loss: 0.4196, train/total_loss/avg: 0.4813, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 804ms, time_since_start: 10m 37s 387ms, eta: 13m 11s 762ms\n",
            "\u001b[32m2022-03-13T14:15:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, train/hateful_memes/cross_entropy: 0.4114, train/hateful_memes/cross_entropy/avg: 0.4772, train/total_loss: 0.4114, train/total_loss/avg: 0.4772, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1350, iterations: 1350, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 972ms, time_since_start: 10m 59s 360ms, eta: 12m 54s 409ms\n",
            "\u001b[32m2022-03-13T14:15:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.4114, train/hateful_memes/cross_entropy/avg: 0.4806, train/total_loss: 0.4114, train/total_loss/avg: 0.4806, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 808ms, time_since_start: 11m 21s 168ms, eta: 12m 25s 327ms\n",
            "\u001b[32m2022-03-13T14:16:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, train/hateful_memes/cross_entropy: 0.4114, train/hateful_memes/cross_entropy/avg: 0.4873, train/total_loss: 0.4114, train/total_loss/avg: 0.4873, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1450, iterations: 1450, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 989ms, time_since_start: 11m 43s 158ms, eta: 12m 08s 024ms\n",
            "\u001b[32m2022-03-13T14:16:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.4114, train/hateful_memes/cross_entropy/avg: 0.4875, train/total_loss: 0.4114, train/total_loss/avg: 0.4875, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 777ms, time_since_start: 12m 04s 935ms, eta: 11m 37s 743ms\n",
            "\u001b[32m2022-03-13T14:16:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:16:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:16:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:16:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:16:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:16:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:16:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:16:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-13T14:16:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:17:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:17:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.8046, val/total_loss: 0.8046, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4257, val/hateful_memes/roc_auc: 0.6957, num_updates: 1500, epoch: 3, iterations: 1500, max_updates: 3000, val_time: 22s 204ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.695691\n",
            "\u001b[32m2022-03-13T14:17:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, train/hateful_memes/cross_entropy: 0.4196, train/hateful_memes/cross_entropy/avg: 0.4941, train/total_loss: 0.4196, train/total_loss/avg: 0.4941, max mem: 5495.0, experiment: run, epoch: 3, num_updates: 1550, iterations: 1550, max_updates: 3000, lr: 0.00004, ups: 2.27, time: 22s 303ms, time_since_start: 12m 49s 448ms, eta: 11m 30s 790ms\n",
            "\u001b[32m2022-03-13T14:17:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.4196, train/hateful_memes/cross_entropy/avg: 0.4868, train/total_loss: 0.4196, train/total_loss/avg: 0.4868, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 775ms, time_since_start: 13m 11s 223ms, eta: 10m 51s 165ms\n",
            "\u001b[32m2022-03-13T14:18:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, train/hateful_memes/cross_entropy: 0.3942, train/hateful_memes/cross_entropy/avg: 0.4753, train/total_loss: 0.3942, train/total_loss/avg: 0.4753, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1650, iterations: 1650, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 862ms, time_since_start: 13m 33s 086ms, eta: 10m 30s 435ms\n",
            "\u001b[32m2022-03-13T14:18:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.3751, train/hateful_memes/cross_entropy/avg: 0.4636, train/total_loss: 0.3751, train/total_loss/avg: 0.4636, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 802ms, time_since_start: 13m 54s 889ms, eta: 10m 05s 419ms\n",
            "\u001b[32m2022-03-13T14:18:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/3000, train/hateful_memes/cross_entropy: 0.3751, train/hateful_memes/cross_entropy/avg: 0.4580, train/total_loss: 0.3751, train/total_loss/avg: 0.4580, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1750, iterations: 1750, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 928ms, time_since_start: 14m 16s 818ms, eta: 09m 45s 490ms\n",
            "\u001b[32m2022-03-13T14:19:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.3716, train/hateful_memes/cross_entropy/avg: 0.4485, train/total_loss: 0.3716, train/total_loss/avg: 0.4485, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00005, ups: 2.38, time: 21s 819ms, time_since_start: 14m 38s 637ms, eta: 09m 19s 267ms\n",
            "\u001b[32m2022-03-13T14:19:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/3000, train/hateful_memes/cross_entropy: 0.3716, train/hateful_memes/cross_entropy/avg: 0.4506, train/total_loss: 0.3716, train/total_loss/avg: 0.4506, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1850, iterations: 1850, max_updates: 3000, lr: 0.00005, ups: 2.38, time: 21s 929ms, time_since_start: 15m 566ms, eta: 08m 58s 681ms\n",
            "\u001b[32m2022-03-13T14:19:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.3650, train/hateful_memes/cross_entropy/avg: 0.4396, train/total_loss: 0.3650, train/total_loss/avg: 0.4396, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00005, ups: 2.38, time: 21s 849ms, time_since_start: 15m 22s 416ms, eta: 08m 33s 381ms\n",
            "\u001b[32m2022-03-13T14:20:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/3000, train/hateful_memes/cross_entropy: 0.2692, train/hateful_memes/cross_entropy/avg: 0.4342, train/total_loss: 0.2692, train/total_loss/avg: 0.4342, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 1950, iterations: 1950, max_updates: 3000, lr: 0.00005, ups: 2.38, time: 21s 951ms, time_since_start: 15m 44s 367ms, eta: 08m 12s 322ms\n",
            "\u001b[32m2022-03-13T14:20:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-13T14:20:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:20:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:20:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:20:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.2607, train/hateful_memes/cross_entropy/avg: 0.4297, train/total_loss: 0.2607, train/total_loss/avg: 0.4297, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00005, ups: 1.56, time: 32s 103ms, time_since_start: 16m 16s 471ms, eta: 11m 25s 734ms\n",
            "\u001b[32m2022-03-13T14:20:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:20:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:20:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:20:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:20:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:21:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-13T14:21:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:21:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:21:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 0.8672, val/total_loss: 0.8672, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.4631, val/hateful_memes/roc_auc: 0.7078, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 3000, val_time: 22s 703ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.707765\n",
            "\u001b[32m2022-03-13T14:21:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/3000, train/hateful_memes/cross_entropy: 0.2607, train/hateful_memes/cross_entropy/avg: 0.4245, train/total_loss: 0.2607, train/total_loss/avg: 0.4245, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2050, iterations: 2050, max_updates: 3000, lr: 0.00005, ups: 2.27, time: 22s 306ms, time_since_start: 17m 01s 483ms, eta: 07m 32s 648ms\n",
            "\u001b[32m2022-03-13T14:21:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.2545, train/hateful_memes/cross_entropy/avg: 0.4181, train/total_loss: 0.2545, train/total_loss/avg: 0.4181, max mem: 5495.0, experiment: run, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00005, ups: 2.38, time: 21s 872ms, time_since_start: 17m 23s 355ms, eta: 07m 474ms\n",
            "\u001b[32m2022-03-13T14:22:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/3000, train/hateful_memes/cross_entropy: 0.2545, train/hateful_memes/cross_entropy/avg: 0.4155, train/total_loss: 0.2545, train/total_loss/avg: 0.4155, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2150, iterations: 2150, max_updates: 3000, lr: 0.00004, ups: 2.27, time: 22s 120ms, time_since_start: 17m 45s 476ms, eta: 06m 41s 620ms\n",
            "\u001b[32m2022-03-13T14:22:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.2545, train/hateful_memes/cross_entropy/avg: 0.4070, train/total_loss: 0.2545, train/total_loss/avg: 0.4070, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 817ms, time_since_start: 18m 07s 293ms, eta: 06m 12s 817ms\n",
            "\u001b[32m2022-03-13T14:23:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/3000, train/hateful_memes/cross_entropy: 0.2607, train/hateful_memes/cross_entropy/avg: 0.4054, train/total_loss: 0.2607, train/total_loss/avg: 0.4054, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2250, iterations: 2250, max_updates: 3000, lr: 0.00004, ups: 2.38, time: 21s 931ms, time_since_start: 18m 29s 225ms, eta: 05m 51s 342ms\n",
            "\u001b[32m2022-03-13T14:23:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.2607, train/hateful_memes/cross_entropy/avg: 0.4028, train/total_loss: 0.2607, train/total_loss/avg: 0.4028, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 884ms, time_since_start: 18m 51s 110ms, eta: 05m 27s 222ms\n",
            "\u001b[32m2022-03-13T14:23:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/3000, train/hateful_memes/cross_entropy: 0.2545, train/hateful_memes/cross_entropy/avg: 0.3946, train/total_loss: 0.2545, train/total_loss/avg: 0.3946, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2350, iterations: 2350, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 932ms, time_since_start: 19m 13s 043ms, eta: 05m 04s 516ms\n",
            "\u001b[32m2022-03-13T14:24:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.2318, train/hateful_memes/cross_entropy/avg: 0.3885, train/total_loss: 0.2318, train/total_loss/avg: 0.3885, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 792ms, time_since_start: 19m 34s 835ms, eta: 04m 39s 292ms\n",
            "\u001b[32m2022-03-13T14:24:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/3000, train/hateful_memes/cross_entropy: 0.2318, train/hateful_memes/cross_entropy/avg: 0.3856, train/total_loss: 0.2318, train/total_loss/avg: 0.3856, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2450, iterations: 2450, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 927ms, time_since_start: 19m 56s 763ms, eta: 04m 17s 603ms\n",
            "\u001b[32m2022-03-13T14:24:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.2318, train/hateful_memes/cross_entropy/avg: 0.3835, train/total_loss: 0.2318, train/total_loss/avg: 0.3835, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00003, ups: 2.38, time: 21s 801ms, time_since_start: 20m 18s 564ms, eta: 03m 52s 836ms\n",
            "\u001b[32m2022-03-13T14:24:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:24:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:24:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:24:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:25:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:25:04 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-03-13T14:25:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:25:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:25:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 0.9177, val/total_loss: 0.9177, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4710, val/hateful_memes/roc_auc: 0.7244, num_updates: 2500, epoch: 5, iterations: 2500, max_updates: 3000, val_time: 22s 014ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.724426\n",
            "\u001b[32m2022-03-13T14:25:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/3000, train/hateful_memes/cross_entropy: 0.2147, train/hateful_memes/cross_entropy/avg: 0.3767, train/total_loss: 0.2147, train/total_loss/avg: 0.3767, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2550, iterations: 2550, max_updates: 3000, lr: 0.00002, ups: 2.27, time: 22s 303ms, time_since_start: 21m 02s 886ms, eta: 03m 34s 378ms\n",
            "\u001b[32m2022-03-13T14:25:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.1578, train/hateful_memes/cross_entropy/avg: 0.3700, train/total_loss: 0.1578, train/total_loss/avg: 0.3700, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 811ms, time_since_start: 21m 24s 698ms, eta: 03m 06s 361ms\n",
            "\u001b[32m2022-03-13T14:26:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, train/hateful_memes/cross_entropy: 0.1578, train/hateful_memes/cross_entropy/avg: 0.3648, train/total_loss: 0.1578, train/total_loss/avg: 0.3648, max mem: 5495.0, experiment: run, epoch: 5, num_updates: 2650, iterations: 2650, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 887ms, time_since_start: 21m 46s 586ms, eta: 02m 43s 633ms\n",
            "\u001b[32m2022-03-13T14:26:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.1578, train/hateful_memes/cross_entropy/avg: 0.3582, train/total_loss: 0.1578, train/total_loss/avg: 0.3582, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00002, ups: 2.38, time: 21s 837ms, time_since_start: 22m 08s 423ms, eta: 02m 19s 931ms\n",
            "\u001b[32m2022-03-13T14:27:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, train/hateful_memes/cross_entropy: 0.1144, train/hateful_memes/cross_entropy/avg: 0.3518, train/total_loss: 0.1144, train/total_loss/avg: 0.3518, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2750, iterations: 2750, max_updates: 3000, lr: 0.00001, ups: 2.27, time: 22s 015ms, time_since_start: 22m 30s 439ms, eta: 01m 57s 564ms\n",
            "\u001b[32m2022-03-13T14:27:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.1020, train/hateful_memes/cross_entropy/avg: 0.3461, train/total_loss: 0.1020, train/total_loss/avg: 0.3461, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 813ms, time_since_start: 22m 52s 253ms, eta: 01m 33s 189ms\n",
            "\u001b[32m2022-03-13T14:27:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, train/hateful_memes/cross_entropy: 0.0974, train/hateful_memes/cross_entropy/avg: 0.3407, train/total_loss: 0.0974, train/total_loss/avg: 0.3407, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2850, iterations: 2850, max_updates: 3000, lr: 0.00001, ups: 2.27, time: 22s 045ms, time_since_start: 23m 14s 298ms, eta: 01m 10s 633ms\n",
            "\u001b[32m2022-03-13T14:28:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.1020, train/hateful_memes/cross_entropy/avg: 0.3387, train/total_loss: 0.1020, train/total_loss/avg: 0.3387, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00001, ups: 2.38, time: 21s 818ms, time_since_start: 23m 36s 116ms, eta: 46s 603ms\n",
            "\u001b[32m2022-03-13T14:28:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, train/hateful_memes/cross_entropy: 0.1020, train/hateful_memes/cross_entropy/avg: 0.3350, train/total_loss: 0.1020, train/total_loss/avg: 0.3350, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 2950, iterations: 2950, max_updates: 3000, lr: 0., ups: 2.38, time: 21s 971ms, time_since_start: 23m 58s 088ms, eta: 23s 465ms\n",
            "\u001b[32m2022-03-13T14:28:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-03-13T14:28:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:29:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:29:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:29:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.0974, train/hateful_memes/cross_entropy/avg: 0.3301, train/total_loss: 0.0974, train/total_loss/avg: 0.3301, max mem: 5495.0, experiment: run, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.56, time: 32s 264ms, time_since_start: 24m 30s 352ms, eta: 0ms\n",
            "\u001b[32m2022-03-13T14:29:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-03-13T14:29:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:29:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:29:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:29:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 34\n",
            "\u001b[32m2022-03-13T14:29:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:29:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-03-13T14:29:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-03-13T14:29:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-03-13T14:29:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 1.3054, val/total_loss: 1.3054, val/hateful_memes/accuracy: 0.7111, val/hateful_memes/binary_f1: 0.5273, val/hateful_memes/roc_auc: 0.7169, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 3000, val_time: 16s 924ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.724426\n",
            "\u001b[32m2022-03-13T14:29:22 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-03-13T14:29:22 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-03-13T14:29:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-03-13T14:29:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-13T14:29:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2500\n",
            "\u001b[32m2022-03-13T14:29:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2500\n",
            "\u001b[32m2022-03-13T14:29:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n",
            "\u001b[32m2022-03-13T14:29:29 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2022-03-13T14:29:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:29:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:29:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 125/125 [00:19<00:00,  6.43it/s]\n",
            "\u001b[32m2022-03-13T14:29:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 125\n",
            "\u001b[32m2022-03-13T14:29:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:29:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, test/hateful_memes/cross_entropy: 0.8769, test/total_loss: 0.8769, test/hateful_memes/accuracy: 0.7070, test/hateful_memes/binary_f1: 0.4991, test/hateful_memes/roc_auc: 0.7609\n",
            "\u001b[32m2022-03-13T14:29:49 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 25m 14s 265ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <font color='#A8EB15'> <b> Evaluation on Validation set </b>"
      ],
      "metadata": {
        "id": "ckVfA7x6sD7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
        "              model=visual_bert \\\n",
        "              dataset=hateful_memes \\\n",
        "              run_type=val \\\n",
        "              checkpoint.resume_file=save/best.ckpt \\\n",
        "              checkpoint.resume_pretrained=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdrBV0AjsDP7",
        "outputId": "e4c79fd7-fc71-4793-9090-10ba12f20cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf_cli.run: \u001b[0mUsing seed 19567245\n",
            "\u001b[32m2022-03-13T14:49:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:49:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:49:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T14:49:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:49:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:49:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T14:49:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-13T14:49:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-13T14:49:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-13T14:49:28 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:49:28 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T14:49:28 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2500\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2500\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "\u001b[32m2022-03-13T14:49:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 9/9 [00:05<00:00,  1.63it/s]\n",
            "\u001b[32m2022-03-13T14:49:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 9\n",
            "\u001b[32m2022-03-13T14:49:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-03-13T14:49:35 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 0.9177, val/total_loss: 0.9177, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4710, val/hateful_memes/roc_auc: 0.7244\n",
            "\u001b[32m2022-03-13T14:49:35 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06s 932ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <font color='#A8EB15'> <b> Predict on Test set"
      ],
      "metadata": {
        "id": "EKXjT0USvVo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_predict config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
        "              model=visual_bert \\\n",
        "              dataset=hateful_memes \\\n",
        "              run_type=test \\\n",
        "              checkpoint.resume_file=save/best.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqm7IexDvrco",
        "outputId": "0d37b754-e917-4b43-d8f2-dde23d53380b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf_cli.run: \u001b[0mUsing seed 10792640\n",
            "\u001b[32m2022-03-13T15:02:10 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T15:02:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T15:02:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2022-03-13T15:02:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T15:02:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T15:02:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-03-13T15:02:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"freeze_base\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"visual_bert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_strategy\": \"default\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"zerobias\": false\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-03-13T15:02:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-03-13T15:02:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-03-13T15:02:20 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T15:02:21 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-03-13T15:02:21 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-03-13T15:02:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 32/32 [00:24<00:00,  1.28it/s]\n",
            "\u001b[32m2022-03-13T15:02:46 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/save/hateful_memes_visual_bert_10792640/reports/hateful_memes_run_test_2022-03-13T15:02:46.csv\n",
            "\u001b[32m2022-03-13T15:02:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 32\n",
            "\u001b[32m2022-03-13T15:02:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pred = pd.read_csv(\"/content/save/hateful_memes_visual_bert_10792640/reports/hateful_memes_run_test_2022-03-13T15:02:46.csv\")\n",
        "test = pd.read_json(\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/test_unseen.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "etUzmCwQwvqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(pred['label'] == test['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85hsIWK4yBmI",
        "outputId": "4527babd-8b54-4235-f23b-b5d5ddf08855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "837"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Martix\n",
        "plotConfusionMatrix(test['label'], pred['label'], 2)\n",
        "ConfMatrix=metrics.confusion_matrix(test['label'], pred['label'])\n",
        "print(\"Accuracy:\", (ConfMatrix[0][0]+ConfMatrix[1][1])/2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "JyYXv6Hpw5dw",
        "outputId": "a3dfd2cf-7fcc-48b0-aee0-81ceca1cd843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6335\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(test['label'], pred['proba'],  pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuXxxtQmzwo2",
        "outputId": "8e9c655f-646f-4b00-e4bb-69702383e3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6992773333333333"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r save/* drive/MyDrive/VisBERTCOCO"
      ],
      "metadata": {
        "id": "J18fLMpQ1FoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/VisBERTCOCO"
      ],
      "metadata": {
        "id": "gWqxXV_W021D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='#A8EB15'> <b> Summary Models - Baseline</b> </font>\n",
        "\n",
        "|Model| Acc. Val   | AuROC Val  | Acc. Test  |  AuROC Test |\n",
        "|--        -|-    --|-     --|--     -|--     -|\n",
        "| mmbt      | 63.33  | 62.83  | 66.05  | 67.15 |\n",
        "| VisualBERT| 68.15  | 67.36  | 68.80  | 72.57 |\n",
        "| VilBERT   | 68.15  | 68.93  | 69.15  | 71.46 |\n",
        "| VisualBERT COCO| 69.63 | 72.44 | 70.70 | 76.09|\n",
        "| concat_bert| 62.78 | 62.78  |       |       |      |"
      ],
      "metadata": {
        "id": "pL7FWMZeYUVV"
      }
    }
  ]
}